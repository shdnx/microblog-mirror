<!DOCTYPE html>
<html>

  <head>
	<meta name="generator" content="Hugo 0.91.2" />

  <title>
      
      The Stargazer
      
  </title>

</head>


  <body>

    

	
<div class="h-feed">

	
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/01/ncleanstall-the-nvidia.html">NCleanstall: the NVIDIA driver installer we don&#39;t deserve</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/01/ncleanstall-the-nvidia.html" class="u-url"><time class="dt-published" datetime="2023-09-01 01:19:27 &#43;0200">Sep 1, 2023</time></a>

		<div class="e-content">
			 <p>Installing NVIDIA Graphics drivers have become annoying. For one, they bundle this GeForce Experience horribleness in it, which requires an NVIDIA account (WTF is that and why would I ever want one?) but in turn has a lot of useful features locked behind it: <strong>automatic driver updates</strong>, for one. Thanks, I hate it. What we as a society have done with technology sometimes just makes me sad.</p>
<p>Thankfully I was not alone in despising the situation, and someone wonderful created <a href="https://www.techpowerup.com/download/techpowerup-nvcleanstall/">NVCleanstall</a> that restored my faith in humanity. It allows you to strip out all the stuff you don&rsquo;t want or need from the NVIDIA installers, <em>and</em> notifies you when there is a new version available. There is a guide available at <a href="https://www.makeuseof.com/customize-nvidia-driver-installation-with-cleanstall/">MakeUseOf</a> if you need help with configuring it.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/27/the-paradox-of.html">The paradox of historical knowlage (Homo Deus)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/27/the-paradox-of.html" class="u-url"><time class="dt-published" datetime="2023-08-27 01:42:51 &#43;0200">Aug 27, 2023</time></a>

		<div class="e-content">
			 <p>As I mentioned in a <a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html">previous post</a>, I&rsquo;ve been reading <a href="https://app.thestorygraph.com/books/c168be57-aaab-44ef-933a-c8c6687e01f2">Homo Deus: A Brief History of Tomorrow</a>. I found this part very interesting (pages 65-67. &ldquo;The Paradox of Knowledge&rdquo;):</p>
<blockquote>
<p>In the middle of the nineteenth century Karl Marx reached brilliant economic insights. Based on these insights he predicted an increasingly violent conflict between the proletariat and the capitalists, ending with the inevitable victory of the former and the collapse of the capitalist system. Marx was certain that the revolution would start in countries that spearheaded the Industrial Revolution - such as Britain, France and the USA - and spread to the rest of the world.</p>
<p>Marx forgot that capitalists know how to read. At first only a handful of disciples took Marx seriously and read his writings. But as these socialist firebrands gained adherents and power, the capitalists became alarmed. They too perused Das Kapital, adopting many of the tools and insights of Marxist analysis. In the twentieth century everybody from street urchins to presidents embraced a Marxist approach to economics and history. [&hellip;]</p>
<p>As people adopted the Marxist diagnosis, they changed their behaviour accordingly. Capitalists in countries such as Britain and France strove to better the lot of the workers, strengthen their national consciousness and integrate them into the political system. Consequently when workers began voting in elections and Labour gained power in one country after another, the capitalists could still sleep soundly in their beds. As a result, Marx&rsquo;s predictions came to naught. Communist revolutions never engulfed the leading industrial powers such as Britain, France and the USA and the dictatorship of the proletariat was consigned to the dustbin of history.</p>
<p>This is the paradox of historical knowledge. Knowledge that does not change behaviour is useless. But knowledge that changes behaviour quickly loses its relevance.</p>
</blockquote>
<p>It&rsquo;s an interesting narrative and I think the overall the idea makes sense, but I&rsquo;m not sure I fully buy it here.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/26/zen-story-maybe.html">Zen Story: Maybe</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/26/zen-story-maybe.html" class="u-url"><time class="dt-published" datetime="2023-08-26 01:56:16 &#43;0200">Aug 26, 2023</time></a>

		<div class="e-content">
			 <p>I came by this story the other day: <a href="https://thedailyzen.org/2015/03/20/zen-story-maybe/">Zen Story: Maybe</a>. Since it&rsquo;s very short, I&rsquo;ll just quote it in full here:</p>
<blockquote>
<p>There is a Taoist story of an old farmer who had worked his crops for many years. One day his horse ran away. Upon hearing the news, his neighbors came to visit. “Such bad luck,” they said sympathetically. “Maybe,” the farmer replied.</p>
<p>The next morning the horse returned, bringing with it three other wild horses. “How wonderful,” the neighbors exclaimed. “Maybe,” replied the old man.</p>
<p>The following day, his son tried to ride one of the untamed horses, was thrown, and broke his leg. The neighbors again came to offer their sympathy on his misfortune. “Maybe,” answered the farmer.</p>
<p>The day after, military officials came to the village to draft young men into the army. Seeing that the son’s leg was broken, they passed him by. The neighbors congratulated the farmer on how well things had turned out. “Maybe,” said the farmer.</p>
</blockquote>
<p>Life is unpredictable. Losing something will at times lead to winning something greater, but just as often it will not. &ldquo;When one door closes, another opens&rdquo;, except in real life, sometimes there is no other door, and the loss is just that: a loss. I think that is the truest test of one&rsquo;s Zen.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/24/languagelearning-ability-children.html">Language-learning ability: children vs adults</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/24/languagelearning-ability-children.html" class="u-url"><time class="dt-published" datetime="2023-08-24 03:04:27 &#43;0200">Aug 24, 2023</time></a>

		<div class="e-content">
			 <p>A long, long article summarizing lots of research on language-learning ability, and whether the commonly shared belief that children are much better at learning languages than adults actually holds true: <a href="https://astralcodexten.substack.com/p/critical-periods-for-language-much">Critical Periods For Language: Much More Than You Wanted To Know</a>.</p>
<p>In summary:</p>
<blockquote>
<p>Children seem to be able to pick up second languages faster than adults. It’s hard to tell exactly when the learning rate slows, or to be sure this is a biological phenomenon instead of an effect of school ending or picking low-hanging fruits. Mastering a language perfectly in adulthood is hard, but maybe just because there’s not enough time to learn it at adult’s slower learning rates.</p>
<p>Babies don’t seem any better than older children (eg 17 year olds), and are limited by being babies. The difference between their excellent ability to learn a first language, and (for example) a middle-schooler struggling to learn a second language, probably is just exposure and motivation, and not an additional magic language ability.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html">Progress without brakes, for good or ill (Homo Deus)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html" class="u-url"><time class="dt-published" datetime="2023-08-24 01:15:24 &#43;0200">Aug 24, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;ve been reading <a href="https://app.thestorygraph.com/books/c168be57-aaab-44ef-933a-c8c6687e01f2">Homo Deus: A Brief History of Tomorrow</a>. In the first chapter <em>The new human agenda</em>, the author gives his interpretation of history and how humanity has ended up as it is today due to scientific and technological development &ndash; roughly as follows: humans seeking comfort and happiness developed technology to try to eradicate or at least mitigate ills such as famines and plagues. I find this view to be a bit naive, and I would at least add &ldquo;seeking better ways of killing each other&rdquo; to the list of factors encouraging technological development; but I can also see the argument that developing weapons is also ultimately about comfort and happiness, via the morally-perhaps-questionable route of, well, <em>acquiring</em> an abundance of resources and then protecting it.</p>
<p>Nonetheless, he then continues on to assert that this same drive will keep on transforming our world with unforeseeable consequences. I found this part to be a good reality check:</p>
<blockquote>
<p>When people realize just how fast we are rushing towards the unknown, and that they cannot count even on death to shield them from it, their reaction is to hope that somebody will hit the brakes and slow us down. But we cannot hit the breaks, for several reasons.</p>
<p>Firstly, nobody knows where the brakes are. While some experts are familiar with developments in one field, such as artificial intelligence, nanotechnology, big data or genetics, no one is an expert on everything. No one is therefore capable of connecting all the dots and seeing the full picture. Different fields influence one another in such intricate ways that even the best minds cannot fathom how breakthroughs in artificial intelligence might impact nanotechnology or vice versa. Nobody can absorb all the latest scientific discoveries, nobody can predict how the global economy will look in ten years, and nobody has a clue where we are heading in such a rush. Since no one understands the system anymore, no one can stop it.</p>
<p>Secondly, if we somehow succeed in hitting the brakes, our economy will collapse, along with our society. As explained in a later chapter, modern economy needs constant and indefinite growth in order to survive. If growth ever stops, the economy won&rsquo;t settle down to some cozy equilibrium; it will fall to pieces. [&hellip;]</p>
</blockquote>
<p>(Page 58-59, &ldquo;Can Someone Please Hit the Breaks?&quot;)</p>
<p>The book is definitely growing on me: it is not the one-sided, ever-optimistic rosy vision of a utopistic future built by Us Enlightened Humans (tm) that I expected, but rather a fairly nuanced analysis of the fundamental forces that motivate humans and got us to where we are and will continue to drive us &ndash; for good or ill.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/22/a-guide-to.html">A guide to error-handling</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/22/a-guide-to.html" class="u-url"><time class="dt-published" datetime="2023-08-22 03:44:27 &#43;0200">Aug 22, 2023</time></a>

		<div class="e-content">
			 <p>Error-handling is a heavily debated topic. Generally speaking, the following mechanisms exist to handle errors when implementing a function:</p>
<ol>
<li>Return something to indicate the error: an error code, a sentinel value (like <code>NaN</code>), or using a result-or-error type (like C++&rsquo;s <code>std::expected</code> or Rust&rsquo;s <code>Result</code>).</li>
<li>Throw an exception and unwind the stack (i.e. walk it backwards) until a handler is found and return control there; or if no viable handlers exist, terminate the application.</li>
<li>Abort or terminate the application, after printing an error message and generating a core dump. (<code>assert()</code> is also just a flavor of this.)</li>
</ol>
<p>Notably, options 1 and 2 (return values and exceptions) are <em>recoverable</em>, while option 3 (terminate) is not. So let&rsquo;s start with these two categories: when do you want errors to be recoverable? Some errors are almost always recoverable, such as attempting to open a file that does not exist (maybe we should create it then, or try another file), and some are almost never such as encountering inconsistent application state (which usually indicates a programming error or memory corruption).</p>
<h2 id="recoverable-vs-non-recoverable-errors">Recoverable vs non-recoverable errors</h2>
<p>In my experience, the answer mostly depends on the context of your code: are you writing application code or library code? A library code that will be used across many applications usually shouldn&rsquo;t decide on the application&rsquo;s behalf whether that should crash or not, as it doesn&rsquo;t tend to be aware of the context of its own usage: it cannot tell whether the using application is a critical service that must never go down or perhaps just a one-off script. Therefore libraries should typically use recoverable forms of error reporting, even for severe errors.</p>
<p>When writing application code, simply crashing on an error may be an option, but again depends on the type of application and whether you are aware of all possible contexts it may run in. As a rule of thumb however, it&rsquo;s better for most applications to crash rather than risk doing something unintended or corrupting data.</p>
<p>So in application code, I tend to use assertions a lot, even for things like function preconditions when I know all contexts and all places where the function may be called from, since in these cases a precondition failure must be a programming error and I want to crash early rather than risk doing something unintended. In library code, I tend to use assertions only as sanity checks: conditions that must always hold, barring a severe programming error inside the library or memory corruption.</p>
<p>There are, of course, caveats. For example, even if an error is unrecoverable, you may want to use a recoverable mechanism for reporting it to give a chance for user code to gracefully shut down and signal the error to downstream services or dump its internal state to help with debugging. This is a painful use case because it is a subversion of the whole concept of an unrecoverable error; this is one main reason why many (most?) programming languages typically rely on exceptions for reporting every type of error. (The other being simplicity by having fewer competing mechanisms in the language.)</p>
<p>Finally, some errors may be unrecoverable in the vast majority of cases, but actually recoverable in a few: one example is a memory allocation failure. Most libraries and applications in most cases just want to crash when this happens because there is nothing sane they can do otherwise, but sometimes you know you&rsquo;re loading a gigantic amount of data into memory and can gracefully handle running out of memory; or you are using something like an arena allocator or ring buffer where running out of space can happen as part of normal operations.</p>
<h2 id="reporting-recoverable-errors">Reporting recoverable errors</h2>
<p>To recap, the two main ways of reporting errors in way that is recoverable, i.e. it&rsquo;s up to the caller to decide what to do with, is by using the function return value mechanism by returning an error value or by throwing an exception and walking upwards on the call stack until a suitable handler could be found. They both have advantages and disadvantages.</p>
<p>Returning an error is conceptually and implementationally simple, and usually very efficient (mostly depending on whether how expensive is it to construct the error object). However, propagating errors (i.e. transporting them from where the error occurred to a level where we can handle them) tends to be very tedious. Some languages like Rust provide language-level support to do this conveniently, but this is very painful in languages like C and Go given that most function calls have no reasonable ways of handling errors, so usually errors have to propagate a good distance up before reaching code that knows what to do with them.</p>
<p>Exceptions were designed with that last insight in mind, and explicitly allow the error site (where the error is detected) and the handler to be arbitrarily far away from each other, without this having to impact any other code in between. In return though, exceptions tend to be very expensive in terms of runtime performance, much more so than returning error values.</p>
<p>Further, the fact that in a call chain of <code>A -&gt; B -&gt; C</code> the fact that <code>C</code> may throw exceptions that <code>A</code> knows how to catch while B has no knowledge of any of this can be viewed as an anti-feature: when a developer is looking at the code of <code>B</code>, they may not be aware that <code>C</code> may throw. In the error-returning approach, the possible presence of errors is always spelled out explicitly, to the point that it can easily become annoying (because e.g. you have to keep writing <code>.unwrap()</code> in Rust or <code>if err != nil then return err</code> in Go); but the opposite can also easily be counter-productive.</p>
<h2 id="exceptions-in-c">Exceptions in C++</h2>
<p>C++ makes things more complicated by offering both options, in a typical C++ fashion. You may return error codes, or use the newly added <a href="https://en.cppreference.com/w/cpp/utility/expected"><code>std::expected</code></a> which is basically a specialized union that either represents the function result or an error, but has no special handling or syntactical sugar support.</p>
<p>You may also throw and catch exceptions in the same way as many mainstream languages, but unlike them, all major C++ compilers let you disable exceptions altogether by passing a compiler flag like <code>-fno-exceptions</code>. The C++ standard library does use exceptions, but inconsistently: the C++17 <code>std::filesystem</code> implementation attempts to cater to both groups by offering two distinct set of functions that do the same thing, except one set uses exceptions and the other error codes (see e.g. <a href="https://en.cppreference.com/w/cpp/filesystem/create_directory"><code>std::filesystem::create_directory()</code></a>).</p>
<p>This should already tell you that the C++ community is split on whether exceptions are a good idea or not.
Like with so many problematic things in C++, the concerns about exceptions revolve around performance:</p>
<ul>
<li>Exceptions may be prohibitively expensive or not even supported in resource-constrained embedded environments, where C++ is often used.</li>
<li>Exceptions have a large run-time performance penalty when thrown.</li>
<li>Even when not thrown, the possible presence of exceptions may prohibit some compiler optimizations.</li>
<li>The possibility of exceptions significantly raise the size of the binary due to the unwind code that has to be generated.</li>
</ul>
<p>C++ has so far been trying to cater to both groups: <a href="https://en.cppreference.com/w/cpp/language/noexcept_spec"><code>noexcept</code></a> was added to mark functions that can never throw to try an alleviate some performance concerns about exceptions, and there is a proposal for <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0709r0.pdf">Zero-overhead deterministic exceptions</a>. On the other side, <code>std::variant</code> and <code>std::expected</code> were both added recently, both of which make it easier to return and store error values.</p>
<p>The C++ <code>noexcept</code> specifier is actually a bit of a lie: it doesn&rsquo;t actually promise that the given function cannot throw exceptions, but rather than exceptions will never escape from that function; this would call <code>std::terminate()</code> instead. It still works as a guarantee that can be leveraged by the caller, enabling optimizations by for example not having to emit code to call destructors when an exception is unwinding the stack. The canonical example is resizing an <code>std::vector</code>: when it is full while we are trying to insert a new element, a larger array needs to be allocated and the elements moved before the old array can be deallocated and insertion may proceed. However, a move operation throwing would result in the vector being in an inconsistent state, as some objects may already have been moved while others have not. Therefore, if the move constructor of the element type is declared as non-noexcept, then <code>std::vector</code> will copy objects instead of move them when reallocating, even though copying can be hugely more expensive; however if a copy operation throws then we can simply deallocate the new array and propagate the exception, leaving the vector in its original state.</p>
<h2 id="recoverable-errors-in-other-languages">Recoverable errors in other languages</h2>
<p>It is also interesting to see how different languages deal with error-handling, given its heavily debated nature.</p>
<ul>
<li>Most mainstream languages like Python, Java, C#, JavaScript, and PHP all primarily use exceptions.</li>
<li>C uses error codes given that it has no proper objects nor exceptions, though it does have the facilities to implement exceptions with <code>goto</code>, <a href="https://en.cppreference.com/w/cpp/utility/program/setjmp"><code>setjmp()</code> and <code>longjmp()</code></a>.</li>
<li>Rust mainly relies on a <a href="https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html"><code>Result</code> type</a> which is the equivalent of <code>std::expected</code>, but has dedicated language support. Rust does not have exceptions, but it does have unwinding panics and a way to catch them with <a href="https://doc.rust-lang.org/std/panic/fn.catch_unwind.html"><code>std::panic::catch_unwind</code></a>.</li>
<li>Python uses exceptions not only for errors but also for control flow, e.g. to indicate that there&rsquo;s no more data to consume in an iterator by raising <a href="https://docs.python.org/3/library/exceptions.html#StopIteration"><code>StopIteration</code></a>. Python also has <code>assert</code> as a language keyword, but it raises an <code>AssertionError</code> exception on failure, unlike in other languages.</li>
<li><a href="https://ziglang.org/documentation/master/#Errors">Zig uses an approach</a> similar in spirit to C&rsquo;s error codes but implemented more like Rust&rsquo;s <code>Result</code> or C++&rsquo;s <code>std::expected</code>: functions return either their return value or an error, where an error is essentially a dedicated enum.</li>
<li><a href="https://go.dev/blog/error-handling-and-go">Go has a simple approach</a> where functions typically return a <code>(result, error)</code> pair. Errors are not particularly special, and little-to-no language support exists for them.</li>
<li>The Microsoft .NET Framework that C# relies on used to throw an exception of type <a href="https://stackoverflow.com/a/4104845"><code>System.ExecutionEngineException</code></a> when the runtime detected a corrupted internal state: using a recoverable error mechanism to report a fundamentally unrecoverable error. At least they stopped doing this, and now the class is <a href="https://learn.microsoft.com/en-us/dotnet/api/system.executionengineexception?view=net-7.0">marked as obsolate</a>.</li>
</ul>
<p>I personally particularly dislike Go&rsquo;s approach, and struggle to see how that is a good idea: it doesn&rsquo;t even have the excuse of being an ancient, very low-level language like C.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/20/the-technology-of.html">The technology of Maglev trains</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/20/the-technology-of.html" class="u-url"><time class="dt-published" datetime="2023-08-20 14:18:32 &#43;0200">Aug 20, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;m a big fan of traveling by trains, and it always pains me how impractical it is in most parts of the world, even in places where they would make economic and environmental sense. Europe has been getting better at this, with excellent high-speed rail connections from Amsterdam to Paris for instance, but other than a few anomalies like that, you&rsquo;re mostly out of luck.</p>
<p>Maglev trains are super high-tech and an order of magnitude more expensive than traditional high-speed railways, so they are also much harder to economically justify, but what they lack in that area they more than make up for by the technology just being absolutely <em>awesome</em>. Is wanting to ride one a good enough reason to plan a trip to Japan? You know, it might be!</p>
<p>The Real Engineering channel on YouTube recently published a video on them, which I can heartily recommend:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/S4L_0CDsd1I' frameborder='0' allowfullscreen></iframe></div>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/13/the-carrot-problem.html">The Carrot Problem: sharing the secret of your success</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/13/the-carrot-problem.html" class="u-url"><time class="dt-published" datetime="2023-08-13 16:31:45 &#43;0200">Aug 13, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://www.atvbt.com/the-carrot-problem/">The Carrot Problem</a> seen via <a href="https://news.ycombinator.com/item?id=37100226">Hacker News</a>:</p>
<blockquote>
<p>In World War II, the story goes, the British invented a new kind of onboard radar that allowed its pilots to shoot down German planes at night.</p>
<p>They didn&rsquo;t want the Germans to know about this technology, but they had to give an explanation for their new, improbable powers.</p>
<p>So they invented a propaganda campaign that claimed their pilots had developed exceptional eyesight by eating &ldquo;an excess of carrots.&rdquo;</p>
<p>If you&rsquo;re going to trick people into doing something pointless, eating excessive carrots seems like one of the better ones. Still, there&rsquo;s an issue: people who believed the propaganda and tried to get super-sight would be spending time and effort on something that couldn&rsquo;t possibly solve their issue.</p>
<p>I&rsquo;ll call this a Carrot Problem.</p>
<p>Once you look for Carrot Problems, you see them everywhere. Essentially, any time someone achieves success in a way they don&rsquo;t want to admit publicly, they have to come up with an excuse for their abilities. And that means misleading a bunch of people into (potentially) wasting their time, or worse.</p>
</blockquote>
<p>I love the name, but this is genuinely good insight. I think I&rsquo;d distill the problem a bit more fundamentally down to this: <em>successful individuals or organizations rarely have an interest in sharing what made them successful</em>; most of them time in fact, they are heavily disincentivized from doing so.</p>
<p>The reasons aren&rsquo;t necessarily nefarious: even if you have done nothing immoral or illegal, sharing the secret of your success simply creates more competition for you, which is counter-productive for you.</p>
<p>Further, you may not actually even know what made you successful! Oh you might <em>think</em> that you know: it was definitely all your hard work, or connections, or unique idea, etc. but chances are that in reality you had a generous dose of luck<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> without which it all would have been for nothing.</p>
<p>Keep this in mind whenever you encounter the success story of a famous person or rich company: chances are that even if the story is accurate and true and they are not trying to make you eat carrots, it is unlikely to be reproducible, as it will almost inevitably fail to account for all the hidden &ldquo;lucky&rdquo; factors.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>By luck I mean circumstances that you do not control and are not even fully aware of. There is no such fundamental force in the universe, to the best of my knowledge and understanding.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/13/everything-you-need.html">Everything you need to know about light therapy</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/13/everything-you-need.html" class="u-url"><time class="dt-published" datetime="2023-08-13 15:43:38 &#43;0200">Aug 13, 2023</time></a>

		<div class="e-content">
			 <p>Here is a <a href="https://lorienpsych.com/2020/12/19/light-therapy/">comprehensive guide to light therapy</a> by a renowned and practicing psychiatrist. In short:</p>
<blockquote>
<p>Light therapy can treat seasonal affective disorder (eg winter depression) and sometimes regular depression. Buy this light box [see link below], and at some time between 6 AM and 9 AM, sit exactly 12 inches away from it and do some activity that doesn’t involve staring directly at the light box. Continue every morning for the period of time you’re at risk of depression. If bipolar, don’t try this without medical supervision.</p>
</blockquote>
<p>It is detailed, easy to read and understand (no medical jargon), and no-nonsense.</p>
<blockquote>
<p>If you’ve read this document, you already know more than 95% of psychiatrists about light therapy. Trust me, I’ve given lectures about this to psychiatrists and they don’t know any of this stuff.</p>
</blockquote>
<p>The linked light box: <a href="https://www.amazon.com/dp/B00PCN4UVU?ots=1&amp;slotNum=1&amp;imprToken=36bb3cd9-767a-b515-0b4&amp;ascsubtag=%5B%5Dst%5Bp%5Dcjnm77g18005q1by6kggjizam%5Bi%5Dh0nXBx%5Bd%5DD%5Bz%5Dm%5Bt%5Dw%5Br%5Dgoogle.com&amp;tag=thestrategistsite-20">Carex Day-Light Classic Plus Bright Light Therapy Lamp</a></p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/12/sometimes-setting-yourself.html">Sometimes setting yourself on fire sheds light on the situation</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/12/sometimes-setting-yourself.html" class="u-url"><time class="dt-published" datetime="2023-08-12 03:41:46 &#43;0200">Aug 12, 2023</time></a>

		<div class="e-content">
			 <p>In today&rsquo;s &ldquo;so that is a thing that happened&rdquo; column, found in a completely unrelated article (<a href="https://astralcodexten.substack.com/p/still-alive">Still Alive</a>):</p>
<blockquote>
<p>In Street Fighter, the hero confronts the Big Bad about the time he destroyed her village. The Big Bad has destroyed so much stuff he doesn&rsquo;t even remember: &ldquo;For you, the day [I burned] your village was the most important day of your life. For me, it was Tuesday.&rdquo;</p>
<p>&hellip;</p>
<p>In 2010, a corrupt policewoman demanded a bribe from impoverished pushcart vendor Mohammed Bouazizi. He couldn&rsquo;t afford it. She confiscated his goods, insulted him, and (according to some sources) slapped him. He was humiliated and destitute and had no hope of ever getting back at a police officer. So he made the very reasonable decision to douse himself in gasoline and set himself on fire in the public square. One thing led to another, and eventually a mostly-peaceful revolution ousted the government of Tunisia. I am very sorry for Mr. Bouazizi and his family. But he did find a way to make the offending policewoman remember the day she harassed him as something other than Tuesday. As the saying goes, &ldquo;sometimes setting yourself on fire sheds light on the situation&rdquo;.</p>
</blockquote>
<p>That is absolutely horrible, but I also feel like I have to ask about the practical applicability of that saying in the last sentence. (Also, that &ldquo;One thing led to another&rdquo; is carrying a whole lot in there.)</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/12/designs-vs-the.html">Designs vs the world</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/12/designs-vs-the.html" class="u-url"><time class="dt-published" datetime="2023-08-12 02:11:31 &#43;0200">Aug 12, 2023</time></a>

		<div class="e-content">
			 <p>A fun short article I found today: <a href="https://cabel.com/2023/07/30/fantasy-meets-reality/">Fantasy Meets Reality</a></p>
<blockquote>
<p>One of my favorite things to notice as a weirdo is when the good intentions of design slam into the hard reality of humans and the real world. It’s always interesting. [&hellip;]</p>
<p>When it comes to design in the real world, there are a few basic rules that seem to always apply: if it looks neat, people will want to take a photo with it. If it looks comfortable, people will want to sit on it. If it looks fun, people will play around on it. Etc.</p>
<p>And yet, designers are often still caught by surprise!</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		

		<a href="https://blog.gaborkozar.me/2023/08/11/i-didnt-know.html" class="u-url"><time class="dt-published" datetime="2023-08-11 01:05:29 &#43;0200">Aug 11, 2023</time></a>

		<div class="e-content">
			 <p>I didn&rsquo;t know, but apparently iPhone 14 and later offer <a href="https://support.apple.com/en-us/HT213426">Emergency SOS via satellite</a>:</p>
<blockquote>
<p>Emergency SOS via satellite can help you connect with emergency services under exceptional circumstances when no other means of reaching emergency services are available. If you call or text emergency services and can&rsquo;t connect because you&rsquo;re outside the range of cellular and Wi-Fi coverage, your iPhone tries to connect you via satellite to the help that you need.</p>
</blockquote>
<p>Apparently this has <a href="https://daringfireball.net/linked/2023/08/10/maui-emergency-sos">saved some people&rsquo;s lives</a> during the recent Maui wildfires.</p>
<p>Hell yeah, technology!</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		

		<a href="https://blog.gaborkozar.me/2023/08/11/i-recently-encountered.html" class="u-url"><time class="dt-published" datetime="2023-08-11 00:49:34 &#43;0200">Aug 11, 2023</time></a>

		<div class="e-content">
			 <p>I recently encountered this on <a href="https://news.ycombinator.com/item?id=37080506">Hacker News</a>: <a href="https://kentonshouse.com/">Kenton&rsquo;s House</a></p>
<blockquote>
<p>I used to own a house optimized for LAN parties, with fold-out gaming stations built into the walls. (I sold it in early 2020 and moved to Austin, TX, where I plan to build another one&hellip;)</p>
</blockquote>
<p>Now <em>that</em> is cool! Is that the dream? It very well might be.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/07/the-relative-age.html">The Relative Age Effect: How Your Birth Month Impacts Your Success</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/07/the-relative-age.html" class="u-url"><time class="dt-published" datetime="2023-08-07 00:45:57 &#43;0200">Aug 7, 2023</time></a>

		<div class="e-content">
			 <p>The key insight behind the <a href="https://en.wikipedia.org/wiki/Relative_age_effect">Relative Age Effect</a> is that schools and all types of education all over the world have fixed yearly start dates, and which year a kid starts depends on their age measured in years. One year is quite a long time, especially when it comes to the development of children: consider that they start attending primary school at the age of 6. The development difference between somebody 6 years and 1 day old vs 6 years and 364 days old is massive: the latter has been alive for 17% longer and had all that extra time to develop. This means that they are taller, stronger, and smarter: they will most likely do better in school or any areas of life.</p>
<p>Due to the Matthew Effect of compounding advantages, this advantage early on adds up and has a huge effect for success later in life, in a way that is statistically <em>glaringly</em> obvious as the video demonstrates. Children born in different months of the same year will have hugely different outcomes. Fascinating!</p>
<p>This is an excellent video explaining it with good visualizations and lots of sources and references:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/eYRgTj9MRLg' frameborder='0' allowfullscreen></iframe></div>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/07/modern-movies-on.html">Modern movies on human bodies: everyone is beautiful and no one is horny</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/07/modern-movies-on.html" class="u-url"><time class="dt-published" datetime="2023-08-07 00:08:29 &#43;0200">Aug 7, 2023</time></a>

		<div class="e-content">
			 <p>An interesting, thought-provoking, and well-written article:</p>
<h2 id="everyone-is-beautiful-and-no-one-is-hornyhttpsbloodknifecomeveryone-beautiful-no-one-horny"><a href="https://bloodknife.com/everyone-beautiful-no-one-horny/">Everyone is beautiful and no one is horny</a></h2>
<blockquote>
<p>Modern action and superhero films fetishize the body, even as they desexualize it.</p>
</blockquote>
<p>See also the discussion on <a href="https://news.ycombinator.com/item?id=36990215">Hacker News</a>.</p>
<blockquote>
<p>Of course there’s sex in a movie. Isn’t there always?</p>
<p>The answer, of course, is not anymore—at least not when it comes to modern blockbusters</p>
<p>We’re told that Tony Stark and Pepper Potts are an item, but no actual romantic or sexual chemistry between them is shown in the films. [&hellip;]</p>
</blockquote>
<p>I&rsquo;m not a particularly avid movie-watcher, and I feel mostly uninterested in the superhero movies that appear to have flooded the cinemas in the last few years. I say this to highlight the point that <em>even I have noticed this</em>: the utter sterility of many (most?) mainstream movies with their total lack of sexual chemistry. I have thought that this is just Hollywood appealing to disappointingly dated purist sensitivities that the United States seems to be eternally stuck in, but this article offers a much more interesting exploration:</p>
<blockquote>
<p>When a nation feels threatened, it gets swole. Germans and Norwegians became obsessed with individual self-improvement through physical fitness around the end of the Napoleonic Era. British citizens took up this Physical Culture as the 19th century—and their empire—began to wane. And yoga, in its current practice as a form of meditative strength training, came out of the Indian Independence movement of the 1920s and 30s.</p>
<p>The impetus of these movements isn’t fitness for the sake of pleasure, for the pure joys of strength and physical beauty. It’s competitive. It’s about getting strong enough to fight The Enemy, whoever that may be.</p>
<p>The United States is, of course, not immune to this. [&hellip;] The attack on the World Trade Center and the Pentagon sparked a new War on Terror, and America needed to get in shape so we could win that war. The USA’s hyper-militaristic troop-worshipping post-9/11 culture seeped into the panic over obesity and gave birth to a terrifying, swole baby.</p>
</blockquote>
<p>Go read the article! I&rsquo;m going to just pull this one paragraph out of the article without context, because I found it hilarious and accurate:</p>
<blockquote>
<p>Contemporary gym ads focus on rigidly isolated self-improvement: be your best self. Create a new you. We don’t exercise, we don’t work out: we train, and we train in fitness programs with names like Booty Bootcamp, as if we’re getting our booties battle-ready to fight in the Great Booty War. There is no promise of intimacy. Like our heroes in the Marvel Cinematic Universe, like Rico and Dizzy and all the other infantry in Starship Troopers, we are horny only for annihilation.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/27/why-is-online.html">Why is online dating terrible?</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/27/why-is-online.html" class="u-url"><time class="dt-published" datetime="2023-07-27 00:48:05 &#43;0200">Jul 27, 2023</time></a>

		<div class="e-content">
			 <p>It&rsquo;s not exactly controversial to say that online dating is awful, at least for those seeking heterosexual relationships. It sucks very differently for men and women though, and from what I hear it can actually be pretty good for homosexual relationships. (Especially for gay men, I think? I haven&rsquo;t really heard about it for women, but then again I do live under a rock, in a cave, and on another planet.)</p>
<p>This difference is interesting and worth exploring, though anything as socially and culturally involved as sex, dating, and relationships is obviously riddled with proverbial mines. I&rsquo;ve recently come across this video though that I think did an unusually good job at analyzing the situation:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/x3lypVnJ0HM' frameborder='0' allowfullscreen></iframe></div>
<p>He highlights two main problems: there are many more men on these dating apps then women, and women appear to judge the profiles of men much more harshly for attractiveness than vica-versa. It&rsquo;s obviously nigh-impossible to make online dating not be awful, with such a premise, though various attempts have been made.</p>
<p>Beyond the cultural and social controversy surrounding the whole topic, I would like to highlight one thing: running an effective online dating app would be <em>terrible</em> business under most models, whether it&rsquo;s making money from ads, optional purchasable extra features, or subscriptions. Imagine if the average user signed up for the app, created their profile, started using it, and the app would be so effective that they&rsquo;d immediately get genuine dates with real, interesting partners. They&rsquo;d probably find themselves in a new relationship in no time and would have no need for the app anymore!</p>
<p>A non-user doesn&rsquo;t make money, therefore it is the financial interest of the business to keep you a user. On the other hand, if the app is completely ineffective, then again it would lose all but the most desperate potential users. because then it&rsquo;s just a waste of time, why would you pay for it? In order to keep the active user count as high as possible, the app cannot be too effective nor too ineffective.</p>
<p>The incentives of the users (who want to find sexual or romantic partners) and the business (who wants you to keep using it for as long as possible) are greatly misaligned, and that is not good. Any success of any kind of social or economic system only comes from successfully aligning the interests of as many groups as possible.</p>
<p>Perhaps there is a business model that could work for online dating that would not be so horribly misaligned, but if so, I haven&rsquo;t seen it yet.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/26/prql-its-like.html">PRQL: it&#39;s like SQL but actually makes sense</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/26/prql-its-like.html" class="u-url"><time class="dt-published" datetime="2023-07-26 02:13:22 &#43;0200">Jul 26, 2023</time></a>

		<div class="e-content">
			 <p>I have never had to write particularly complex SQL queries, but I&rsquo;ve written and seen enough to know that that SQL gets nasty and complicated once you start using the non-trivial features, not to mention things like optimizing queries based on the execution plan, or optimizing indexes, or the other million things that make Database Engineer an actual job title.</p>
<p>One thing in particular that has always bothered me (even with my very limited experience!) about SQL is how it tries to read like English, which makes it harder to both read and write in many cases, the simplest and canonical one being that you have to write <code>SELECT ... FROM table</code>. What table we are querying is the first most important information for any readers, and it&rsquo;s also the first thing a query writer must decide, yet SQL forces you to write it second. This gets especially annoying when the <code>SELECT</code> clause is very complex and consists of multiple lines. To illustrate, here is an example I blatantly stole from an <a href="https://learnsql.com/blog/25-advanced-sql-query-examples/">SQL tutorial website</a>:</p>
<pre><code>SELECT
  SUM (CASE
    WHEN dept_id IN (‘SALES’,’HUMAN RESOURCES’)
    THEN salary
    ELSE 0 END) AS total_salary_sales_and_hr,
  SUM (CASE
    WHEN dept_id IN (‘IT’,’SUPPORT’)
    THEN salary
    ELSE 0 END) AS total_salary_it_and_support
FROM employee
</code></pre>
<p>The table you are reading from is <em>critical</em> information in giving you the context of what you are looking at and allow you to begin understanding what this query is supposed to be doing, and it&rsquo;s hidden at the very end of the query.</p>
<p>So yeah, SQL is not the best, and this is where Pipelined Relational Query Language (PRQL, pronounced “Prequel”) comes in and promises to do better: <a href="https://prql-lang.org/">PRQL: a modern language for transforming data</a> (<a href="https://news.ycombinator.com/item?id=36866861">Hacker News thread</a>), promising &ldquo;a simple, powerful, pipelined SQL replacement&rdquo;. At a glance, it looks like everything I&rsquo;d want it to be!</p>
<p>I&rsquo;m really glad this exists, even though I personally don&rsquo;t have much use for it at the moment.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/25/its-turtles-all.html">It&#39;s turtles all the way down: from high-level programming to CPU microcode</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/25/its-turtles-all.html" class="u-url"><time class="dt-published" datetime="2023-07-25 03:06:15 &#43;0200">Jul 25, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">It&rsquo;s turtles all the way down</a>:</p>
<blockquote>
<p>The following anecdote is told of William James. [&hellip;] After a lecture on cosmology and the structure of the solar system, James was accosted by a little old lady.</p>
<p>&ldquo;Your theory that the sun is the centre of the solar system, and the earth is a ball which rotates around it has a very convincing ring to it, Mr. James, but it&rsquo;s wrong. I&rsquo;ve got a better theory,&rdquo; said the little old lady.</p>
<p>&ldquo;And what is that, madam?&rdquo; inquired James politely.</p>
<p>&ldquo;That we live on a crust of earth which is on the back of a giant turtle.&rdquo;</p>
<p>Not wishing to demolish this absurd little theory by bringing to bear the masses of scientific evidence he had at his command, James decided to gently dissuade his opponent by making her see some of the inadequacies of her position.</p>
<p>&ldquo;If your theory is correct, madam,&rdquo; he asked, &ldquo;what does this turtle stand on?&rdquo;</p>
<p>&ldquo;You&rsquo;re a very clever man, Mr. James, and that&rsquo;s a very good question,&rdquo; replied the little old lady, &ldquo;but I have an answer to it. And it&rsquo;s this: The first turtle stands on the back of a second, far larger, turtle, who stands directly under him.&rdquo;</p>
<p>&ldquo;But what does this second turtle stand on?&rdquo; persisted James patiently.</p>
<p>To this, the little old lady crowed triumphantly,</p>
<p>&ldquo;It&rsquo;s no use, Mr. James—it&rsquo;s turtles all the way down.&rdquo;</p>
<p>—  J. R. Ross, Constraints on Variables in Syntax, 1967</p>
</blockquote>
<p>As a teenager, what caught my interest in programming was the question: wait, but how does it work?
It started &ndash; as is probably very common &ndash; with a game: a browser-based online game called Galactica. One day I discovered the View Source option of the browser, and the rest, as they say, is history.</p>
<p>That was how I started out with web development, using PHP, HTML, and just enough JavaScript and CSS to think that I Know Stuff. Then I switched to harder stuff: C# was my next main language. Shortly after (you know, when the rush just isn&rsquo;t the same anymore, so you need heavier stuff) I tried picking up C++. Learning C++ only by doing is very hard though: it is a supremely beginner-unfriendly language, so I didn&rsquo;t get very far with it. It was only during my Computer Science Bachelor&rsquo;s studies that I received more formal education in C++, and it became my favorite language: it allows you to go as low-level or high-level as you want. You want to take a random integer, convert it to a memory address, and write to it? Sure thing, buddy! <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> You&rsquo;d like to have a hash map mapping strings to callback functions defined for example as lambda expressions? You&rsquo;ve got it! <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>I should mention that I also have a fondness for C, but it&rsquo;s too cumbersome to work with it for any nontrivial project. Who wants to have to re-implement even trivial mechanisms like lists (or vectors, as C++ calls them) or strings? There&rsquo;s nothing really appealing about C, in my opinion, compared to C++, if you have the choice; of course, many micro-controllers and more exotic architectures only have working C compilers and toolchains.</p>
<p>For most practical purposes, there&rsquo;s just no reason to go lower level than C or C++, but of course, you can if you&rsquo;re determined: you can write Assembly. (C and C++ both support inline assembly, but that doesn&rsquo;t work for everything.) Some things you can only do in Assembly, usually things that are the responsibilities of the operating system kernel: configuring the CPU is one of them, context-switching between threads is another (since you need to switch stacks by overwriting <code>%rsp</code> on x86-64). But yeah, if C is often an inconvenient language to work in, Assembly is just torture, even with modern assemblers having added many convenience features. Generally speaking, you&rsquo;d write just a file or two in Assembly, containing the functionality you need, and call it from C (or C++ via <code>extern &quot;C&quot;</code>).</p>
<p>There&rsquo;s yet more turtles though. Assembly compiles down into machine code, which really is just highly unpleasant to even read, especially after the compiler applied optimizations to it (something that the C and C++ compilers are notoriously good at). It&rsquo;s a common idea that the CPU then executes the machine code, but this has not been true for a very long time: even machine code is just a &ldquo;high-level&rdquo; language for the CPU, except one that the CPU knows how to compile itself into <em>microcode</em>. The primary reason for this is the usual one: seeking extra performance.</p>
<p>Indeed, the most common motivations for anybody choosing to work in a language lower-level than something than C# or Java is the need to more closely control what the hardware does, often in order to ensure that it is <em>fast</em>. Game development and high-frequency trading are two good examples. But even working in C++ and doing clever things with memory only gets you so far: compiler optimizations will invoke Cthulhu and sacrifice virgin goats to turn your code into an unspeakable horror-show in the name of the performance. This involves doing things you&rsquo;d never want to do by hand, such as unrolling a loop, or creating multiple versions of the same code specialized under different run-time conditions.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>Applying optimizations on code written in unrestrictive languages such as C or C++ is no easy task though. These are the languages where the programmer is allowed to manually manipulate pointers or define (raw, untagged) unions. For optimizations to take place, the compiler must be able to make assumptions about what your code does: in general, the better the compiler understands what you are trying to do, the better and faster code it will be able to generate for you to achieve it. For these assumptions to be guaranteed to hold, you need rules, and therein lies the fundamental tension between the &ldquo;you are the boss&rdquo; philosophy of the language and the need to make rules so that the compiler can optimize your code. (This is also, sort of, the reason why code written in e.g. C# or Java can sometimes outperform common C++ code: those languages are a lot more restrictive, allowing for less fishy business, and thereby allowing the compiler and their runtime environment to have a better understanding and therefore generate faster code.)</p>
<p>This same idea applies a lot more broadly than just programming languages and compilers: if you&rsquo;ve ever written a highly performance-sensitive code as a library, you&rsquo;ve felt the struggle. On the one hand, you want your library to be easy to use, without the user having to understand your implementation details, but you also want to make sure that the performance stays optimal, which almost inevitably leads to implementation details leaking out of your elegant API: you need the user code to tell you what it wants to achieve in details that the user code fundamentally shouldn&rsquo;t really have to particularly care about but is forced to due to performance considerations. (A simple example is the size of an internal buffer to use.)</p>
<p>But, to get back to our stack of turtles, this is also why even machine code needs to be compiled: machine code often gets new and more complex or more specialized instructions to execute, such as <a href="https://stackoverflow.com/a/43845829">instructions prefixed with <code>rep</code></a> that allow an operation to be repeated more efficiently than writing a loop. These complex instructions need to be broken down into more fundamental, low-level instructions that is practical to build hardware circuitry for, but which may be specific even to particular CPU models. The opposite also happens: sometimes multiple machine code instructions are actually compiled into a single microcode instruction, called <a href="https://stackoverflow.com/a/56413946">macro-fusion</a>.</p>
<p>Microcode by the way can receive updates, when CPU vendors need to fix or work around bugs in the hardware (I know right), or to fix or mitigate security vulnerabilities. There have been many examples for the latter in the last 5 years, <a href="https://lock.cmpxchg8b.com/zenbleed.html">Zenbleed</a> (<a href="https://news.ycombinator.com/item?id=36848680">Hacker News thread</a>) being the most recent one. Updates live in volatile memory (usually the RAM), and so it must be <a href="https://wiki.gentoo.org/wiki/Microcode">loaded on every boot</a>, either by BIOS/UEFI, or in some cases, by the OS kernel directly.</p>
<p>Do we have an bigger turtle that underlies this microcode? We definitely do have at least
<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> one more: they are called <a href="https://en.wikipedia.org/wiki/Logic_gate">logic gates</a>, the fundamental primitives that ultimately underlie all instruction execution. Logic gates usually operate on just individual bits: a 0 (logical false) or 1 (logical true). They are very simple beasts, and there are not that many of them. The simplest ones are: <code>not</code> (flips a bit from 0 to 1 or from 1 to 0), <code>and</code> (yields 1 if both input bits are 1, else 0), and <code>or</code> (yields 1 if either input bits are 1, else 0).</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Of course, caveats apply, mostly revolving around <em>undefined behaviour</em>. The compiler needs to have a good understanding of what your code is trying to do in order to be able to properly optimize it. If you violate its assumptions, bad things happen: if you&rsquo;re lucky, your application will just crash. If you&rsquo;re not lucky, you get impossible-to-track down, strange, once-in-a-while odd behaviour in a completely unrelated part of the program.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>The C++11, 17, and 20 standards have added lots of convenience features to the language and standard library alike. C++03 and earlier are just sadness: if you are forced to work with them, my condolences!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>One example of this is <em>loop unswitching</em> where a loop containing a branch whose condition is invariant in a loop is pulled out of the loop, turning <code>while (true) { if (flag) do_a(); else do_b(); }</code> into <code>if (flag) { while (true) do_a(); } else { while (true) do_b(); }</code> if it can be proven that the value of <code>flag</code> cannot be affected by either <code>do_a()</code> or <code>do_b()</code>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>In between microcode and logic gates there are definitely some more layers of turtles, which I don&rsquo;t have a good idea about: it&rsquo;s the realm of hardware implementation.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/22/us-lawmakers-trading.html">US lawmakers trading and stock ownership</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/22/us-lawmakers-trading.html" class="u-url"><time class="dt-published" datetime="2023-07-22 01:43:15 &#43;0200">Jul 22, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://www.wsj.com/articles/senators-to-propose-ban-on-u-s-lawmakers-executive-branch-members-owning-stock-6db6411">Senators to propose ban on US lawmakers and executive branch members owning single stocks</a>, once again. The <a href="https://news.ycombinator.com/item?id=36785467">Hacker News thread</a> is also quite informative, <a href="https://news.ycombinator.com/item?id=36786367">for example</a>:</p>
<blockquote>
<p>I started building out tools to track congressional stock trading in 2020.</p>
<p>Since then, I believe there have been 9 other proposals similar to this one. None of them have been even called to a vote.</p>
<p>It seem like Congress is pretty unwilling to regulate its own trading.
Until they are, feel free to track the trading here: <a href="https://www.quiverquant.com/congresstrading/">www.quiverquant.com/congresst&hellip;</a></p>
</blockquote>
<p>Like <a href="https://news.ycombinator.com/item?id=36785705">many others</a>, I feel quite disillusioned and uninspired by this:</p>
<blockquote>
<p>All show and no go.</p>
<p>Like most such &ldquo;reforms&rdquo;, it is intended for naive public consumption while also being easily circumvented.
Setup a trust or corporation run by a family member and continue with business a usual.</p>
</blockquote>
<p>Senators being allowed to do what is for all intents and purposes insider trading, is frankly, completely insane. Pelosi is perhaps the most egregious example: <a href="https://nypost.com/2022/10/05/house-speaker-nancy-pelosi-has-accrued-millions-from-husbands-trades-report/">House Speaker Nancy Pelosi has accrued millions from husband’s trades: report </a>:</p>
<blockquote>
<p>The speaker, one of the richest members of Congress, has vehemently denied sharing any information with her spouse — a venture capitalist.</p>
<p>However, many have questioned trades made by Paul Pelosi that happened to coincide with major congressional decisions.</p>
<p>In June, he exercised call options to purchase up to $5 million in the graphics card manufacturer Nvidia just weeks before the House considered a bill to provide more than $50 billion in subsidies to domestic semiconductor manufacturers, the Beacon said.</p>
</blockquote>
<p>Yeah. She&rsquo;s not going to jail. So I&rsquo;d strongly support efforts to curtail this, but I just do not believe that you actually can effectively do so in practice. Ultimately, the forms of power are interchangable: if you&rsquo;re influential, for example because you&rsquo;re in Congress, then you&rsquo;ll always have easy opportunities to get (more) rich off of it, and also vica versa.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/22/financial-news-and.html">Financial news and moving the markets</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/22/financial-news-and.html" class="u-url"><time class="dt-published" datetime="2023-07-22 01:25:19 &#43;0200">Jul 22, 2023</time></a>

		<div class="e-content">
			 <p>Daring Fireball wrote about the <a href="https://daringfireball.net/2023/07/apple_gpt_bloomberg">Apple GPT news</a>. I think the news itself is pretty uninteresting: Apple wants in on the ChatGPT gold-rush even as it seems to be winding down. The more interesting part is what he writes about Bloomberg:</p>
<blockquote>
<p>Bloomberg reporters are evaluated and receive bonuses tied to reporting market-moving news. They’re incentivized financially to make mountains out of molehills, and craters out of divots, to maximize the immediate effect of their reporting on stock prices. And Bloomberg appends these stock price movements right there in their reports, to drive home the notion that Bloomberg publishes market-moving news, so maybe you too should spend over $2,000 per month on a Bloomberg Terminal so that you can receive news reports from Bloomberg minutes before the general public, and buy, sell, and short stocks based on that news.</p>
<p>&hellip;</p>
<p>Apple’s brief 2.7 percent jump and Microsoft’s smaller but still-significant drop, both at 12:04pm, were clearly caused by Gurman’s report. Bloomberg Terminal subscribers get such reports before anyone else. [&hellip;] most of their original reporting is delivered with the goal of moving the stock prices of the companies they’re reporting on, for the purpose of proving the value of a Bloomberg Terminal’s hefty subscription fee to day-trading gamblers [&hellip;]</p>
</blockquote>
<p>Tinfoil hat time? I mean&hellip; that&rsquo;s quite the stretch. Even if Bloomberg doesn&rsquo;t report particular news, a million other websites will; how does Bloomberg move the market in particular?
Bloomberg is hardly the only company selling ahead-of-the-crowd services; it&rsquo;s obviously a lucrative thing to do successfully, given the herd mentality that all markets follow<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
They obviously want to communicate news that have <em>the potential</em> of moving stocks, otherwise they&rsquo;d not be a good source of finance news!</p>
<p>As for making mountains out of molehills, I think that&rsquo;s a fair criticism to an extent, but it&rsquo;s also not one that is in any way unique to Bloomberg. Clickbait and blowing news out of proportion is just business as usual for mass media these days, thanks to the advertising industry financing a large chunk of websites and competition <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">fierce enough to trigger</a> a race to the bottom with low-effort reporting. My personal impression is that Bloomberg is actually better than many in this regard.</p>
<p>By the way, the Apple stock <em>stayed up</em> after these news (see the <a href="https://www.investing.com/equities/apple-computer-inc">Investing.com chart</a>), only dropping back to previous levels some 24 hours later when manufacturing issues were reported with the upcoming iPhone 15. So it was <em>good reporting</em> by Bloomberg.</p>
<!-- raw HTML omitted -->
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Predicting how a market will move though is pretty much hopeless in the general case, as financial markets are a good example of a chaotic system in that it responds not just to events, but predictions and expectations as well, and does so in a recursive, self-predicting way: if enough people think that upon some news others will want to buy a particular stock, then the stock will move on the news. This is one of the factors that makes markets very &ldquo;noisy&rdquo;: prices move very frequently, even for large, fairly stable stocks.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/15/outofmemory-while-trying.html">Out-of-memory while trying to free it: into virtual memory</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/15/outofmemory-while-trying.html" class="u-url"><time class="dt-published" datetime="2023-07-15 03:10:14 &#43;0200">Jul 15, 2023</time></a>

		<div class="e-content">
			 <p>I recently came by this fun story on getting an out-of-memory error on Linux when trying to <em>free</em> memory: <a href="https://ayende.com/blog/199649-B/production-postmortem-enomem-when-trying-to-free-memory?Key=c9261116-c324-4280-a1e1-35da0d1fa882">Production postmortem: ENOMEM when trying to free memory</a></p>
<blockquote>
<p>That error made absolutely no sense, as you can imagine. We are trying to release memory, not allocate it. Common sense says that you can’t really fail when you are freeing memory. After all, how can you run out of memory? I’m trying to give you some, damn it!</p>
</blockquote>
<p>The linked blog post does a good job of explaining the problem briefly and clearly, so I&rsquo;m just going to provide some further context on how this can indeed be a problem.</p>
<p>Modern computer architectures like x86-64 and even most (all?) the ARMs have a lot funkier relationship with memory than most people &ndash; and even most software developers! &ndash; tend to be aware of. (This is a good thing! You don&rsquo;t need to know how to sausage is made. Abstraction is an invaluable tool for managing complexity.)</p>
<p>The memory that a running application (any running application) sees and lives in (where it loads all of its data and where all of its allocations live) is actually not a direct representation of the main memory of the computer (the RAM sticks). It used to be, long ago, but we&rsquo;ve moved away from that. Instead, what your process is interacting with is called <em>virtual memory</em>, or more specifically, a <em>virtual memory address-space</em>. Virtual memory is a layer of abstraction on top of <em>physical memory</em>, a decoupling useful because it grants additional performance, stability, and safety. How?</p>
<p>A process&rsquo;s virtual memory address-space is constructed specifically for that one particular process, and reflects <em>only</em> the parts of the physical memory that are used by it. Memory used by other processes is normally not visible here: it is simply not mapped (outside of special cases like shared memory created by <a href="https://man7.org/linux/man-pages/man3/shm_open.3.html"><code>shm_open()</code></a> and friends or debugging via <a href="https://man7.org/linux/man-pages/man2/ptrace.2.html"><code>ptrace()</code></a>). This means that, by default, no process can accidentally or maliciously access the memory of other processes and extract secrets from it or change the values in unexpected ways leading to erratic behaviour or crashes.</p>
<p>This layer of abstraction also enables other useful features that reduce overall memory usage at the expense of runtime performance:</p>
<ul>
<li>Demand-paging: memory allocated but not yet accessed doesn&rsquo;t actually need to be available: if the application decided to allocate a gigantic 2 GB buffer but hasn&rsquo;t yet used more than 100 KB of it, then the rest doesn&rsquo;t have to be available</li>
<li>Swapping: rarely used regions of memory are written to disk, and the memory it was used for is made available for new allocations</li>
<li>Compression: rarely used regions of memory are compressed, and the space freed up is reclaimed for other purposes</li>
<li>De-duplication: if there are multiple identical copies of the sequence of bytes in memory, we don&rsquo;t actually need to store it multiple times (though caveats apply!)</li>
</ul>
<p>For any physical memory to be accessible in a virtual memory address space, it has to be mapped in, typically by the operating system kernel. This is done by writing into a data structure called the <em>page table</em>, which is shared between the kernel and the CPU&rsquo;s memory management unit (MMU). A <em>page table entry</em> typically represents the mapping of some physical memory region into a virtual memory address space: in this case, the entry will contain the physical memory address that it corresponds to. You may have multiple page table entries refer to the same or overlapping regions of physical memory: that is totally cool and allowed! (This has some very interesting uses e.g. in cybersecurity that I wrote <a href="https://github.com/shdnx/dangless-malloc">my Master&rsquo;s thesis on</a>.)</p>
<p>A page table entry, beyond the physical memory address, also contains bits used for purposes. Some of these are used for protection, to indicate whether the region is readable, writable, or executable. This is enforced by the MMU: attempting to write to a virtual memory region whose page table entry does not have the writable bit set will fail (well, it will trigger a page fault, which can be handled &ndash; more on this later). For example, after a running application&rsquo;s executable is loaded into memory at startup, the region is marked as non-writable (but readable and executable) to prevent buggy software from accidentally modifying its own code and also to limit the options of malware. Similarly, memory regions where the application&rsquo;s runtime data is stored (so the stack, the heap, but also the .data, .bss, and .rodata sections containing global variables) are marked as non-executable, as that is a very easy attack vector for malicious users.  (They can be made executable, otherwise things like Jit-in-Time compilers would not work, but you have to explicitly request it. e.g. on Linux by using <code>mmap()</code> with <code>PROT_EXEC</code>.)</p>
<p>The page table entry also contains a &ldquo;dirty&rdquo; bit. This is set the MMU whenever that particular memory region is used. The kernel can periodically clear this bit, and then check later if the bit is set: if not, then the corresponding memory region has not been used recently, and so may be candidate to be swapped out or compressed.</p>
<p>We need a mechanism though for detecting when e.g. a swapped-out piece of memory is accessed. Clearly, such an access cannot simply proceed as normal: the page table entry is marked as invalid, there is no physical memory backing it. The MMU will trigger a <em>page fault</em> in this case, which is an interrupt that the kernel handles. The kernel will consult its own internal book-keeping of the process&rsquo;s memory and will find in this case that it has decided to swap the memory to disk: now, since the data is requested, the data must be loaded back from disk into main memory and the page table entry must be made valid again. Once this is done, the process resumes execution from the exact same place as it was left suspended when trying to access swapped memory, none the wiser to what happened in the background &ndash; except perhaps that some operation ending up having taken much more time than usual.</p>
<p>This pattern is actually very common, and is used for all of the memory-use optimizations I&rsquo;ve mentioned so far. For instance, in the case of memory de-duplication, identical memory regions may be referenced by multiple processes at the same time. So long as they all only read from it, life is good, but  when one of them wants to write to it, the kernel needs to intervene, as the write would make the memory region no longer contain what the other processes expect it to. This is achieved by marking the page table entries as non-writeable, such that attempting to write results in a page fault. On such page  fault, the kernel quietly duplicates the memory region (by allocating a new one and then copying into it) and updates the writing process&rsquo;s page table entry to point to the new address, restoring its writeability before returning control to the process.</p>
<p>You may have noticed that I was talking about individual bits of the page table entry. Indeed, these entries are extremely limited in size: typically just 64 bits (8 bytes), most of which is needed to store the referenced physical memory address. This is not always enough for more complicated features built on top of the virtual memory abstraction, so the kernel has additional book-keeping: in Linux, these are called <em>virtual memory areas</em> or VMAs for short: one for each distinct memory region of each running process. For reasons of simplicity and performance, these VMAs are stored in a large pre-allocated array with a fixed capacity.</p>
<p>This takes us back to the blog post that motivated this write-up: freeing or de-allocating memory may require an additional VMA to be allocated in the case where you&rsquo;re not freeing an entire allocated memory region, but only a part of it. This is not possible to do via the commonly used <code>malloc()</code> / <code>free()</code> APIs or even the C++ <code>new</code> and <code>delete</code> equivalent operators, but it is possible to do via direct system calls using <code>mmap()</code> and <code>munmap()</code>. To illustrate:</p>
<ol>
<li>We allocate 1000 bytes of memory</li>
<li>Linux creates a new VMA for us to record this (note: no actual memory has been allocated unless you used <code>MAP_POPULATE</code> as demand paging kicks in!)</li>
<li>We now wish to free the middle 500 bytes</li>
<li>Linux now has no choice but to split the original one VMA spanning 1000 bytes into two of 250 bytes each, with a 500 byte gap in the middle</li>
</ol>
<p>Granted, this is a bit contrived, and not even accurate as you cannot free 500 bytes: memory is split into pages of 4096 bytes (4 KB) each (by default, on x86-64). But you can imagine the same situation happening when e.g. different protection bits are applied to different parts of a huge memory region: some part of writeable, other parts are not. The VMA having to be split causes the memory deallocation operation to actually have to allocate memory: for the new VMA. This is how you can get an out-of-memory error.</p>
<p>As you can imagine, there are a lot more details to this topic. Some teasers:</p>
<ul>
<li>A single virtual memory address space can span up to 256 terrabytes of memory, making virtual memory much more plentiful than physical memory.</li>
<li>Resolving virtual memory addresses into physical memory addresses is slow even when performed by dedicated hardware: modern CPUs spend something like 25% of their total execution time just doing this.</li>
<li>Caching (the TLB) is used to mitigate this, and while its hit-rate is very high (96%+ in typical applications), it opens up timing-related side-channel attacks that have become well-known in recent years.</li>
<li>Huge pages of 2 MB and 1 GB are another effort to improve performance, but are a mess, and difficult to work with in practice.</li>
</ul>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/15/brought-down-by.html">Brought down by the font</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/15/brought-down-by.html" class="u-url"><time class="dt-published" datetime="2023-07-15 01:11:22 &#43;0200">Jul 15, 2023</time></a>

		<div class="e-content">
			 <p>Oh wow, now <em>this</em> is something: <a href="https://www.theverge.com/2017/7/12/15961354/pakistan-calibri-font-scandal-forged-documents">A Microsoft font may have exposed corruption in Pakistan</a>:</p>
<blockquote>
<p>The Microsoft font Calibri is now a key piece of evidence in a corruption investigation surrounding Pakistan’s prime minister. Investigators noticed that documents handed over by the prime minister’s daughter, Maryam Nawaz Sharif, were typed up in the font Calibri. But the documents were dated from 2006 — and Calibri wasn’t widely available at that point, making a good case that they were forged.</p>
</blockquote>
<p>I love everything about this, but mostly just the fact that there are people know and care enough about fonts of all things to be able to point things like this out. This is true digital forensics. I almost feel sorry for Pakistani Prime Minister Nawaz Sharif! Not enough though to keep me from highlight this amazing snippet of peak journalism from Arstechnica&rsquo;s slightly more detailed article <a href="https://arstechnica.com/tech-policy/2017/07/not-for-the-first-time-microsofts-fonts-have-caught-out-forgers/">Not for the first time, Microsoft’s fonts have caught out forgers</a>:</p>
<blockquote>
<p>Ultimately, the fallout of the corruption and cover-up is that Pakistan may soon, like Calibri itself, be sans-Sharif.</p>
</blockquote>
<p>The article goes on to note that this hasn&rsquo;t been the first time that fonts have been the demise of the mighty:</p>
<blockquote>
<p>Other Word features have caught out forgers, too. The &ldquo;Killian documents,&rdquo; which claimed President George W. Bush was declared unfit for service during his time with the Air National Guard, purported to have been produced on a typewriter in 1973. However, those documents used proportional fonts and curly quotes, making it spectacularly unlikely that they were authentic. Standard 1973 vintage typewriters didn&rsquo;t offer either proportional fonts or curly quotes, but 2004-vintage Microsoft Word did both.</p>
</blockquote>
<p>But yeah, apparently there are indeed people that both know and care a lot about fonts, such as <a href="https://www.youtube.com/watch?v=cJYm-de_UHE">this guy who researched the market share of fonts used for memes</a>. Peak Friday evening material.</p>
<p>This whole rabbit hole was enabled by the <a href="https://news.ycombinator.com/item?id=36719987">Hacker News discussion</a> on Microsoft&rsquo;s new Aptos font, aimed to replace Calibre as the default, which I only even clicked on because I thought they were referring to <a href="https://calibre-ebook.com/">Calibre the ebook management software</a> that I use.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/12/writing-a-popular.html">Writing a popular blog must be dreadful</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/12/writing-a-popular.html" class="u-url"><time class="dt-published" datetime="2023-07-12 01:38:49 &#43;0200">Jul 12, 2023</time></a>

		<div class="e-content">
			 <p>In <a href="https://astralcodexten.substack.com/p/why-do-i-suck">Why do I suck</a>, Scott Alexander is analyzing why his earlier articles (from 2013 - 2016) were much more popular than his recent ones.</p>
<p>I would like to highlight his insight into the perception difference between a small blog and a big one:</p>
<blockquote>
<p>If you have a small blog, and you have a cool thought or insight, you can post your cool thought or insight. People will say “interesting, I never thought of that before” and have vaguely positive feelings about you. If you have a big blog, people will get angry. They’ll feel it’s insulting for you to have opinions about a field when there are hundreds of experts who have written thousands of books about the field which you haven’t read. Unless you cite a dozen sources, it will be “armchair speculation” and you’ll be “speaking over real academics”. If anyone has ever had the same thought before, you’re plagiarizing them, or “reinventing the wheel”, or acting like a “guru”, or claiming that all knowledge springs Athena-like from your head with no prior influences.</p>
<p>I try really hard to block or ignore these people when I spot them, but they do a little bit of psychic damage each time.</p>
</blockquote>
<p>I wonder how much of that is simply bigger audience = worse audience, given <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">regression to the mean</a>. Basically: when your blog has 100 readers, it is not difficult for this 100 to be &ldquo;reasonable people&rdquo; (from you, the blog author&rsquo;s, perspective), but if you have 10 000, then it is much harder: as the size of a group grows, the more it tends to resemble the population at large. The population as a whole contains a whole lot of people who suck! (This seems universally true, regardless of what group you identify with.)</p>
<p>Another perspective on the same idea: out of 10 000 people, you&rsquo;re much more likely to annoy / step on the toes of somebody by something you write than out of 100 people. 10 000 people will also likely contain some trolls and griefers, who just tend to make things worse.</p>
<p>The other possible explanation is authority. A blog with few readers has little to no authority on any subject, and therefore is largely safe from the kind of scrutiny that having authority (even informal!) invites. So you can get away just expressing your opinions in a way that makes sense to you. Doing so when you have a large audience is risky, expectations are higher, and the whole thing becomes a lot more high-stakes.</p>
<p>Taking this further, what does this mean for official authorities like the CDC or the FED? Obviously that&rsquo;s kind of as high-stakes as it gets, but beyond that, they are very constrained in what they can actually say if they want people to actually take them seriously. They can&rsquo;t just say things that make sense to them! It needs to be as unambiguous as possible, supported by research and data.</p>
<hr>
<p>As an aside, I find it interesting that one of his plausible explanations is just that media overall is less bad today than it used to be. Not often do you see this proposed!</p>
<blockquote>
<p>Nowadays I think there are many good science bloggers, and the media has gotten embarrassed enough times that it will sometimes run a take by someone who knows what they’re talking about before publishing it.</p>
<p>In the same way, I see fewer people outright denying the existence of genetics, totally failing to understand AI risk, or utterly bungling basic concepts in risk and probability.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/08/regulation-unexpected-consequences.html">Regulation, unexpected consequences of (Sesame edition)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/08/regulation-unexpected-consequences.html" class="u-url"><time class="dt-published" datetime="2023-07-08 23:43:54 &#43;0200">Jul 8, 2023</time></a>

		<div class="e-content">
			 <p>Matt Levine writes the amazing <a href="https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine">Money Stuff</a> newsletter for Bloomberg. I&rsquo;ve been a reader of his for a few years now, and I&rsquo;m a huge fan of his writing. He recently wrote about the <a href="https://www.bloomberg.com/opinion/articles/2023-06-21/big-firms-want-normal-crypto-markets">new US legislation on sesame seeds</a> (search for the title &ldquo;Sesame&rdquo;) that I found hilarious:</p>
<blockquote>
<p>Congress passed legislation intended to make life better for people allergic to sesame seeds. Instead, it made things worse.
The bill, passed with overwhelming bipartisan support and signed into law by President Biden in 2021, requires manufacturers to label sesame on their products starting this year.</p>
<p>In response, some companies began adding sesame to products that hadn’t included it in the past—saying it was safer to add sesame and label it, rather than certify they had eliminated all traces of it.</p>
</blockquote>
<p>Why would they do that? Clearly, if anything, that&rsquo;s the opposite of what the new regulation was intended for. Well:</p>
<blockquote>
<p>The issue is that it is hard to eliminate trace amounts of sesame, and the law now requires food manufacturers to label sesame as an allergen. Not putting sesame on the label effectively constitutes a promise that there is no sesame in the product, and if there is a little bit then you get in trouble: [&hellip;]</p>
<p>But if you say that the product definitely contains sesame, then you are immunized from trouble. So you just chuck some sesame into everything, change the labels, and you’re fine. It is easier to make sure that there is sesame than that there isn’t, so that’s what companies do.</p>
</blockquote>
<p>Unintended consequences! It <em>makes sense</em> that companies do this, given their incentives. A friend of mine put it as &ldquo;regulators just do not think like hackers&rdquo;, which is not how I&rsquo;d put it, but I do see his point: regulators should have known better than to do this. Don&rsquo;t they consult with experts and people in the industry? Someone would have definitely pointed this out to them.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/06/the-twitterkiller-metas.html">The Twitter-killer? Meta&#39;s new Threads app</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/06/the-twitterkiller-metas.html" class="u-url"><time class="dt-published" datetime="2023-07-06 00:29:43 &#43;0200">Jul 6, 2023</time></a>

		<div class="e-content">
			 <p>Facebook (err, sorry, Meta) is launching their Twitter competitor today in the US in the form of a new stand-alone app called <a href="https://apps.apple.com/us/app/threads-an-instagram-app/id6446901002">Threads</a>.</p>
<p>The Verge has a good summary on what is known: <a href="https://www.theverge.com/2023/7/5/23784480/threads-instagram-meta-news-twitter-competitor">Instagram’s Threads: all the updates on the new Twitter competitor</a>. I can also recommend the <a href="https://news.ycombinator.com/item?id=36580192">Hacker News thread</a>; it has received quite a bit of attention and drew a lot of discussion.</p>
<blockquote>
<p>As Twitter continues to flail about under Elon Musk, all eyes are on Instagram Threads as a potential replacement.</p>
</blockquote>
<p>Twitter&rsquo;s story with getting bought by Elon Musk is truly wild, and semi-seriously could be turned into a movie. Matt Levine wrote a lot about it in his excellent Money Stuff, and it was all great fun, truly peak 21st century amusement. Since the acquisition, Twitter has arguably received more attention than before, but also a lot more criticism, and is <a href="https://www.dexerto.com/tech/twitter-users-claim-elon-musks-recent-changes-may-be-self-ddosing-the-platform-2196413/">clearly struggling</a>, <a href="https://daringfireball.net/linked/2023/07/03/everything-continues-to-be-going-just-great-at-twitter">dear lord</a>, with even <a href="https://mashable.com/article/twitter-api-elon-musk-developer-issues-apps">high-paying customers being ignored</a>.</p>
<p>Elon Musk himself has stated multiple times that bankruptcy in the near future was possible. It remains to be seen whether these are the kind of temporary issues that can be expected from a fairly dramatic shift in leadership and strategy, or perhaps the beginning of the end of Twitter&rsquo;s prevalence.
Either way, this is excellent timing from Meta, and I expect they have a good chance of capitalizing on Twitter&rsquo;s plight.</p>
<blockquote>
<p>From what we know so far, Threads is “Instagram’s text-based conversation app” where “communities come together to discuss everything from the topics you care about today to what’ll be trending tomorrow.” The app is closely tied to Instagram, meaning you’ll get to use the same username across both apps, as well as quickly follow all of the accounts you’ve been following on Instagram.</p>
</blockquote>
<p>Sounds a lot like Twitter, i.e. a more social version of micro-blogging. Reportedly it will even have two-way <a href="https://joinmastodon.org/">Mastodon</a> integration, which is interesting and again a good move: Mastodon gained some popularity following Twitter&rsquo;s, uh, adventures, and by embracing it, Threads can attempt to woo Twitter refugees more effectively than by <em>not</em> integrating&hellip; not to mention, it&rsquo;s a greater blow to Twitter.</p>
<blockquote>
<p>The Twitter alternative from Meta appears set to launch on July 6th, but the Irish Independent reports that Ireland’s Data Protection Commission has been in contact with the company about the new product and confirmed the launch won’t extend to the EU “at this point.”</p>
</blockquote>
<p>GDPR strikes again! I will be curious to see what happens here; no doubt there&rsquo;s an element of power play here on the part of Meta. GDPR and other privacy legislation in the EU are clearly anti-Meta; Meta is probably evaluating whether it makes sense to even bother with it.</p>
<hr>
<p>An interesting perspective to take is that Threads appears to be very much like Facebook&rsquo;s own Wall, where people can write posts, upload photos, and engage with people in the comments. Facebook&rsquo;s own shift towards providing something more like a news feed has reduced the Wall&rsquo;s relevance a lot, but the Threads can potentially be its reincarnation in a purer form.</p>
<p>Threads is cleverly branded under Instagram rather than Facebook or Meta to leverage its better reputation. Meta knows that as far as associations go, people (at least in much of the US and Europe) tend to like and trust Instagram, but tend to at least distrust Facebook or Meta. Many, many people still aren&rsquo;t really aware that Instagram, and even WhatsApp are owned and operated by Meta now! They&rsquo;d very much like to keep it this way.</p>
<p>Branding- and image-wise, Meta is in an interesting situation. On the one hand, both in the United States and much of Europe, Facebook is generally a disliked and mistrusted brand, following lots and lots of bad publicity from, well, being Facebook and doing Facebook things. On the other hand, they still have a metric ton of daily active users, passing <a href="https://www.statista.com/statistics/346167/facebook-global-dau/">2 billion recently</a>, so clearly they are, uh, successful. Sadly.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/04/quantum-immortality-explored.html">Quantum immortality</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/04/quantum-immortality-explored.html" class="u-url"><time class="dt-published" datetime="2023-07-04 19:49:34 &#43;0200">Jul 4, 2023</time></a>

		<div class="e-content">
			 <p>Quantum immortality is a thought experiment that runs roughly like follows:</p>
<ol>
<li>If the many-worlds interpretation of quantum mechanics is correct, each &ldquo;choice&rdquo; (however this may be defined) creates a branching point: there will be a universe that explores each possibility. Simply put, if at an intersection you can choose to go left or right, in one universe you&rsquo;ll always go left, and in another you&rsquo;ll always go right, and the history of each universe will then evolve independently, and continue branching along each subsequent choice.</li>
<li>Which one of these many many universes will <em>you</em> (the awareness reading this text) experience? You are clearly in one of them: your awareness does not span universes. (At least, I would expect so. My apologies if it does! Please drop me an e-mail, I&rsquo;ll buy you a coffee.)</li>
<li>A person, pretty much by definition, is incapable of experiencing their own death. So you are clearly experiencing a universe in which you are not dead. (Again, my honest apologies if you are! Please drop me an e-mail, let&rsquo;s chat.)</li>
<li>What if this rule is universal? What if your awareness will always continue along a path of these possible universes where you remain alive (or perhaps just aware), however unlikely that may be?</li>
</ol>
<p><strong>Disclaimer</strong>: <em>This is purely a thought experiment. Do not take this seriously. As far as anyone can tell, you only have one life: do not throw it away.</em></p>
<p>This idea has some interesting consequences! The following is a short story by Robert Charles Wilson, originally published in 1998 (!), that explores this:</p>
<h3 id="divided-by-infinityhttpswwwtorcom20100805divided-by-infinity"><a href="https://www.tor.com/2010/08/05/divided-by-infinity/">Divided by Infinity</a></h3>
<hr>
<p>As I said, interesting idea! It would imply that you get away with any crazy shit! For starters:</p>
<ol>
<li>Buy a lottery ticket.</li>
<li>Set up some mechanism that will kill you immediately and without warning, should it turn out that you have <strong>not</strong> won the lottery.</li>
<li>Profit??</li>
</ol>
<p>Obviously this is not fool-proof, and, err, certainly not lifestyle advice! This would just be testing the likelihood of whatever mechanism you have set up failing against the likelihood of you winning the lottery. Examples of how this may come to be include but are not limited to:</p>
<ul>
<li>If you hire somebody to kill you, they will have an accident or something else that prevents them from completing their task.</li>
<li>If you set up some kind of machine or contraption, it will fail. The more fail-safe you make it, it will start failing in increasingly less plausible ways.</li>
<li>The lottery gets cancelled.</li>
<li>You win the lottery, but after the fact, your ticket turns out to be invalid or counterfeit.</li>
</ul>
<p>Another interesting consequence of this thought experiment is that it only confers &ldquo;immortality&rdquo; to <em>your own subjective awareness</em> or viewpoint. It does not protect others! So if you watch somebody else try e.g. the above, the result will most likely be the predictable one: they die. In <em>your</em> perspective that is, or rather, in the universe that you are experiencing. In <em>their</em> subjective perspective, they will survive; it will just not be a universe that you can ever experience or interact with.</p>
<p>Finally, we have been talking about <em>immortality</em>, but the premise does not actually promise that. It only promises a <strong>continuity of awareness</strong>. This does not protect against you suffering a horrible accident that will leave you alive and aware, but miserable! The possibilities start with the run-of-the-mill horribleness that comes from inhabiting a physical body (e.g. going blind or becoming totally paralyzed), but actually go much further: you could find yourself at the mercy of aliens experimenting on you, or your consciousness (and awareness) being uploaded into a computer and then being subjected to&hellip; whatever.</p>
<p>I will leave this here before it gets any darker. :)</p>
<p>To end this post on a lighter note, here is an only tangentially related but fun short story video about a guy using a One-minute Time Machine to try to pick up a woman. Enjoy:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/CXhnPLMIET0' frameborder='0' allowfullscreen></iframe></div>
<p>(Thank you Melvin for the recommendation!)</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/01/light-and-fun.html">Light and fun fiction: Idol Words</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/01/light-and-fun.html" class="u-url"><time class="dt-published" datetime="2023-07-01 18:18:28 &#43;0200">Jul 1, 2023</time></a>

		<div class="e-content">
			 <p>At this point my blog has been alive for a few days now, and so it&rsquo;s high time I mentioned Scott Alexander, and his blog(s): the Star Slate Codex and Astral Codex Ten. He writes about complex topics in a refreshingly sober, straight-forward and illuminating manner. His articles will most definitely make a regular appearance here.</p>
<p>To start with, I would recommend a light and short piece of fiction he wrote: Idol Words. It&rsquo;s more a series of short scenes rather than a short story.
It semi-seriously explores the idea of three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly, with each &ldquo;scene&rdquo; featuring another person coming along to ask them a question. What kind of people would ask what kind of questions? The themes are perhaps predictable, but no less interesting or satisfying to read.</p>
<p>Enjoy:</p>
<h1 id="idol-wordshttpsastralcodextensubstackcompidol-words"><a href="https://astralcodexten.substack.com/p/idol-words">Idol Words</a></h1>
<blockquote>
<p>It was another boring day as the keeper of the three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/30/im-very-excited.html">The Three Body Problem</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/30/im-very-excited.html" class="u-url"><time class="dt-published" datetime="2023-06-30 00:44:10 &#43;0200">Jun 30, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;m very excited for the upcoming The Three Body Problem series!
I&rsquo;ve read and thoroughly enjoyed <a href="https://www.goodreads.com/book/show/20518872-the-three-body-problem">the book trilogy</a>, and it looks like they are sticking decently close to the source material. This must be a challenging story to put on screen!</p>
<p>The book&rsquo;s cover text:</p>
<blockquote>
<p>Set against the backdrop of China&rsquo;s Cultural Revolution, a secret military project sends signals into space to establish contact with aliens. An alien civilization on the brink of destruction captures the signal and plans to invade Earth. Meanwhile, on Earth, different camps start forming, planning to either welcome the superior beings and help them take over a world seen as corrupt, or to fight against the invasion.</p>
</blockquote>
<p>You can watch the teaser trailer for the series on YouTube:
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/5lj99Uz1d50' frameborder='0' allowfullscreen></iframe></div></p>
<p>The books form a trilogy, so the series will have plenty of material to work with!
My favorite book was the second, titled <a href="https://www.goodreads.com/book/show/23168817-the-dark-forest">The Dark Forest</a>, with a host of exciting new ideas, including a to me previously unknown theoretical explanation for the <a href="https://en.wikipedia.org/wiki/Fermi_paradox">Fermi Paradox</a>.</p>
<p>As an aside, this short video gives a brief explanation on the title-giving N-body Problem: that is, we can model the behaviour of a gravitational system with 2 objects, but we are incapable of doing so for 3 or more objects. That&rsquo;s kind of a problem, especially if you happen to live in a solar system with, let&rsquo;s say, 3 stars&hellip; :)
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/D89ngRr4uZg' frameborder='0' allowfullscreen></iframe></div></p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/30/inside-the-ai.html">Inside the AI factory: the human element of AIs</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/30/inside-the-ai.html" class="u-url"><time class="dt-published" datetime="2023-06-30 00:37:48 &#43;0200">Jun 30, 2023</time></a>

		<div class="e-content">
			 <p>A fascinating deep-dive from New York Magazine and The Verge into how AI models &ndash; including ChatGPT, Bard, and other LLMs (Large Language Models) &ndash; are trained, and the industry that provides the data, control, and feedback:</p>
<h2 id="inside-the-ai-factoryhttpsnymagcomintelligencerarticleai-artificial-intelligence-humans-technology-business-factoryhtml"><a href="https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html">Inside the AI factory</a></h2>
<blockquote>
<p>[&hellip;] ChatGPT seems so human because it was trained by an AI that was mimicking humans who were rating an AI that was mimicking humans who were pretending to be a better version of an AI that was trained on human writing.</p>
<p>This circuitous technique is called “reinforcement learning from human feedback,” or RLHF, and it’s so effective that it’s worth pausing to fully register what it doesn’t do. When annotators teach a model to be accurate, for example, the model isn’t learning to check answers against logic or external sources or about what accuracy as a concept even is. The model is still a text-prediction machine mimicking patterns in human writing, but now its training corpus has been supplemented with bespoke examples, and the model has been weighted to favor them. Maybe this results in the model extracting patterns from the part of its linguistic map labeled as accurate and producing text that happens to align with the truth, but it can also result in it mimicking the confident style and expert jargon of the accurate text while writing things that are totally wrong. There is no guarantee that the text the labelers marked as accurate is in fact accurate, and when it is, there is no guarantee that the model learns the right patterns from it.</p>
</blockquote>
<p>Makes sense, and it&rsquo;s so very important to recognize this! We, humans, like to anthropomorphize things, and doubly so for LLMs since they can come across so&hellip; genuinely human.</p>
<p>Generating training data for AIs though appears&hellip; distinctly not-so-fun, mainly because it is so alien. Indeed, you are training an <em>excessively dumb</em> machine:</p>
<blockquote>
<p>It is in part a product of the way machine-learning systems learn. Where a human would get the concept of “shirt” with a few examples, machine-learning programs need thousands, and they need to be categorized with perfect consistency yet varied enough (polo shirts, shirts being worn outdoors, shirts hanging on a rack) that the very literal system can handle the diversity of the real world.
[&hellip;]
Instruction writers must come up with rules that will get humans to categorize the world with perfect consistency. To do so, they often create categories no human would use. A human asked to tag all the shirts in a photo probably wouldn’t tag the reflection of a shirt in a mirror because they would know it is a reflection and not real. But to the AI, which has no understanding of the world, it’s all just pixels and the two are perfectly identical. Fed a dataset with some shirts labeled and other (reflected) shirts unlabeled, the model won’t work. So the engineer goes back to the vendor with an update: DO label reflections of shirts. Soon, you have a 43-page guide descending into red all-caps.</p>
</blockquote>
<p>I fear though that before this era is over, most of us will have a &ldquo;job&rdquo; like this one, if indeed any at all.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/29/tech-erosion-the.html">Tech Erosion: the effect of technology (and AI) on our lives</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/29/tech-erosion-the.html" class="u-url"><time class="dt-published" datetime="2023-06-29 23:02:19 &#43;0200">Jun 29, 2023</time></a>

		<div class="e-content">
			 <p>StillDrinking has a lot of great essays. He has a way with words that makes reading his articles both entertaining and illuminating at the same time &ndash; a powerful combination.</p>
<p>Anyway, their essay on AI and ChatGPT, and how the world we have built will continue manages to be human-hostile in many ways in spite of (and indeed, in part <strong>because of</strong>) technology:</p>
<h2 id="tech-erosionhttpswwwstilldrinkingorgtech-erosion"><a href="https://www.stilldrinking.org/tech-erosion">Tech Erosion</a></h2>
<blockquote>
<p>People are already losing their jobs. It’s not only the artists, whom nobody cares about until they’re gone, it’s copyeditors and clerks and designers. And just like self-checkouts and airport entry surveys, the humans are replaced by something a little bit worse. But it’s cheaper, and novelty often obscures indignity long enough for it to entrench, and we all accept that everything is a little bit slower, a little bit less trustworthy, and everything has a little more friction to grind us down over each day. The replacement bots could be honed into better tools, but who will bother once they’re accepted? Market trends always converge on giving us as little as possible.</p>
</blockquote>
<p>I&rsquo;d also be amiss if I didn&rsquo;t highlight this crucial insight into our collective fears of AIs:</p>
<blockquote>
<p>The terror of building a super artificial intelligence is not due to having something super intelligent hanging around, it is the terror of having something super intelligent that acts like a human. Because if we manage to build something technologically superior to us that also acts like us, it will do what technologically superior humans always do to their neighbors.</p>
</blockquote>
<p>Perhaps there are things that we, humanity as a whole, should just decide that we should not build &ndash; not because we cannot or because it would be evil, but because we are not sure it&rsquo;s a net positive as a whole. Of course, if we <em>were</em> able to choose so, we probably would have less to be concerned about already, given that we&rsquo;d clearly have a mechanism for informed, preventive, and collective action.</p>

		</div>
	  </div>
	

</div>


    
    

  </body>

</html>
