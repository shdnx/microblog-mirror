<!DOCTYPE html>
<html>

  <head>
	<meta name="generator" content="Hugo 0.91.2" />

  <title>
      
      The Stargazer
      
  </title>

</head>


  <body>

    

	
<div class="h-feed">

	
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/20/why-chess-bots.html">Why chess bots are virtually unbeatable</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/20/why-chess-bots.html" class="u-url"><time class="dt-published" datetime="2023-12-20 04:39:56 &#43;0200">Dec 20, 2023</time></a>

		<div class="e-content">
			 <p>A really interesting video with some details about how the Stockfish chess engine works and what makes it so good at chess:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/CdFLEfRr3Qk' frameborder='0' allowfullscreen></iframe></div>
<p>A few things that stood out to me in particular:</p>
<ul>
<li>I had no idea that it would use a neutral net to evaluate positions. I expected that this would be way too slow, and prevent it from analyzing the game to sufficient depth (i.e. a sufficient number of moves ahead) to be effective. I was wrong!</li>
<li>Once there are 7 or fewer pieces remaining in the game, the game is solved, as there are pre-generated tables about the ideal set of moves to make. This takes up about 20 terabytes apparently. Wild!</li>
<li>On the flip side, it sounds like that Stockfish doesn&rsquo;t special-case openings. I guess it doesn&rsquo;t have to?</li>
</ul>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/17/good-and-evil.html">Good and evil genies: the AI alignment problem</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/17/good-and-evil.html" class="u-url"><time class="dt-published" datetime="2023-12-17 02:01:09 &#43;0200">Dec 17, 2023</time></a>

		<div class="e-content">
			 <p>I feel like that as much hype as AI has gotten, rightfully so, the people advocating for caution and restraint, such as the <a href="https://en.wikipedia.org/wiki/Effective_altruism">Effective Altruism</a> movement, have not been very popular. Part of that is just being in the unenviable and intrinsically unpopular position of a naysayer, but I think a large part comes from not understanding very well what exactly they are afraid of, other than vague fears of a Terminator-style Skynet being created.</p>
<p>At the heart of AI safety issues is the problem of dealing with an entity that is much smarter than we are (like a true, general, wide AI is likely going to be), but has a limited or no understanding or our value and morality system. These thought experiments often think of AI as a genie that can grant wishes&hellip; but while adhering to the letter of your wish, the outcome is likely not going to be what you wanted, unless the genie somehow understands exactly what you value.</p>
<p>Eliezer Yudkowsky is one of the very smart people working on the AI safety and alignment, and he wrote <a href="https://www.readthesequences.com/The-Hidden-Complexity-Of-Wishes">The Hidden Complexity of Wishes</a> which explains the problem in a clear, concise, and entertaining way.
The Rational Animations YouTube channel has a beautifully animated version of the same article:
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/gpBqw2sTD08' frameborder='0' allowfullscreen></iframe></div></p>
<p>A follow-up video on the same channel continues to explore the topic, and talks about <a href="https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/">Specification gaming</a> more explicitly in the context of AI:
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/jQOBaGka7O0' frameborder='0' allowfullscreen></iframe></div></p>
<p>One thing that is not commonly talked about, and the article and video also do not cover is that the human value- and morality system are actually incomplete and inconsistent, clear from philosophical dilemmas such as the <a href="https://en.wikipedia.org/wiki/Trolley_problem">Trolley Problem</a> and its variations. Even worse, even we humans cannot all agree on critical questions of morality, even basic ones such as whether there should be such a thing as a death sentence, whether women should be allowed to have abortions, or whether immigration is a good or a bad thing. How could we possibly expect a super-intelligent AI to not do harm on a massive scale if we cannot even possibly define what is considered harmful?</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/17/measuring-work-performance.html">Measuring work performance</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/17/measuring-work-performance.html" class="u-url"><time class="dt-published" datetime="2023-12-17 00:39:45 &#43;0200">Dec 17, 2023</time></a>

		<div class="e-content">
			 <p>In <a href="https://blog.gaborkozar.me/2023/12/14/the-value-of.html">The value of your work</a> I have talked about how in a job, work is useful for your individual career only if your manager deems it so, even though that is not always the same thing as what is valuable for your team or your organization. In fact, it is rather difficult to come up with a good, unqualified definition of what even “valuable” means. Now I want to talk a bit about the same phenomenon but from the opposite perspective: what does this mean for organizations and managers?</p>
<p>If you are a department head, manager, or team lead, you should be extremely mindful of the bottom line: whatever you incentivize, knowingly or otherwise, is what is generally going to happen, and what you disincentivize, is generally not going to happen. Know also that you will unavoidably be creating incentives and disincentives without even being conscious of doing so.</p>
<p>The problem lies in how to measure value. It is simply not possible for you to examine everything that each of your reports do in full detail, even though that is the only way with which you could even <em>hope</em> to have a decent measure of their worth and value. (Note that even this would be no guarantee, due to your own inescapable perceptions and biases.) Therefore, you have to rely on proxies: some kind of measurements that &ndash; hopefully &ndash; correlate well with the actual value produced to be useful. You need a <em>metric</em>. Incentives however get created implicitly with each such metric.</p>
<p>For example, you could try measuring the number of lines of code that each of your software developers produce. More code does more stuff, and more stuff makes more money, yes? Well&hellip; maybe, sometimes, but not all code is created equal<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Even putting aside whether the metric is a good one, consider the incentive you have created: to write more lines of code. Your developers will quickly learn that what they should to get raises and promotions is to write more lines of code, and do just barely good enough work otherwise to avoid suspicion. Oh, you will get more lines of code, that is for sure, and it will look <em>amazing</em> if you plot it on a graph and show it at a board meeting&hellip; but it will almost certainly not mean that more features were added or bugs avoided and fixed; likely on the contrary, since now the focus of your reports is maximizing the lines of code whether it makes sense or not, which will definitely be detrimental to real value created (whatever that mystical beast even might be).</p>
<p>You can play this cat-and-mouse game for a while, but nature and people will always find a way. For example, suppose that you catch on to one of the tricks used to maximize the number of lines:</p>
<pre tabindex="0"><code>now
each
word
is
in
its
own
line.
</code></pre><p>Oh no! Okay, so you change the metric to be for number of characters instead of number of lines, and now your codebase looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#75715e">// defines a new variable called &#34;foo&#34; of type integer (whole signed number) and assigns it an initial value of 0 (zero, nothing)
</span><span style="color:#75715e"></span><span style="color:#66d9ef">int</span> foo <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;

<span style="color:#75715e">// Adds the value of &#34;bar&#34; to that of &#34;foo&#34; numerically, for example if &#34;bar&#34; is 4, then &#34;foo&#34; will now be 4 as well because &#34;foo&#34; was initially 0 (see the previous line).
</span><span style="color:#75715e">// Note that functionally, we could also just use a simple assignment in this case, or, you know, just initialize &#34;foo&#34; to be &#34;bar&#34; to begin with, but This Is More Maintainable (TM).
</span><span style="color:#75715e"></span>foo <span style="color:#f92672">+=</span> bar;
</code></pre></div><p>Oh dear. It&rsquo;s okay though, the solution is simple: just exclude comments from your counting tool. But wait, what&rsquo;s this, why did the above code turn into:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#75715e">#if CHARACTERS_ARE_BEING_COUNTED
</span><span style="color:#75715e"></span>This is not a comment<span style="color:#f92672">!</span>
Defines a <span style="color:#66d9ef">new</span> variable called <span style="color:#e6db74">&#34;foo&#34;</span> of type integer (whole <span style="color:#66d9ef">signed</span> number) and assigns it an initial value of <span style="color:#ae81ff">0</span> (zero, nothing).
<span style="color:#75715e">#endif
</span><span style="color:#75715e"></span><span style="color:#66d9ef">int</span> foo <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;

<span style="color:#75715e">#if CHARACTERS_ARE_BEING_COUNTED
</span><span style="color:#75715e"></span>Adds the value of <span style="color:#e6db74">&#34;bar&#34;</span> to that of <span style="color:#e6db74">&#34;foo&#34;</span> numerically, <span style="color:#66d9ef">for</span> example <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;bar&#34;</span> is <span style="color:#ae81ff">4</span>, then <span style="color:#e6db74">&#34;foo&#34;</span> will now be <span style="color:#ae81ff">4</span> as well because <span style="color:#e6db74">&#34;foo&#34;</span> was initially <span style="color:#ae81ff">0</span> (see the previous line).
Note that functionally, we could also just use a simple assignment in <span style="color:#66d9ef">this</span> <span style="color:#66d9ef">case</span>, or, you know, just initialize <span style="color:#e6db74">&#34;foo&#34;</span> to be <span style="color:#e6db74">&#34;bar&#34;</span> to begin with, but This Is More Maintainable (TM).
<span style="color:#75715e">#endif
</span><span style="color:#75715e"></span>foo <span style="color:#f92672">+=</span> bar;
</code></pre></div><p><code>CHARACTERS_ARE_BEING_COUNTED</code> is never defined anywhere, therefore all that text is essentially a comment anyway, but now you need a C++ preprocessor to defeat this particular gaming of your metric. Man! It&rsquo;s annoying, but you get through this.</p>
<p>Now Bob is complaining that he is working on complex multi-threaded computational code that is easily twice as hard to build as Joe&rsquo;s code. That is a fair point indeed, so you change your metric to count double any code that is multi-threaded. You see where this is going, right? You guessed it: now <em>all of the code</em> is multi-threaded, for absolutely no reason, and your production servers caught fire because Joe&rsquo;s button coloring code now uses 16 threads.</p>
<p>I could go on (and I can&rsquo;t say I&rsquo;m not tempted, this is fun) but I consider the point made: what gets produced is dictated by what metric you use, or, simply put: you will get what you measure, at the exclusion of all else. This is known as <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart&rsquo;s law</a>: when a measure becomes a target, it ceases to be a good measure.</p>
<p>Of course, you, dear reader, know better than this and would never introduce such a silly metric as a measure of productivity. Okay, but what <em>do</em> you use then? You could try measuring number of tickets resolved: but wait, what used to be 63 tickets is now over 18 000 and growing because some guy figured out how to automate creating and resolving tickets.</p>
<p>It is possible to create decent metrics for some jobs, but the more involved the job is, the exponentially more difficult it gets, and when it comes to software development, it becomes nigh impossible. This results in invisible, interpersonal metrics: who stands out to you or the other managers as someone who solves problems and pushes out feature after feature like a machine. Realize though that with this you have created a different incentive.</p>
<p>Consider two of your reports: Joe and Bob. Joe is super social, everybody likes him, and he always seems to find an opportunity to mention the latest feature he added or bug he has fixed. It is really impressive stuff! Just yesterday he spent the entire afternoon adding a new button to a form, which was apparently really difficult because of something to do with layouts and flexbox and margins and viewports and mobile devices. You are really glad to have somebody like Joe on your team who can fix complicated problems like that! Then there is Bob, nobody is super sure what he does, as he mostly keeps to himself. Yesterday he muttered something about optimizing the calculation engine so that each calculation now takes 20 milliseconds instead of 3 seconds, but you are not super sure what a millisecond is or why this is useful, and Bob did not seem too inclined to explain. 20 seems to be a bigger number than 3, so that seems worse, but it is hard to tell.</p>
<p>The incentive you have now is for developers to make their work well-known and <em>visible</em>. They will actively avoid tasks which are less visible, and therefore get less credit. You are now selecting for developers with good social and communication skills first, and programming skill only second, at best. Highly complex, technical issues, such as networking code that is responsible for keeping your system functioning reliably even under heavy load is now avoided, as it is immensely difficult to work with and then challenging to explain to a non-technical manager, unless you can show a graph for the number of incidents dropping dramatically since you have rolled out your new version or something.</p>
<p>In real life, most companies end up using some combination of concrete metrics (such as tickets closed) and personal impressions gathered during daily or weekly meetings, one-on-one meetings, and casual chats by the coffee machine. Because concrete metrics are easy to game as we have seen, the latter tends to have a higher weight, hence, good salespeople tend to be rewarded more than good professionals.</p>
<p>There is value generated not just by implementing new features, but also fixing bugs and issues. I have mentioned the <a href="https://en.wikipedia.org/wiki/Preparedness_paradox">Preparedness Paradox</a> in the previous article, and this is as standard as it gets. Fixing disasters <em>before</em> they occur is not a good strategy, because it is difficult to get credit for: there is no emotional weight attached to an event that did not happen, even if it would have been Very Bad. However, coming in at 4 AM on a Saturday to fix a critical production issue whereby all servers caught fire and working on it all day so that by Sunday the system can work again tends to get valued immensely, and the person gets hailed a hero. The same extends to fixing regular run-of-the-mill bugs in the code: a bug that never made it into production is less visible than a bug that did, and was then gloriously defeated in single combat, documented by a ticket and all.</p>
<p>Every leader and manager is human, and is subject to all the usual human biases, which then affect all of the people working around them, and can even define the experience of their reports. Every measure of performance and productivity is bad, some are just less bad than others. There is no silver bullet: you can only try to actively and consciously seek as much data as possible from as many sources as possible before rendering judgement, and you can be careful and thoughtful in assigning performance targets and metrics. But at the end of the day, you have to live with the fact that your judgement will always be unfair, and there will be people you prize that have done little to earn it, and there will be others who you have fired despite them quietly being the most valuable member of your team. Such is the role of a manager.</p>
<p><strong>P.S.:</strong> I find it interesting how all of this is very similar to <a href="https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/">Specification gaming</a>, which is a major problem of AI safety and alignment research. See also: <a href="https://blog.gaborkozar.me/2023/12/17/good-and-evil.html">Good and evil genies: the AI alignment problem</a></p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Consider that some of the most complex algorithms in existence span only a few hundred lines of code, if even that, and can take weeks or months to create; however, completely routine boilerplate code, such as what makes up most websites (e.g. routine <a href="https://developer.mozilla.org/en-US/docs/Glossary/CRUD">CRUD websites</a>) and tests take pretty much only as long to create as it takes to type them.</p>
<p>Some coding styles also naturally lend themselves to more lines of code; for instance, when writing <code>if (condition) { do-stuff }</code> some styles prefer to put the <code>{</code> on its own line, while others prefer to keep it at the end of line with <code>if (condition)</code>.</p>
<p>It&rsquo;s even worse if you try to compare code length in different languages: C++ is extremely verbose compared to Python, for example, and the exact same functionality (if not performance) may take 2-3 times as much code to achieve in C++ as in Python. This is one of the reasons why C++ is less popular than Python.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		

		<a href="https://blog.gaborkozar.me/2023/12/15/as-usual-xkcd.html" class="u-url"><time class="dt-published" datetime="2023-12-15 16:36:01 &#43;0200">Dec 15, 2023</time></a>

		<div class="e-content">
			 <p>As usual, XKCD is dead right and manages to entertain at the same time: <a href="https://xkcd.com/2867/">DateTime</a>.</p>
<p>I have had the misfortune of trying to implement sane date and time primitives at my last job, as well as dealing with measuring elapsed time with less overhead than <code>clock_gettime()</code>, and it is a nightmare. For a computer, date and time are completely meaningless, bonkers concepts. And god save me from timezones and daylight savings time. You may enjoy this: <a href="https://www.kiwi.com/stories/5-of-the-weirdest-time-zone-facts/">5 of the weirdest time zone facts</a></p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/15/genetically-modified-bacteria.html">Genetically modified bacteria to end tooth decay?</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/15/genetically-modified-bacteria.html" class="u-url"><time class="dt-published" datetime="2023-12-15 02:13:18 &#43;0200">Dec 15, 2023</time></a>

		<div class="e-content">
			 <p>This will be huge, if true: <a href="https://www.astralcodexten.com/p/defying-cavity-lantern-bioworks-faq">A genetically modified bacterium that outcompetes bacteria causing tooth decay</a> (<a href="https://news.ycombinator.com/item?id=38562939">Hacker News discussion</a>):</p>
<blockquote>
<p>Lantern Bioworks says they have a cure for tooth decay. Their product is a genetically modified bacterium which infects your mouth, outcompetes all the tooth-decay-causing bacteria, and doesn&rsquo;t cause tooth decay itself. If it works, it could make cavities a thing of the past&hellip;</p>
</blockquote>
<p>The prices:</p>
<blockquote>
<p>5: What&rsquo;s the plan to sell Lumina?</p>
<ul>
<li>Phase 1: (January 2024) Sell to biohackers in Prospera for $20,000.</li>
<li>Phase 2: (2025??) Sell to ordinary people in the US for a few hundred dollars.</li>
</ul>
</blockquote>
<p>It&rsquo;s worth noting that the FDA has thwarted this product in this past:</p>
<blockquote>
<p>The FDA demanded a study of 100 subjects, all of whom had to be &ldquo;age 18-30, with removable dentures, living alone and far from school zones&rdquo;. Hillman wasn&rsquo;t sure there even were 100 young people with dentures, but the FDA wouldn&rsquo;t budge from requiring this impossible trial. Hillman gave up and switched to other projects [&hellip;]</p>
</blockquote>
<p>So yeah, 2025 seems very optimistic to me, but I&rsquo;d be very happy to be proven wrong. There&rsquo;s further cause to take this with a huge grain (hell, maybe a silo of) salt, in that there appear to be some knowledgeable commenters in the HN discussion who are calling bullshit on this, <a href="https://news.ycombinator.com/item?id=38565695">for example</a>:</p>
<blockquote>
<p>This story has been around for far too long and evidence is unbelievably weak, and the claims border on fraudulent.</p>
<ol>
<li>
<p>The claim that this strain outcompetes the same S mutans from occupying the same niche cannot be true unless this strain is also capable of creating and tolerating environments with low pH. By definition if it creates acid to the same degree it will also cause cavities.</p>
</li>
<li>
<p>Single applications are insufficient to cause any persistent colonization. Even multiple daily applications of oral probiotics don&rsquo;t lead to colonization. [&hellip;]</p>
</li>
<li>
<p>S mutans is not the only species that causes cavities. [&hellip;]</p>
</li>
</ol>
</blockquote>
<p>But still, exciting! I&rsquo;m also looking forward to seeing more results of genetic engineering, as I believe that has at minimum as much potential to change the world as AI.</p>
<p>As an aside, this is the first time I&rsquo;m hearing about Prospera:</p>
<blockquote>
<p>Prospera [is] a libertarian charter city in Honduras. Prospera allows the sale of any biotech product under an informed consent rule: as long as the company is open about risks and the patient signs a waiver saying they were informed, people can do what they want.</p>
</blockquote>
<p>Appears to be quite problematic in general: <a href="https://www.vice.com/en/article/k7a7ae/foreign-investors-are-building-a-hong-kong-of-the-caribbean-on-a-remote-honduran-island">Foreign Investors Are Building a &lsquo;Hong Kong of the Caribbean&rsquo; on a Remote Honduran Island</a>, though I guess you&rsquo;d already expect little good from a place categorized as a &ldquo;libertarian charter city&rdquo;&hellip;</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/14/the-frame-pointers.html">The frame pointers strike back</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/14/the-frame-pointers.html" class="u-url"><time class="dt-published" datetime="2023-12-14 02:48:25 &#43;0200">Dec 14, 2023</time></a>

		<div class="e-content">
			 <p>Interesting: <a href="https://news.ycombinator.com/item?id=38629626">Ubuntu 24.04 LTS will enable frame pointers by default</a>, following Fedora which <a href="https://fedoraproject.org/wiki/Changes/fno-omit-frame-pointer">made the same change in version 38</a>. In short: GCC will by default start emitting frame pointers again as if using <code>-fno-omit-frame-pointer</code>, which helps debugging and profiling tools a bit, at the cost of what they suggest is a generally minimal performance impact.</p>
<p><a href="https://community.ibm.com/community/user/wasdevops/blogs/kevin-grigorenko1/2023/02/21/lessons-from-the-field-26-frame-pointers-and-why-t">Frame pointers and why they matter for performance tuning</a> summarizes the history of frame pointers very well, and reveals that this change both in Fedora and Ubuntu actually started by <a href="https://fedoraproject.org/wiki/Changes/fno-omit-frame-pointer#Potential_performance_impact">large tech companies like Google, Meta, and Netflix</a> re-building the Linux distributions they run on their servers with ones where frame pointers are enabled.</p>
<p>The Ubuntu announcement is way too long for the information it conveys, but here is the important part:</p>
<blockquote>
<p>[&hellip;] beginning with Ubuntu 24.04 LTS, GCC will enable frame pointers by default for 64-bit platforms. All packages in Ubuntu, with very few exceptions, will be rebuilt with frame pointers enabled, making them easier to profile and subsequently optimise.</p>
<p>&hellip;</p>
<p>There is a small performance penalty associated with the change, but in cases where the impact is high (such as the Python interpreter), we&rsquo;ll continue to omit frame pointers until this is addressed. Our analysis suggests that the penalty on 64-bit architectures is between 1-2% in most cases. We will not make this change on 32-bit architectures where the penalty is higher.</p>
<p>&hellip;</p>
<p>By enabling frame pointers by default, we&rsquo;re lowering the barrier to entry for performance profiling and debugging, meaning:</p>
<ul>
<li>Simplified Profiling: [&hellip;] profiling without worrying about compiler configurations.</li>
<li>Lower Overhead: Unwinding with frame pointers is significantly cheaper than using DWARF or DWARF-derived information.</li>
<li>Debugging Accessibility: [&hellip;] it will allow bcc-tools, bpftrace, perf and other such tooling to work out of the box.</li>
</ul>
</blockquote>
<p>The announcement does not attempt to explain what frame pointers are or why they are omitted by default, but there are plenty of good sources for this online, for example <a href="https://stackoverflow.com/a/13007258/128240">this answer on Stack Overflow</a>. In short, frame pointers make it easier and faster to figure out at runtime what the current call stack is (without having to consult debug info metadata), at the cost of losing one general-purpose register (<code>rbp</code>). Some more details can be found e.g. here: <a href="https://stackoverflow.com/a/14666730/128240">Trying to understand gcc option <code>-fomit-frame-pointer</code></a>.</p>
<p>The performance penalty of this seems to be a bit debated: the previous Stack Overflow answer mentions a geometric average of 14% slowdown, whereas the Ubuntu announcement mentions a 1-2% penalty, notably with the exception of Python 3.11, <a href="https://discuss.python.org/t/python-3-11-performance-with-frame-pointers/22507">where it is 9.5%</a>. I recommend reading the linked investigation to the Python performance regression as it is very interesting, but in summary it turns out that there are cases where having one more general-purpose register makes a large difference:</p>
<blockquote>
<p>In summary, to put it bluntly, there is just more work to do for CPU saving/restoring state to/from stack. But I don&rsquo;t think <code>_PyEval_EvalFrameDefault</code> example is typical of how application code is written, nor is it, generally speaking, a good idea to do so much within single gigantic function. So I believe it&rsquo;s more of an outlier than a typical case.</p>
</blockquote>
<p>Digging a bit further into the benefits of having frame pointers, as <a href="https://news.ycombinator.com/item?id=38634302">it is pointed out by a HN commenter</a> <code>perf</code> currently relies on frame pointers to do its job without absolutely murdering the performance of the application being profiled, though there are long-term plans to alleviate the need of this with <a href="https://lwn.net/Articles/940686/">SFrame: fast, low-overhead stack traces</a>.</p>
<p>I personally am not really convinced of the benefits of re-enabling frame pointers &ndash; at least on x86-64 &ndash; but I do understand that different companies have different priorities and requirements. Still, it looks to me that performing a backtrace should be rather infrequent outside of development and testing, so paying a performance penalty does not seem worthwhile overall, even if it is just 1%. We as software developers should be more mindful of the small ways in which we keep inflating hardware requirements, which essentially forces people to buy new phones and computers were more often than is healthy or sane.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/14/the-value-of.html">The value of your work</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/14/the-value-of.html" class="u-url"><time class="dt-published" datetime="2023-12-14 01:59:15 &#43;0200">Dec 14, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://blog.gaborkozar.me/2023/12/14/the-value-of.html">2023-12-14</a></p>
<p>Work is valued if and only if it is something that management (or whoever evaluates your performance) cares about, which does not necessarily have to be in any way related to how much real value it provides.</p>
<p>Have you fixed a bug or introduced a feature that helps every software developer in the company? That&rsquo;s nice, good work! But, if management does not include these software developers or cares about their opinion sufficiently, then the sad reality is that as far as they are concerned, you have achieved close to <strong>nothing</strong>. Have you replaced an SQLite database with a sharded PostreSQL system with Memcached for catching query results? That is only a good thing strictly if it addresses a pain point in a way that management cares about. If the only thing that solves is a headache for your devops team, and the devops team doesn&rsquo;t have the ear of management, then you&rsquo;re out of luck again.</p>
<p>Did you spot an accidentally quadratic algorithm on the hot path of your service that hasn&rsquo;t caused an issue yet only because the server happens to be restarted every week for unrelated reasons, and you&rsquo;ve spent a few weeks reworking the aforementioned algorithm to be linear? Whether you&rsquo;ve delivered real value is a bit subjective: how likely is it that this would have a caused an issue in the future? How big of an issue could this realistically have caused? You have to also take into account the expected lifetime of the software: if there&rsquo;s a good chance that it&rsquo;ll be scrapped or undergo a major rewrite in the near future, then accordingly the value is less. These are good questions to ask to determine &ldquo;real value&rdquo; delivered, but they are the wrong ones for judging if your work will actually be valued. The only correct question for that is: can you make management care that you have done this?</p>
<p>What management cares about is sometimes a good proxy for real value delivered. Have you optimized code that nobody needs to be any faster? That was not a good use of your time and management should rightfully not be impressed: little actual value was created. Same for adding features that nobody needed or asked for. When proposing to migrate from one technology / framework / platform to another, management will usually ask two questions: why do we need to do this (i.e. how much value will it add), and how long will it take (i.e. what will it cost)? This is a good thing. It is your job as a software developer (especially in a more senior or lead role) to accurately relay this information, and to present a strong case for doing work that you truly, genuinely believe should be done. It is a fact of life that sometimes you will fail because you haven&rsquo;t managed to explain the issue in the right way for them to understand, or, looking at this in the opposite way because they have failed to understand.</p>
<p>But management consists of people (unless you are reading this from a future in which your boss is ChatGPT). Hopefully intelligent, wise, competent, and driven people with the right incentives&hellip; but people nonetheless. And people have biases and limitations. For example, the unfortunate reality tends to be that fixing problems before they arise is essentially invisible (known as the <a href="https://en.wikipedia.org/wiki/Preparedness_paradox">Preparedness Paradox</a>), and therefore valued little or not at all; but letting it set fire to your production environment and then coming in to fix it is going to be very visible, appreciated, and valued (you may even get called a hero!). Of course, I certainly would not recommend taking this philosophy to the extreme: issues frequently popping up will usually lead to unhappy management and therefore bad reviews; but at the same time, preventing all fires at the cost of not delivering new features (or whatever it is that the Powers That Be care about you doing) is also likely going to have the same effect on your perceived performance.</p>
<p>Of course, a good manager or team lead will usually understand the value that your work provided even if it is indirect. But this is really really hard! It is fighting an uphill battle. Indirect value tends to be difficult to measure or quantify. It may even start being effective later, after some time has passed, and few people will take care to trace the cause back to your work months prior. You&rsquo;ll have to work hard to sell the value you provided, and that in and of itself may get you in trouble, as it can be easy to mistake you for someone who does nothing useful but talks.</p>
<p>Whenever you create value indirectly, whether it will be recognized is &ndash; to a large extent &ndash; a question of trust, which in turn tends to be a function of how much your manager likes and respects you. Depending on your job and the environment you&rsquo;re in, the ratio of direct to indirect value supplied by you and your work may make it anything between &ldquo;exceptionally helpful&rdquo; to &ldquo;absolutely critical&rdquo; for you to be liked and trusted by management in order for you to be appreciated and to have a long, successful career.</p>
<p><strong>Edit 2023-12-16</strong>: added mention and link to the Preparedness Paradox at the suggestion of a colleague &ndash; thank you!</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/09/how-do-allyoucaneat.html">How do all-you-can-eat restaurants stay profitable?</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/09/how-do-allyoucaneat.html" class="u-url"><time class="dt-published" datetime="2023-12-09 02:34:11 &#43;0200">Dec 9, 2023</time></a>

		<div class="e-content">
			 <p>All-you-can-eat buffets are so not so widespread in Europe (as best as I can tell), but apparently they&rsquo;re a thing in the USA. I&rsquo;ve been in this kind of restaurant exactly once in my life, and I can&rsquo;t say that I didn&rsquo;t find the idea appealing: it instinctively seems like a really good deal. At least, until you remember that your stomach has limited capacity, and regardless of the quantity (or even quality!) of food available, you will mostly not be able to eat much more than a regular meal.</p>
<p>Anyway, here is a super interesting article on such restaurants and how they stay profitable: <a href="https://thehustle.co/the-economics-of-all-you-can-eat-buffets/">The economics of all-you-can-eat buffets</a>.</p>
<p>I found it shocking how cheap food actually is at this scale (1.13 USD per serving of chicken?!), but I did suspect that restaurants have some more psychological tricks up their sleeve, and oh yeah, they do. I have not been surprised in the slightest though to learn that there are limits on what these restaurants will actually allow.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/12/03/towards-understanding-ai.html">Towards understanding AI models</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/12/03/towards-understanding-ai.html" class="u-url"><time class="dt-published" datetime="2023-12-03 03:22:45 &#43;0200">Dec 3, 2023</time></a>

		<div class="e-content">
			 <p>As much hype and attention that the machine learning / artificial intelligence field gets, and in spite of some impressive results that have come out of it in the form of e.g. ChatGPT, overall we have very little understanding in exactly <em>how</em> do these models work. We can train them, and we can use them, but we cannot answer any questions about what a particular AI model will or will not do, outside of testing this empirically. This is quite a big problem, especially for safety or alignment reasons, i.e. making sure that we don&rsquo;t accidentally create an AI model that has both the capability and the inclination to rename itself Skynet and do skynet-y things.</p>
<p>Thankfully we have some of the brightest minds of our generation working on this problem, and Scott Alexander has this excellent article trying to summarize recent findings (from Anthropic&rsquo;s interpretability team) on the topic: <a href="https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand">God Help Us, Let&rsquo;s Try To Understand AI Monosemanticity</a>. It starts for relatively basic concepts and does I think an excellent job at explaining both the problems and the results in a way that remains relatively comprehensible even for someone (such as myself) who has virtually no knowledge of the field. As a bonus, it even goes a bit into discussing how the properties of artificial neural nets relate to those of biological neural nets (i.e. our brains), though there&rsquo;s very little of credible research results on this.</p>
<p>Go read it! If you need extra motivation, the subtitle of the post is <em>Inside every AI is a bigger AI, trying to get out</em> and that is just too funny to ignore.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/11/22/recovering-from-segmentation.html">Recovering from segmentation faults</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/11/22/recovering-from-segmentation.html" class="u-url"><time class="dt-published" datetime="2023-11-22 03:21:05 &#43;0200">Nov 22, 2023</time></a>

		<div class="e-content">
			 <p>This is brilliant: <a href="https://feepingcreature.github.io/handling.html">Cleanly recovering from Segfaults under Windows and Linux (32-bit, x86)</a>.</p>
<p>I do not think I&rsquo;d want to actually use this in production, for the same reason that the article itself points out: segmentation faults often indicate that something has gone <em>horribly wrong</em> inside the application, and attempting to recover from it even by throwing an exception instead of crashing easily has the potential to cause very weird and unexpected behaviours which will be nigh-impossible to debug. But, the idea is clever.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/11/22/c-vs-rust.html">C&#43;&#43; vs Rust: performance vs safety</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/11/22/c-vs-rust.html" class="u-url"><time class="dt-published" datetime="2023-11-22 03:03:29 &#43;0200">Nov 22, 2023</time></a>

		<div class="e-content">
			 <p>I was reading <a href="https://lucisqr.substack.com/p/why-i-think-c-is-still-a-very-attractive">Why I think C++ is still a desirable coding platform compared to Rust</a> on <a href="https://lucisqr.substack.com/">Low Latency Trading Insights</a>, and wanted to share my thoughts. Overall, I really appreciate this article: it raises a very important subject and makes lots of good points. I do disagree with some of it, though I wouldn&rsquo;t go as far as to say that I disagree with the conclusions drawn.</p>
<p><strong>Safety</strong>: I can empathize with the author where he says that segfaults and other issues coming from the lack of safety in C++ are very rare for him: I expect most experienced C++ developers will feel the same way; I certainly do. But in a company you also have to consider how productive junior members can be. I&rsquo;ve mentored and managed my share of juniors, and C++ being essentially a language assembled from landmines definitely adds a drag to the stability of the overall system and the general productivity of developers, in part because less experienced or knowledgeable developers will run into said landmines.</p>
<p><strong>Performance</strong>: even in a trading system, not all code &ndash; in fact, not even <em>most code</em> &ndash; needs to be ultra low-latency. (That&rsquo;s why lots of trading companies use languages like C#, Java, and OCaml in addition to C++.) Using <code>unsafe</code> on the most latency-sensitive code sounds like a fair enough choice to me, or at least I&rsquo;d not completely ignore <code>unsafe</code> in this way. It is completely true though that optimizations currently work better for C++ (due to its long history and untold effort expended in that area) and that GCC most often optimizes better than Clang at the moment: this situation may change over time however. Undefined behaviour in C and C++ does unlock a lot of optimization opportunities, a fact which is often lost on critics.</p>
<p><strong>Ecosystem</strong>: C++ is definitely the established and dominant choice, so companies will have huge existing C++ codebases, and there are plenty of C++ developers to go around. These are difficult for Rust to combat, though over time I expect the share of Rust developers will go up. Rust does have a massive advantage over C++ though in the ecosystem via it&rsquo;s easy accessibility of third-party libraries (similarly to what is enabled by Python&rsquo;s <code>pip</code> and JavaScript&rsquo;s <code>npm</code>). In C++ it can be a huge pain to use third-party libraries, partly because many of them will end up doing things completely differently (see e.g. Qt&rsquo;s <code>QString</code> instead of <code>std::string</code>), and partly because integrating them into the build system can be very problematic. (CMake&rsquo;s popularity alleviates this somewhat, but having to use CMake is its own special kind of hell, so I&rsquo;m not sure whether this actually counts as a win.)</p>
<p>I consider myself a very solid C++ developer, but admittedly I am not a Rust developer as of yet, so my opinions may be based on incomplete or erroneous information. I have been digging into Rust a bit though, because I believe it has merits: I&rsquo;ll be curious to see if my opinion changes on the longer run.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/11/20/c-language-philosophy.html">C&#43;&#43; language philosophy</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/11/20/c-language-philosophy.html" class="u-url"><time class="dt-published" datetime="2023-11-20 04:47:29 &#43;0200">Nov 20, 2023</time></a>

		<div class="e-content">
			 <p>C++ is one of the oldest mainstream programming languages in use today, and also easily one of the most controversial ones: some people love it (including myself), but if you read any online forums, you&rsquo;d easily be under the impression that most people <em>despise</em> it.</p>
<p>The criticisms are, more often than not, fair: not many languages can rival C++ when it comes to sheer complexity, made worse by the fact that the language pretty much assumes that the programmer is an expert who knows exactly what they are doing, meaning that the language will by and large allow you to do whatever you want, no matter how insane, and if you got it wrong, well, you have absolutely no guard-rails to protect you: you&rsquo;ll get cryptic compiler diagnostics, unhelpful runtime errors (&ldquo;Segmentation fault&rdquo; being perhaps the most notorious one), or just completely insane run-time behavior.</p>
<p>However, something that gets talked about a bit less often is that these properties of C++ are deliberate, and they have their uses: while C++ can be very difficult to work with, it is also immensely powerful that can allow a knowledgeable developer to write code in it that will outperform anything you could do in other, safer languages. Not to mention that there are things that you cannot even reasonably implement without having direct access to memory and allocations. There are reasons why operating systems, games, and high-frequency trading software all heavily use C++ even today, when languages like C# or Java are available and generally offer much superior developer productivity. (Rust is an interesting new contender in this consideration: it aims to beat C++ at its own game while providing convenience and more safety.)</p>
<p>C++ is also commonly criticized for its complexity and multi-paradigm approach which offers several distinct ways of writing computer programs. I came across this quote recently from the creator of C++ that explains the philosophy behind the language design, and addresses this point better than I could:</p>
<blockquote>
<p>People don’t just write classes that fit a narrowly defined abstract data type or object-oriented style; they also — often for perfectly good reasons — write classes that take on aspects of both. They also write programs in which different parts use different styles to match needs and taste.</p>
<p>…</p>
<p>The language should support a range of reasonable design and programming styles rather than try to force people into adopting a single notion.</p>
<p>…</p>
<p>There is always a design choice but in most languages the language designer has made the choice for you. For C++ I did not: the choice is yours. This flexibility is naturally distasteful to people who believe that there is exactly one right way of doing things. It can also scare beginners and teachers who feel that a good language is one that you can completely understand in a week. C++ is not such a language. It was designed to provide a toolset for professionals, and complaining that there are too many features is like the “layman” looking into an upholsterer’s tool chest and exclaiming that there couldn’t possibly be a need for all those little hammers.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/11/20/simultaneous-multithreading-priority.html">Simultaneous multi-threading: priority signalling</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/11/20/simultaneous-multithreading-priority.html" class="u-url"><time class="dt-published" datetime="2023-11-20 04:26:10 &#43;0200">Nov 20, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;ve encountered a statement today in a <a href="https://devblogs.microsoft.com/oldnewthing/20180809-00/?p=99455">random blog post about the IBM PowerPC 600 series</a> that broke my brain for a minute while I was trying to figure out what it could possibly have meant, so now I&rsquo;m going to subject you to it as well. Here goes:</p>
<blockquote>
<p>Moving a register to itself is functionally a nop, but the processor overloads it to signal information about priority.</p>
</blockquote>
<p>The blog post does go on to explain this&hellip; well, for certain values of &ldquo;explain&rdquo; anyway:</p>
<pre><code> or      r1, r1, r1       ; low priority
 or      r6, r6, r6       ; medium-low priority
 or      r2, r2, r2       ; normal priority
</code></pre>
<blockquote>
<p>A program can voluntarily set itself to low priority if it is waiting for a spin lock.</p>
</blockquote>
<p>Ok, let&rsquo;s back up a step and see if we can make sense of this. The code we&rsquo;re looking it is Assembly (the human-readable form of machine code) for this particular IBM CPU architecture. <code>or</code> executes a bit-wise logical OR, that is, an independent logical OR operation for each bit in a machine word (however many bits that is): <code>ra | rb</code> with the result stored in <code>rd</code> would be written as <code>or      rd, ra, rb</code>. (The <code>r</code> prefixes just denote that we are talking about CPU registers; just think of them as hardware-level variables if you&rsquo;re not sure what that means.)</p>
<p>Based on this, <code>or      r1, r1, r1</code> would calculate <code>r1 | r1</code> and store the result in <code>r1</code>. It should be easy to see that <code>r1 | r1</code> will always just yield <code>r1</code> unchanged as the result (since <code>1 | 1 = 1</code> and <code>0 | 0 = 0</code>), so this simplifies down to: store <code>r1</code> into <code>r1</code>, i.e. do not do anything at all. Instructions that have no actual effect can be useful in a few cases (such as ensuring memory alignment of code, or as patching points for debuggers), and there are lots of ways of writing code that has no effect: here, for example, we could use any register in place of <code>r1</code> and it would work the same way. (x86-64, the dominant architecture for PCs today, has a dedicated <code>nop</code> instruction for this purpose.)</p>
<p>Apparently, the PowerPC 600 series architecture uses these different typings of &ldquo;no operation&rdquo; to signal priority: using <code>r1</code> signals low priority, and <code>r2</code> normal priority (I&rsquo;m not sure what to make of &ldquo;medium-low priority&rdquo; denoted by the use of <code>r6</code>). But how is &ldquo;priority&rdquo; even a concept in CPU land? One might think that it has a single stream of instructions to execute, and it just goes executing them one by one. This turns out to not exactly be the case, even putting aside more complicated optimizations such as out-of-order execution: the PowerPC architecture uses simultaneous multi-threading (SMT) between two &ldquo;threads&rdquo; per core (similarly to Intel&rsquo;s hyper-threading technology for x86-64).</p>
<p>To understand why this is useful, consider that each CPU core contains multiple execution units (e.g. 8 per core for PowerPC5), which essentially means that each core separately is actually able to perform multiple calculations concurrently. These execution units are specialized: for example, you&rsquo;d have different execution units for integer and floating point (decimal) arithmetic: usually called ALU (Arithmetic-Logical Unit) and FPU (Floating-Point Unit). This means that if, for example. the running application (more precisely, the currently scheduled operating system thread) is performing lots of integer arithmetic, then the core&rsquo;s ALUs are be fully utilized while the FPUs are sitting idle. This is a waste, especially if we know that there&rsquo;s another thread that could be utilizing those FPUs.</p>
<p>Beyond this, there are other situations where allowing two threads to run per CPU core is useful: the most common one would be when one thread is waiting for a result of a memory operation. CPUs are much, much, much faster than accessing memory is, so CPUs spend subjective lifetimes simply waiting for the two numbers to arrive so that it can finally add them together. (This is the entire justification for the multi-level caches.) While one thread is waiting for memory (perhaps due to a cache miss), the other may be able to proceed.</p>
<p>Of course, the two threads sharing the same CPU core must also share lots of hardware resources on the core (such as the register file or an instruction decoder), so they will never be able to run nearly as concurrently as two threads running on two separate cores can. Therefore, performance of the individual threads suffers, even though the overall amount of work done by the entire system improves. (This is sometimes actually undesirable, such as when running latency-sensitive applications like high-frequency trading algorithms. In these scenarios, you&rsquo;ll want to disable this hardware feature.)</p>
<p>Now that we understand all of this, we should be able to suspect where the concept of priority comes into play when dealing with two threads running on the same CPU core; <a href="https://course.ece.cmu.edu/~ece740/f13/lib/exe/fetch.php?media=kalla04_power5.pdf">the higher-priority thread gets more of the shared resources</a>:</p>
<blockquote>
<p>In SMT mode, the Power5 uses two separate instruction fetch address registers to store the program counters for the two threads.
&hellip;
Instruction fetches alternate between the two threads. After fetching, the Power5 places instructions in the predicted path in separate instruction fetch queues for the two threads. [&hellip;] On the basis of thread priorities, the processor selects instructions from one of the instruction fetch queues [&hellip;].
&hellip;
The Power5 chip observes the difference in priority levels between the two threads and gives the one with higher priority additional decode cycles.
If both threads are at the lowest running priority, the microprocessor assumes that neither thread is doing meaningful work and throttles the decode rate to conserve power.</p>
</blockquote>
<p>The article even goes into examples for when this could be especially useful:</p>
<blockquote>
<ul>
<li>A thread is in a spin loop waiting for a lock. Software would give the thread lower priority, because it is not doing useful work while spinning.</li>
<li>A thread has no immediate work to do and is waiting in an idle loop. Again, software would give this thread lower priority.</li>
<li>One application must run faster than another. For example, software would give higher priority to real-time tasks over concurrently running background tasks.</li>
</ul>
</blockquote>
<p>Implementations of the x86-64 architecture also <a href="https://stackoverflow.com/a/74152773/128240">feature similar simultaneous multi-threading capability</a>: Intel calls this Hyper-threading, while AMD has named it Clustered Multi-Threading. x86 has a dedicated <code>nop</code> instruction for not doing anything, and does not have anything to my knowledge for signalling priority. There is a separate <code>pause</code> instruction specifically for spin-wait loops, which <a href="https://stackoverflow.com/a/26389922/128240">while helps with SMT</a> is actually an <a href="https://stackoverflow.com/a/12904645/128240">important performance optimization even in ST (single-threaded) mode</a> because it shields against the cost incurred by the branch misprediction when the wait stops.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/11/10/earning-interest-rate.html">Earning interest rate on Interactive Brokers cash balance</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/11/10/earning-interest-rate.html" class="u-url"><time class="dt-published" datetime="2023-11-10 22:13:33 &#43;0200">Nov 10, 2023</time></a>

		<div class="e-content">
			 <p>For my investments I normally use <a href="https://www.degiro.nl/">Degiro</a> as a broker, because their app and website is quite decently usable, and their fees are low. However, it&rsquo;s come to my attention that Interactive Brokers pays <a href="https://www.interactivebrokers.com/en/accounts/fees/pricing-interest-rates.php">quite good interest</a> on your cash balance with them: up to 4.83% currently. This is much better than a normal savings account! My ING savings account currently pays 1.25% interest (well, 1.5% below 10K EUR, but above that it&rsquo;s 1.25%). So yeah, 4.8% sounds great!</p>
<p>Opening an account with Interactive Brokers (IKBR) is a bit annoying, but honestly wasn&rsquo;t that bad. The worst part is that they require a proof of address, and for the Netherlands they don&rsquo;t appear to want to accept anything other than a municipality extract, which costs some money and takes like a few weeks to get.</p>
<p>But, once you&rsquo;re through all that, and you&rsquo;ve transferred the money to your IKBR account, you can start earning that sweet sweet interest, right? Well, yes, but there are caveats and things that are not easy to figure out, because IKBR&rsquo;s website and app is a <em>mess</em>.</p>
<p>First of all, that 4.8% interest rate applies only on settled USD cash balances. You can also get interest on EUR cash balances, but less: currently up to 3.4%. For me this was fine and I&rsquo;ve converted my whole IKBR balance to USD, so that not all of my wealth is in EUR: basically, to hedge a bit against the currency (FX) rate. Then, note that you will not earn any interest on the first 10K USD, only on the amount over it. Also, you only earn the full interest rate as stated above if your net asset value (NAV) is over 100K USD: below that, you get a proportionally down-adjusted rate. All of this is explained on the <a href="https://www.interactivebrokers.com/en/accounts/fees/pricing-interest-rates.php">IKBR website</a> pretty clearly: so far so good.</p>
<p>If you actually go ahead and do all of this, and then a month or so later you check your account statement, you will be confronted with a whole bunch of puzzling information that the IKBR client portal does an incredibly poor job of explaining. On the topic of interest, you will find 3 separate things in the statement: accrued interest (positive), interest reversals (negative), and &ldquo;Withholding @ 20% on Credit Interest&rdquo; (negative). At this point if you&rsquo;re anything like me, you&rsquo;ll be confused: what on earth is all this? I just wanted to get some interest!</p>
<p>You have to dig around quite a bit on the IKBR client portal to get an explanation on what&rsquo;s the deal with accrued interest and reversals, but you can find this:</p>
<blockquote>
<p>Interest accruals are converted to your base currency using the daily currency conversion rates. Interest accrues daily during the month and is paid or charged once a month after each month&rsquo;s close. When interest is paid or charged to the account, the daily accruals are reversed.</p>
</blockquote>
<p>Essentially: IKBR credits your account daily on the interest your cash has earned (accrued) on that day, and then once per month coalesces all of these into one and reverses the daily additions. This is weird and counter-intuitive, and my best guess for the reason is that they do this is because both your settled cash balance and the effective interest rate may change often. The monthly reversal and credit may be slightly different in case the interest rate has changed since the beginning of the month.</p>
<p>Finally, what is &ldquo;Withholding @ 20% on Credit Interest&rdquo;? <a href="https://www.reddit.com/r/interactivebrokers/comments/xusa7l/just_received_from_ibkr_20_tax_on_interest/">As it turns out</a>, Ireland imposes a 20% tax on credit interest (i.e. money made from interest payments), and IKBR withholds that automatically. The fun part is that they do this even if you are not a tax resident in Ireland! You have to dig around to get an explanation for all of this, but, to be fair, IKBR <a href="https://www.ibkrguides.com/kb/article-4687.htm">does have an article</a> that explains what this is and what you have to do. You have to complete form 8-3-6 (see the link), then have the tax authorities of the country where you are tax resident <strong>also</strong> complete it, and then submit it to IKBR. Yikes, what a hassle!</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/10/26/the-state-of.html">The state of self-driving: Cruise suspended</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/10/26/the-state-of.html" class="u-url"><time class="dt-published" datetime="2023-10-26 00:44:23 &#43;0200">Oct 26, 2023</time></a>

		<div class="e-content">
			 <p>I was reading <a href="https://news.ycombinator.com/item?id=38002752">California suspends Cruise&rsquo;s autonomous vehicle deployment</a>, which is about a self-driving car company&rsquo;s (Cruise) failures, and how regulators banned them because they were deemed unsafe. The triggering incident:</p>
<blockquote>
<p>In the Order of Suspension, the California DMV said that the Cruise vehicle initially came to a hard stop and ran over the pedestrian. After coming to a complete stop, it then attempted to do a “pullover maneuver while the pedestrian was underneath the vehicle.” The car crawled along at 7 mph for about 20 feet, then came to a final stop. The pedestrian remained under the car the whole time.</p>
<p>The day after the incident, DMV representatives met with Cruise to “discuss the incident.” During that meeting, Cruise only showed footage up to the first complete stop, according to the Order of Suspension. No one at Cruise told the officers or showed any footage of the subsequent pullover maneuver and dragging. The DMV only learned of that from “another government agency.” When DMV asked for footage of that part of the incident, Cruise provided it.</p>
</blockquote>
<p>It&rsquo;s difficult to read this in any other way than &ldquo;Cruise fucked up, they knew it but tried to cover it up&rdquo;, and that&rsquo;s why the DMV pulled the plug. This is bad, but not all in all very surprising or interesting, unlike the experiences of some of the commenters in the above Hacker News thread. For example, <a href="https://news.ycombinator.com/item?id=38003860">this comment</a>:</p>
<blockquote>
<p>Cruise cars do not perform acceptably.</p>
<p>They manage to avoid collisions by driving extremely conservatively, but the way they traverse, say, a left turn against traffic is absurd. They slow everyone down, including emergency vehicles and public transit, by performing far below the level of most Human drives.</p>
<p>They don&rsquo;t work in the rain, they can&rsquo;t handle construction, they block garages and driveways.</p>
</blockquote>
<p>Unfortunately, this makes sense to me: self-driving is <em>hard</em>. In fact, it is so hard to solve in the general case (i.e. to be able to handle all scenarios) that I am low-key convinced that solving it would be equivalent to creating a true artificial intelligence. We are so, so far from that that I&rsquo;m not even really sure that this whole self-driving business makes sense, even though the cost of human-operated cars is measured in deaths in the ten-thousands worldwide each year.</p>
<p>What does surprise me is that the same commenter continues:</p>
<blockquote>
<p>Waymo vehicles are objectively far better. They drive like Humans do. Still some issued with weather and construction, but they work well alongside busses, trucks, and private cars without slowing anyone down.</p>
</blockquote>
<p>Huh. Waymo is Google&rsquo;s self-driving project, and looks like they have a fan. <a href="https://news.ycombinator.com/item?id=38004396">Another comment</a> concurs:</p>
<blockquote>
<p>From my perspective, as a relatively early adopter of Waymo (60+ rides). I have zero gripes with the driving itself. In fact I&rsquo;ve seen Waymos do things that no human would be able to do. [&hellip;]</p>
<p>Waymo made a right turn up a steep hill. Two lanes each way. It then pulled a bit into the left lane abruptly and I didn&rsquo;t get why until a split second later a skateboarder was crouched down and went by on my right against traffic. There was zero chance a human would pull that off. Not enough time for a head check or mirror check. There wouldn&rsquo;t have been an accident but the Waymo clearly has insane reaction time and vision advantages - and uses them.</p>
</blockquote>
<p>Yet <a href="https://news.ycombinator.com/item?id=38005655">another comment</a>:</p>
<blockquote>
<p>I saw a Waymo come to a screeching halt in a split second as a&hellip; less than intelligent individual&hellip; skateboarded in front of a bus, into the extremely busy road.</p>
<p>If it was a human that person would have been dead, no question.</p>
<p>Sometimes I&rsquo;m riding in a Waymo (which I do every single day, 4x) and it does something, and I double take. Then a second later I see whatever it was it reacted to, and I&rsquo;m like &ldquo;dang, why did I doubt you, robotic overload?&rdquo;</p>
<p>The best part is that it works the same way in the day time vs the night time.</p>
<p>Magic.</p>
</blockquote>
<p>Of course, one has to wonder if these are just &ldquo;fanboys&rdquo;, but wow, sounds way impressive! Waymo&rsquo;s website does have a very impressive <a href="https://waymo.com/safety/">Safety</a> page, making all the right sounds. I&rsquo;m cautiously optimistic, but let&rsquo;s see if they manage to roll it out anywhere else.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/10/24/the-nike-smart.html">The Nike smart shoes tragicomedy</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/10/24/the-nike-smart.html" class="u-url"><time class="dt-published" datetime="2023-10-24 02:51:41 &#43;0200">Oct 24, 2023</time></a>

		<div class="e-content">
			 <p>Did you know that Nike has <strong>smart sneakers</strong>, since like 2019? I certainly have not, and I am having difficulty figuring out what to do with this piece of information, so I am posting about it here. The Verge has an article on the topic, which sometimes reads like a parody of itself: <a href="https://www.theverge.com/2019/1/15/18167388/nike-self-lacing-shoes-adapt-bb-smart-bluetooth-app-features-battery-life-price-release-date">Hands-on with Nike’s self-lacing, app-controlled sneaker of the future</a></p>
<blockquote>
<p>I flew across the country to Portland to experience the Adapt BB, Nike’s new self-lacing, Bluetooth-enabled sneakers, but the guy showing me around campus is wearing a pair of Zoom Flys that refuse to stay tied. Within 10 minutes of tying them, they’re untied again, flailing all over. I hate when people point out my untied shoes, but his feel intentional. Of course I notice the laces. Of course I point them out. He laughs and swears he’s not doing this on purpose, that Nike hasn’t deliberately set up my visit with a scene out of an infomercial fail.</p>
</blockquote>
<p>Ok, ok, enough messing around, but what <em>are this</em>?</p>
<blockquote>
<p>The Adapt BB — the BB stands for “basketball” — build on Nike’s decades-long dream to create an auto-lacing smart shoe that adapts to wearers’ feet.</p>
</blockquote>
<p>Ok, sure, I guess, why the hell the not. Except, come on, they are <em>smart sneakers</em>, when was the last time that <em>smart</em> anything since the phone has made anything better for anybody? In <em>completely unrelated news</em>, apparently, due to a software bug, <a href="https://www.theverge.com/circuitbreaker/2019/2/21/18234615/nike-adapt-bb-fix-android-bug-firmware-update-patch">they were completely unusable</a> after launch, and since we&rsquo;re back in comedy land, I&rsquo;m going to quote The Verge again as it is just hilarious:</p>
<blockquote>
<p>The $350 Adapt BB went on sale this past weekend, and users started reporting issues soon after. Some report that either the left or right sneaker fails to pair after attempting to update them through the companion Android app. That means the sneaker can’t be tightened or properly worn. Some users say the update caused the motor to stop functioning, too, so even the physical buttons don’t work.</p>
</blockquote>
<p><a href="https://twitter.com/internetofshit">Internet of Shit</a> indeed.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/10/24/the-eus-stagnating.html">The EU&#39;s stagnating economy vs the USA</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/10/24/the-eus-stagnating.html" class="u-url"><time class="dt-published" datetime="2023-10-24 02:03:44 &#43;0200">Oct 24, 2023</time></a>

		<div class="e-content">
			 <p>Economically and financially speaking, the USA is much more attractive than pretty much anywhere in Europe. US companies dominate the world in many fields, especially technology: just think of Google, Microsoft, or Amazon. Top earners in the USA earn multiples of what top earners in Europe do: in the USA, bright graduates in highly sought-after specializations (e.g. investment banking) may make as much as 400K USD per year starting out; a salary all but unattainable to anybody in Europe beyond top executives.</p>
<p>All of this are just symptoms: while the Industrial Revolution started in Europe, the continent has fallen behind since compared to the rest of the world, especially the USA. This is dramatically visible if you look at a graph for GDP growth, for example. But why is this the case? As with most things, there is no one answer, but a variety of reasons. Economics Explained did a pretty good video on the topic that I think is worth watching, but I&rsquo;ve also put together some further thoughts after it.</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/tsahMxXdW30' frameborder='0' allowfullscreen></iframe></div>
<p>To summarize the reasons mentioned in the video:</p>
<ul>
<li>the USA is a lot more of a unified country than the EU is, which is a huge advantage for local companies: the EU consists of dozens of comparatively tiny countries all speaking different languages and having differing legal, tax, and administrative systems which businesses have to navigate to expand within Europe</li>
<li>the USA is still pretty much the most attractive target country for immigration</li>
<li>the USA uses the world reserve currency (the US dollar) and speaks English, making it attractive for investors both small and large</li>
<li>EU countries tend to prioritize better working conditions and social safety nets (such as unemployment benefit and affordable healthcare) over productivity and economic growth</li>
</ul>
<p>But, there are other reasons that I think the video misses that are also important factors:</p>
<ul>
<li>the two World Wars have devastated Europe and gave the USA a huge head-start, while Eastern Europe has still not really managed to recover (socially as much as economically)</li>
<li>the USA is a massive country that is incredibly rich in natural resources, while Europe is fairly sparse in comparison</li>
<li>Europe&rsquo;s population is aging much faster than the US, partly because of the lower immigration of young professionals; also, the immigration that Europe gets is relatively unskilled, again, compared to the US</li>
</ul>
<p>Of course, in spite of all of these, Europe remains one of the most desirable places in the world to work and live, especially at lower income levels: I&rsquo;d much rather be poor in Europe than in the USA, thank you very much. Admittedly, due to the relatively huge tax burdens on the high-earners in Europe, being wealthy in the USA is much more desirable individually, but as a society that does not seem like a good trade-off to make, even if you only look at the pragmatic reason of security.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/10/19/coordination-problems-vs.html">Coordination problems vs the Techno-Optimist Manifesto</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/10/19/coordination-problems-vs.html" class="u-url"><time class="dt-published" datetime="2023-10-19 01:11:26 &#43;0200">Oct 19, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;ve come across <a href="https://andrewkelley.me/post/the-techno-optimist-manifesto.html">The Techno-Optimist Manifesto</a> and, well, I have thoughts. (I know, it was bound to happen eventually.) I agree with a lot of the points, perhaps even with most of them, but the rest strikes me as plainly naive:</p>
<blockquote>
<p>As techno-optimists, we believe that we must, and we will, create more advanced human systems to prevent war forevermore. Human society will mature and overcome its barbaric history, learning to cooperate with each other so that we can create technological works of global magnitude, such as orbital rings.</p>
<p>Every dollar spent on the military is a blight on the country&rsquo;s honor. We believe that military budget reductions are a cause for celebration because they symbolize progress towards the space age.</p>
</blockquote>
<p>It&rsquo;s a nice thought, and I fully believe that we <em>must</em>, as a civilization, get there, else we will destroy ourselves. But I just do not see a path from <em>here</em> to <em>there</em>, due to coordination problems on every level. Scott Alexander&rsquo;s <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Meditations on Moloch</a> is an extremely long essay that explores how and why this is the case, but here I will try to stay on the most salient points.</p>
<p>On an individual level, you have an incentive to seize an advantage <em>even if it leaves everybody else worse off</em>. After all, if you don&rsquo;t, somebody else will, so in that perspective the question is not whether everybody else will end up being worse off (because they will be regardless of what you choose) but whether the person who is better off is you or somebody else. You have a family, aging parents, sick child, or perhaps just an insatiable lust for fried chicken: surely, it&rsquo;s better if <em>you</em> have that money or power than somebody else, god knows what would they use it for.</p>
<p>The caveat, of course, is that everybody thinking this way destroys trust and cooperation, without which no group, society, or civilization can exist. Therefore, it is critical to collectively disincentive people from choosing so, by rewarding cooperation (people acting in defense of the collective) and punishing defection (people acting against the collective, e.g. by taking an advantage that leaves everybody else worse off), to use the terminology of <a href="https://en.wikipedia.org/wiki/Game_theory">game theory</a>. Relatively few people will risk e.g. stealing, when there is little upside, and the downside is too risky and scary.</p>
<p>Ultimately, much the same principles apply to inter-group, and even international relations: each country is acting in their own self-interest. What kind of leverage does one country have over another, which could be used to keep each other cooperating instead of defecting? Individuals have lots of direct needs, which is useful for incentives, but countries have fewer: at the end of the day, the only tools for exerting influence on another country are:</p>
<ul>
<li>diplomacy: asking nicely, making a trade,</li>
<li>sanctions: shunning a country, denying them economic opportunities even if it hurts you or others as well,</li>
<li>violence.</li>
</ul>
<p>How do you convince a country like the USA, or Russia, or China, or North Korea, that they do not need an army? Or Ukraine for that matter? Each and every one of these countries will point to the others, and say that <em>they</em> have an army, I must be able to protect myself. To even begin to do so would require trust, and trust is difficult to achieve, as few tools exist to provide protection against malicious action.</p>
<p>The only way I see, even in theory, is if diplomacy or perhaps sanctions would always be sufficient. With individuals, these do the trick most of the time because individuals have many needs that they cannot easily meet on their own: even for the most primitive needs of shelter, food, and water, almost everybody is reliant on the society to collectively provide them. It&rsquo;s difficult to imagine this for countries.</p>
<p>There is hope. The past century has seen the world become global and economies and cultures have intertwined in a way that they have never before, and it&rsquo;s no coincidence that this has also been the single most peaceful period that humanity has ever experienced. Perhaps indeed technological development, which defined this century, can also take us further and promote ever greater global unity. Perhaps indeed one day nobody will feel like that they need a military.</p>
<p>To close with, I found Hal Wyler&rsquo;s speech on diplomacy in the season 1 finale of the excellent TV show <a href="https://www.imdb.com/title/tt17491088/">The Diplomat</a> inspiring, and I&rsquo;ll quote it here in full:</p>
<blockquote>
<p>We started the Bosnia talks a few days after Suljic launched a bombing campaign that very nearly killed the woman who&rsquo;s now my wife. It was my lot to spend more hours in locked rooms with that man than in the hospital with Kate. First time I met him, I refused to shake his hand. Rookie move. It probably set the peace back a year.</p>
<p>Communication isn&rsquo;t the key. Diplomacy doesn&rsquo;t open doors with a twist of the wrist. Diplomacy never works. It never fucking works. Diplomacy is 40 days and nights in a Vienna hotel room, listening to the same empty talking points. Getting trashed at the minibar. It&rsquo;s getting to &ldquo;no&rdquo; over, and over, and over. The answer will be &ldquo;no,&rdquo; so don&rsquo;t stop when you hear it. Diplomacy never works.</p>
<p>Until it does. I&rsquo;ve given 30 years of my life for two moments. When enemies stood on blood-soaked ground&hellip; and grasped hands. I&rsquo;d give it 30 more.</p>
<p>The second round of talks with Suljic&hellip; I shook his hand. Two years later, he was a tired man hoping for peace, and together&hellip; we ended the war.</p>
<p>One of the boneheaded truisms of foreign policy is that talking to your enemies legitimizes them. Talk to everyone. Talk to the dictator, and the war criminal. Talk to the poor schmuck three levels down who&rsquo;s so pissed he has to sit in the back of the second car, he may be ready to turn. Talk to terrorists. Talk to everyone. Fail, and fail again. And brush yourself off. And fail again. Because maybe&hellip; Maybe.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/15/who-is-afraid.html">Who is Afraid of Charles Darwin? (Homo Deus)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/15/who-is-afraid.html" class="u-url"><time class="dt-published" datetime="2023-09-15 17:29:26 &#43;0200">Sep 15, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;m <a href="https://blog.gaborkozar.me/2023/08/27/the-paradox-of.html">continuing to read</a> <a href="https://app.thestorygraph.com/books/c168be57-aaab-44ef-933a-c8c6687e01f2">Homo Deus: A Brief History of Tomorrow</a>, which, by the way, is everything but brief at 496 pages. I&rsquo;m currently at page 180 though, so making steady progress. Yuval Noah Harari, the author, continues to present interesting ideas and narratives.</p>
<p>One interesting thing that is examined is the belief that humans are special. According to Christianity, God created us in his own image, and explicitly created the world for us, imbuing humans and only humans with an eternal soul and promises of an afterlife in either Heaven or Hell. Science, however, has a beef to pick with this line of thinking and sent their champion Charles Darwin and Evolution to have a word. After all, if humans have evolved from animals (arguably with more or less success), and our ancients weren&rsquo;t all that much different from chimpanzees today, then at which point exactly would have acquired something like a soul?</p>
<p>From the section titled &ldquo;Who&rsquo;s Afraid of Charles Darwin?&rdquo; (page 119):</p>
<blockquote>
<p>According to a 2012 Gallup survey, only 15 per cent of Americans think that Homo sapiens evolved through natural selection alone, free of all divine intervention; 32 per cent maintain that humans may have evolved from earlier life forms in a process lasting millions of years, but God orchestrated this entire show; 46 per cent believe that God created humans in their current form sometime during the last 10,000 years, just as the Bible says. Spending three years in college has absolutely no impact on these views. [&hellip;] Even among holders of MA and PhD degrees, 25 per cent believe the Bible, whereas only 29 per cent credit natural selection alone with the creation of our species.</p>
<p>Though schools evidently do a very poor job teaching evolution, religious zealots still insist that it should not be taught at all. Alternatively, they demand that children must also be taught the theory of intelligent design, according to which all organisms were created by the design of some higher intelligence (aka God). &ldquo;Teach them both theories,' say the zealots, &lsquo;and let the kids decide for themselves.&rdquo;</p>
</blockquote>
<p>Oh dear, those numbers are distressing: I was aware that creationism was a thing in the USA, but not that it was this bad. Though I guess I shouldn&rsquo;t have been surprised, given some attitudes towards climate change or abortion. Yikes. Anyway:</p>
<blockquote>
<p>Why does the theory of evolution provoke such objections, whereas nobody seems to care about the theory of relativity or quantum mechanics? How come politicians don&rsquo;t ask that kids be exposed to alternative theories about matter, energy, space and time? After all, Darwin&rsquo;s ideas seem at first sight far less threatening than the monstrosities of Einstein and Werner Heisenberg.</p>
<p>The theory of evolution rests on the principle of the survival of the fittest, which is a clear and simple idea. In contrast, the theory of relativity and quantum mechanics argue that you can twist time and space, that something can appear out of nothing, and that a cat can be both alive and dead at the same time. This makes a mockery of our common sense, yet nobody seeks to protect innocent schoolchildren from these scandalous ideas. Why?</p>
<p>The theory of relativity makes nobody angry, because it doesn&rsquo;t contradict any of our cherished beliefs. Most people don&rsquo;t care an iota whether space and time are absolute or relative. If you think it is possible to bend space and time, well, be my guest. Go ahead and bend them. What do I care? In contrast, Darwin has deprived us of our souls. If you really understand the theory of evolution, you understand that there is no soul.</p>
</blockquote>
<p>Awkward.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/15/liking-what-you.html">Liking What You See: disabling our perception of beauty?</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/15/liking-what-you.html" class="u-url"><time class="dt-published" datetime="2023-09-15 16:45:23 &#43;0200">Sep 15, 2023</time></a>

		<div class="e-content">
			 <blockquote>
<p>Beauty is the promise of happiness.</p>
</blockquote>
<p>(<a href="https://en.wikiquote.org/wiki/Stendhal">Stendhal</a>)</p>
<p>I&rsquo;ve been reading Ted Chiang&rsquo;s short story collection <a href="https://app.thestorygraph.com/books/c34ac03b-3b0e-4a24-b694-e46674918a3a">Stories of Your Life and Others</a> and I have to say that I can really recommend it. I&rsquo;m rarely in the mood for short stories, but every single story here presents a unique and interesting idea, usually in the realm of science fiction, and tries to imagine what its consequences would be for how the world looks like and how life is.</p>
<p>You may have heard of or seen the movie <a href="https://www.imdb.com/title/tt2543164/">Arrival (2016)</a>; it is based on one of the stories in this novella collection titled Story of Your Life. The two do explore the same idea a bit differently, but both are excellent presentations in my opinion.</p>
<p>There are a great many other fascinating ideas in here as well, but now I&rsquo;d highlight one of them: the short story Liking What You See and its idea of &ldquo;calliagnosia&rdquo;: technology that, when turned on, impairs your brain&rsquo;s ability to perceive the <em>looks</em> of others as ugly or beautiful. It doesn&rsquo;t impact your ability to recognize people, or to find them attractive or unattractive: it just means that the way they look no longer triggers any emotional response in you.</p>
<p>I am hardly going to shock anyone when I state that looks are, um, important: beautiful people enjoy a considerable advantage over ugly ones in every area of life. Social media has been recently calling this <a href="https://www.myimperfectlife.com/features/pretty-privilege">pretty privilege</a>. The effect is in a large part due to <a href="https://www.verywellmind.com/what-is-the-halo-effect-2795906">The Halo Effect</a>:</p>
<blockquote>
<p>The halo effect is a type of cognitive bias in which our overall impression of a person influences how we feel and think about their character. Essentially, your overall impression of a person (&ldquo;He is nice!&quot;) impacts your evaluations of that person&rsquo;s specific traits (&ldquo;He is also smart!&quot;). Perceptions of a single trait can carry over to how people perceive other aspects of that person.</p>
</blockquote>
<p>Of course, perception of beauty is subjective in many ways, but some factors are less variable than others, especially when it comes to physical beauty. For example, features like facial symmetry and a clear, unblemished skin are pretty much universally considered attractive. There is evidence that the human brain has built-in functions to judge the attractiveness of others, especially based on their face. This is an area of significant research, so thankfully I can look more authentic and serious if I cite a scientific paper and I will do without remorse: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6523404/">Perception and Deception: Human Beauty and the Brain</a>:</p>
<blockquote>
<p>Human physical characteristics and their perception by the brain are under pressure by natural selection to optimize reproductive success. Men and women have different strategies to appear attractive and have different interests in identifying beauty in people. Nevertheless, men and women from all cultures agree on who is and who is not attractive, and throughout the world attractive people show greater acquisition of resources and greater reproductive success than others. The brain employs at least three modules, composed of interconnected brain regions, to judge facial attractiveness [&hellip;]. Key elements that go into the judgment are age and health, as well as symmetry, averageness, face and body proportions, facial color and texture.</p>
</blockquote>
<p>Hopefully the idea behind calliagosia or calli for short is evident: our brains are wired to judge people in a large part based on their looks, while we are, in principle, trying to build a world that is fairer than that, for example by telling kids things like &ldquo;true beauty comes from within&rdquo;. From the short story:</p>
<blockquote>
<p>The deeper societal problem is lookism. For decades people have been willing to talk about racism and sexism, but they are still reluctant to talk about lookism. Yet this prejudice against unattractive people is incredibly pervasive. People do it without even being taught by anyone, which is bad enough, but instead of combating this tendency, modern society actively reinforces it.</p>
<p>Educating people, raising their awareness about this issue, all of that is essential, but it&rsquo;s not enough. That&rsquo;s where technology comes in. Think of calliagnosia as a kind of assisted maturity. It lets you do what you know you should: ignore the surface, so you can look deeper.</p>
</blockquote>
<p>Indeed, beauty is often weaponized: advertisements and movies feature predominantly models, and headhunters and sales people ruthlessly take advantage of the halo effect to be successful at their jobs. Then there&rsquo;s the cost on the individual: the pressure to be beautiful, especially among women, is a source of endless anxiety and creates a huge demand for beauty products, making that a huge industry in and of itself.</p>
<p>I really like this short story not just because of the idea, or how it&rsquo;s presented &ndash; as a sort of a documentary, highlighting different perspectives and experiences &ndash; but also because towards the end it asks a very important question: where do we stop? As our scientific understanding and technology develops, we will be able to affect our lives in ever more fundamental ways. Suppose that we eliminate our perceptions for physical beauty, okay, but how about the sound of one&rsquo;s voice or how they speak for instance? Do we eliminate that as well? Doesn&rsquo;t a person with a naturally beautiful voice enjoy an unfair advantage similarly to one whose face is beautiful? There are no easy answers:</p>
<blockquote>
<p>We&rsquo;ve reached a point where we can begin to adjust our minds. The question is, when is it appropriate for us to do so? We shouldn&rsquo;t automatically accept that natural is better, nor should we automatically presume that we can improve on nature. It&rsquo;s up to us to decide which qualities we value, and what&rsquo;s the best way to achieve those.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/06/scott-alexanders-presidential.html">Scott Alexander&#39;s Presidential Platform of Satire</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/06/scott-alexanders-presidential.html" class="u-url"><time class="dt-published" datetime="2023-09-06 00:21:43 &#43;0200">Sep 6, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://www.astralcodexten.com/p/my-presidential-platform">My Presidential Platform</a> by Scott Alexander gave me a good series of chuckles. It&rsquo;s a truly inspiring combination of <em>absolutely stupid</em> and <em>unusually clever</em>, and should serve as a textbook example of &ldquo;thinking outside the box&rdquo;.</p>
<p>It was all worth it just for this:</p>
<blockquote>
<p>The American rich already enjoy spending their money on exciting vehicles - yachts for the normies, rockets for the more ambitious, Titanic submersibles for the suicidal. Why not redirect this impulse towards public service? Imagine the fear it would strike into the hearts of the Chinese when the USS Musk enters Ludicrous Mode in the waters off the Taiwan Strait, with Elon himself at the wheel.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/02/the-attack-of.html">The attack of the AI-generated mushroom foraging books</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/02/the-attack-of.html" class="u-url"><time class="dt-published" datetime="2023-09-02 02:07:06 &#43;0200">Sep 2, 2023</time></a>

		<div class="e-content">
			 <p>And so it&hellip; begins? Continues? Let&rsquo;s say continues to begin: <a href="https://www.theguardian.com/technology/2023/sep/01/mushroom-pickers-urged-to-avoid-foraging-books-on-amazon-that-appear-to-be-written-by-ai">Mushroom pickers urged to avoid foraging books on Amazon that appear to be written by AI</a> (seen on <a href="https://news.ycombinator.com/item?id=37354577">Hacker News</a>).</p>
<blockquote>
<p>Amateur mushroom pickers have been urged to avoid foraging books sold on Amazon that appear to have been written by artificial intelligence chatbots.</p>
<p>Amazon has become a marketplace for AI-produced tomes that are being passed off as having been written by humans, with travel books among the popular categories for fake work.</p>
<p>Now a number of books have appeared on the online retailer’s site offering guides to wild mushroom foraging that also seem to be written by chatbots. The titles include “Wild Mushroom Cookbook: form [sic] forest to gourmet plate, a complete guide to wild mushroom cookery” and “The Supreme Mushrooms Books Field Guide of the South-West”.</p>
</blockquote>
<p>This is considerably less amusing than <a href="https://www.mirror.co.uk/news/uk-news/robot-writes-engineers-obituary-its-25705438">Robot writes engineer&rsquo;s obituary and it&rsquo;s as good as any human&rsquo;s tribute</a>:</p>
<blockquote>
<p>Brenda Tent retired from living at the age of old, surrounded by family and natural causes. A librarian from birth, Brenda was an avid collector of dust. She had a sweet heart and married her high school. She loved having hobbies and helping her sons to be disadvantaged youths. She had no horses but thought she did. The church gave her a choir because she sang like bird and looked like bird and Brenda was a bird. She owed us so many poems.</p>
<p>The funeral will be held in 1977 at heaven. In lieu of flowers, send Brenda more life.</p>
</blockquote>
<p>It is honestly kind of surprising that it took this long; it&rsquo;s been almost a full year since ChatGPT was released. AI-generated content like these &ldquo;books&rdquo; cost next to nothing to produce, so <em>any</em> amount of money you can make with from them is just good business&hellip; while at the same time clearly leads to a future where our world drowns in low-quality AI-generated trash &ldquo;content&rdquo;.</p>
<blockquote>
<p>An Amazon spokesperson said: “We take matters like this seriously and are committed to providing a safe shopping and reading experience. We’re looking into this.”</p>
</blockquote>
<p>Yeah, I won&rsquo;t hold my breath, not just because I don&rsquo;t trust Amazon, but because fixing this is essentially impossible (at scale). AI does not exist but it will ruin everything anyway indeed:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/EUrOxh_0leE' frameborder='0' allowfullscreen></iframe></div>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/09/01/ncleanstall-the-nvidia.html">NCleanstall: the NVIDIA driver installer we don&#39;t deserve</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/09/01/ncleanstall-the-nvidia.html" class="u-url"><time class="dt-published" datetime="2023-09-01 01:19:27 &#43;0200">Sep 1, 2023</time></a>

		<div class="e-content">
			 <p>Installing NVIDIA Graphics drivers have become annoying. For one, they bundle this GeForce Experience horribleness in it, which requires an NVIDIA account (WTF is that and why would I ever want one?) but in turn has a lot of useful features locked behind it: <strong>automatic driver updates</strong>, for one. Thanks, I hate it. What we as a society have done with technology sometimes just makes me sad.</p>
<p>Thankfully I was not alone in despising the situation, and someone wonderful created <a href="https://www.techpowerup.com/download/techpowerup-nvcleanstall/">NVCleanstall</a> that restored my faith in humanity. It allows you to strip out all the stuff you don&rsquo;t want or need from the NVIDIA installers, <em>and</em> notifies you when there is a new version available. There is a guide available at <a href="https://www.makeuseof.com/customize-nvidia-driver-installation-with-cleanstall/">MakeUseOf</a> if you need help with configuring it.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/27/the-paradox-of.html">The paradox of historical knowlage (Homo Deus)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/27/the-paradox-of.html" class="u-url"><time class="dt-published" datetime="2023-08-27 01:42:51 &#43;0200">Aug 27, 2023</time></a>

		<div class="e-content">
			 <p>As I mentioned in a <a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html">previous post</a>, I&rsquo;ve been reading <a href="https://app.thestorygraph.com/books/c168be57-aaab-44ef-933a-c8c6687e01f2">Homo Deus: A Brief History of Tomorrow</a>. I found this part very interesting (pages 65-67. &ldquo;The Paradox of Knowledge&rdquo;):</p>
<blockquote>
<p>In the middle of the nineteenth century Karl Marx reached brilliant economic insights. Based on these insights he predicted an increasingly violent conflict between the proletariat and the capitalists, ending with the inevitable victory of the former and the collapse of the capitalist system. Marx was certain that the revolution would start in countries that spearheaded the Industrial Revolution - such as Britain, France and the USA - and spread to the rest of the world.</p>
<p>Marx forgot that capitalists know how to read. At first only a handful of disciples took Marx seriously and read his writings. But as these socialist firebrands gained adherents and power, the capitalists became alarmed. They too perused Das Kapital, adopting many of the tools and insights of Marxist analysis. In the twentieth century everybody from street urchins to presidents embraced a Marxist approach to economics and history. [&hellip;]</p>
<p>As people adopted the Marxist diagnosis, they changed their behaviour accordingly. Capitalists in countries such as Britain and France strove to better the lot of the workers, strengthen their national consciousness and integrate them into the political system. Consequently when workers began voting in elections and Labour gained power in one country after another, the capitalists could still sleep soundly in their beds. As a result, Marx&rsquo;s predictions came to naught. Communist revolutions never engulfed the leading industrial powers such as Britain, France and the USA and the dictatorship of the proletariat was consigned to the dustbin of history.</p>
<p>This is the paradox of historical knowledge. Knowledge that does not change behaviour is useless. But knowledge that changes behaviour quickly loses its relevance.</p>
</blockquote>
<p>It&rsquo;s an interesting narrative and I think the overall the idea makes sense, but I&rsquo;m not sure I fully buy it here.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/26/zen-story-maybe.html">Zen Story: Maybe</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/26/zen-story-maybe.html" class="u-url"><time class="dt-published" datetime="2023-08-26 01:56:16 &#43;0200">Aug 26, 2023</time></a>

		<div class="e-content">
			 <p>I came by this story the other day: <a href="https://thedailyzen.org/2015/03/20/zen-story-maybe/">Zen Story: Maybe</a>. Since it&rsquo;s very short, I&rsquo;ll just quote it in full here:</p>
<blockquote>
<p>There is a Taoist story of an old farmer who had worked his crops for many years. One day his horse ran away. Upon hearing the news, his neighbors came to visit. “Such bad luck,” they said sympathetically. “Maybe,” the farmer replied.</p>
<p>The next morning the horse returned, bringing with it three other wild horses. “How wonderful,” the neighbors exclaimed. “Maybe,” replied the old man.</p>
<p>The following day, his son tried to ride one of the untamed horses, was thrown, and broke his leg. The neighbors again came to offer their sympathy on his misfortune. “Maybe,” answered the farmer.</p>
<p>The day after, military officials came to the village to draft young men into the army. Seeing that the son’s leg was broken, they passed him by. The neighbors congratulated the farmer on how well things had turned out. “Maybe,” said the farmer.</p>
</blockquote>
<p>Life is unpredictable. Losing something will at times lead to winning something greater, but just as often it will not. &ldquo;When one door closes, another opens&rdquo;, except in real life, sometimes there is no other door, and the loss is just that: a loss. I think that is the truest test of one&rsquo;s Zen.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/24/languagelearning-ability-children.html">Language-learning ability: children vs adults</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/24/languagelearning-ability-children.html" class="u-url"><time class="dt-published" datetime="2023-08-24 03:04:27 &#43;0200">Aug 24, 2023</time></a>

		<div class="e-content">
			 <p>A long, long article summarizing lots of research on language-learning ability, and whether the commonly shared belief that children are much better at learning languages than adults actually holds true: <a href="https://astralcodexten.substack.com/p/critical-periods-for-language-much">Critical Periods For Language: Much More Than You Wanted To Know</a>.</p>
<p>In summary:</p>
<blockquote>
<p>Children seem to be able to pick up second languages faster than adults. It’s hard to tell exactly when the learning rate slows, or to be sure this is a biological phenomenon instead of an effect of school ending or picking low-hanging fruits. Mastering a language perfectly in adulthood is hard, but maybe just because there’s not enough time to learn it at adult’s slower learning rates.</p>
<p>Babies don’t seem any better than older children (eg 17 year olds), and are limited by being babies. The difference between their excellent ability to learn a first language, and (for example) a middle-schooler struggling to learn a second language, probably is just exposure and motivation, and not an additional magic language ability.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html">Progress without brakes, for good or ill (Homo Deus)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/24/progress-without-brakes.html" class="u-url"><time class="dt-published" datetime="2023-08-24 01:15:24 &#43;0200">Aug 24, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;ve been reading <a href="https://app.thestorygraph.com/books/c168be57-aaab-44ef-933a-c8c6687e01f2">Homo Deus: A Brief History of Tomorrow</a>. In the first chapter <em>The new human agenda</em>, the author gives his interpretation of history and how humanity has ended up as it is today due to scientific and technological development &ndash; roughly as follows: humans seeking comfort and happiness developed technology to try to eradicate or at least mitigate ills such as famines and plagues. I find this view to be a bit naive, and I would at least add &ldquo;seeking better ways of killing each other&rdquo; to the list of factors encouraging technological development; but I can also see the argument that developing weapons is also ultimately about comfort and happiness, via the morally-perhaps-questionable route of, well, <em>acquiring</em> an abundance of resources and then protecting it.</p>
<p>Nonetheless, he then continues on to assert that this same drive will keep on transforming our world with unforeseeable consequences. I found this part to be a good reality check:</p>
<blockquote>
<p>When people realize just how fast we are rushing towards the unknown, and that they cannot count even on death to shield them from it, their reaction is to hope that somebody will hit the brakes and slow us down. But we cannot hit the breaks, for several reasons.</p>
<p>Firstly, nobody knows where the brakes are. While some experts are familiar with developments in one field, such as artificial intelligence, nanotechnology, big data or genetics, no one is an expert on everything. No one is therefore capable of connecting all the dots and seeing the full picture. Different fields influence one another in such intricate ways that even the best minds cannot fathom how breakthroughs in artificial intelligence might impact nanotechnology or vice versa. Nobody can absorb all the latest scientific discoveries, nobody can predict how the global economy will look in ten years, and nobody has a clue where we are heading in such a rush. Since no one understands the system anymore, no one can stop it.</p>
<p>Secondly, if we somehow succeed in hitting the brakes, our economy will collapse, along with our society. As explained in a later chapter, modern economy needs constant and indefinite growth in order to survive. If growth ever stops, the economy won&rsquo;t settle down to some cozy equilibrium; it will fall to pieces. [&hellip;]</p>
</blockquote>
<p>(Page 58-59, &ldquo;Can Someone Please Hit the Breaks?&quot;)</p>
<p>The book is definitely growing on me: it is not the one-sided, ever-optimistic rosy vision of a utopistic future built by Us Enlightened Humans (tm) that I expected, but rather a fairly nuanced analysis of the fundamental forces that motivate humans and got us to where we are and will continue to drive us &ndash; for good or ill.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/22/a-guide-to.html">A guide to error-handling</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/22/a-guide-to.html" class="u-url"><time class="dt-published" datetime="2023-08-22 03:44:27 &#43;0200">Aug 22, 2023</time></a>

		<div class="e-content">
			 <p>Error-handling is a heavily debated topic. Generally speaking, the following mechanisms exist to handle errors when implementing a function:</p>
<ol>
<li>Return something to indicate the error: an error code, a sentinel value (like <code>NaN</code>), or using a result-or-error type (like C++&rsquo;s <code>std::expected</code> or Rust&rsquo;s <code>Result</code>).</li>
<li>Throw an exception and unwind the stack (i.e. walk it backwards) until a handler is found and return control there; or if no viable handlers exist, terminate the application.</li>
<li>Abort or terminate the application, after printing an error message and generating a core dump. (<code>assert()</code> is also just a flavor of this.)</li>
</ol>
<p>Notably, options 1 and 2 (return values and exceptions) are <em>recoverable</em>, while option 3 (terminate) is not. So let&rsquo;s start with these two categories: when do you want errors to be recoverable? Some errors are almost always recoverable, such as attempting to open a file that does not exist (maybe we should create it then, or try another file), and some are almost never such as encountering inconsistent application state (which usually indicates a programming error or memory corruption).</p>
<h2 id="recoverable-vs-non-recoverable-errors">Recoverable vs non-recoverable errors</h2>
<p>In my experience, the answer mostly depends on the context of your code: are you writing application code or library code? A library code that will be used across many applications usually shouldn&rsquo;t decide on the application&rsquo;s behalf whether that should crash or not, as it doesn&rsquo;t tend to be aware of the context of its own usage: it cannot tell whether the using application is a critical service that must never go down or perhaps just a one-off script. Therefore libraries should typically use recoverable forms of error reporting, even for severe errors.</p>
<p>When writing application code, simply crashing on an error may be an option, but again depends on the type of application and whether you are aware of all possible contexts it may run in. As a rule of thumb however, it&rsquo;s better for most applications to crash rather than risk doing something unintended or corrupting data.</p>
<p>So in application code, I tend to use assertions a lot, even for things like function preconditions when I know all contexts and all places where the function may be called from, since in these cases a precondition failure must be a programming error and I want to crash early rather than risk doing something unintended. In library code, I tend to use assertions only as sanity checks: conditions that must always hold, barring a severe programming error inside the library or memory corruption.</p>
<p>There are, of course, caveats. For example, even if an error is unrecoverable, you may want to use a recoverable mechanism for reporting it to give a chance for user code to gracefully shut down and signal the error to downstream services or dump its internal state to help with debugging. This is a painful use case because it is a subversion of the whole concept of an unrecoverable error; this is one main reason why many (most?) programming languages typically rely on exceptions for reporting every type of error. (The other being simplicity by having fewer competing mechanisms in the language.)</p>
<p>Finally, some errors may be unrecoverable in the vast majority of cases, but actually recoverable in a few: one example is a memory allocation failure. Most libraries and applications in most cases just want to crash when this happens because there is nothing sane they can do otherwise, but sometimes you know you&rsquo;re loading a gigantic amount of data into memory and can gracefully handle running out of memory; or you are using something like an arena allocator or ring buffer where running out of space can happen as part of normal operations.</p>
<h2 id="reporting-recoverable-errors">Reporting recoverable errors</h2>
<p>To recap, the two main ways of reporting errors in way that is recoverable, i.e. it&rsquo;s up to the caller to decide what to do with, is by using the function return value mechanism by returning an error value or by throwing an exception and walking upwards on the call stack until a suitable handler could be found. They both have advantages and disadvantages.</p>
<p>Returning an error is conceptually and implementationally simple, and usually very efficient (mostly depending on whether how expensive is it to construct the error object). However, propagating errors (i.e. transporting them from where the error occurred to a level where we can handle them) tends to be very tedious. Some languages like Rust provide language-level support to do this conveniently, but this is very painful in languages like C and Go given that most function calls have no reasonable ways of handling errors, so usually errors have to propagate a good distance up before reaching code that knows what to do with them.</p>
<p>Exceptions were designed with that last insight in mind, and explicitly allow the error site (where the error is detected) and the handler to be arbitrarily far away from each other, without this having to impact any other code in between. In return though, exceptions tend to be very expensive in terms of runtime performance, much more so than returning error values.</p>
<p>Further, the fact that in a call chain of <code>A -&gt; B -&gt; C</code> the fact that <code>C</code> may throw exceptions that <code>A</code> knows how to catch while B has no knowledge of any of this can be viewed as an anti-feature: when a developer is looking at the code of <code>B</code>, they may not be aware that <code>C</code> may throw. In the error-returning approach, the possible presence of errors is always spelled out explicitly, to the point that it can easily become annoying (because e.g. you have to keep writing <code>.unwrap()</code> in Rust or <code>if err != nil then return err</code> in Go); but the opposite can also easily be counter-productive.</p>
<h2 id="exceptions-in-c">Exceptions in C++</h2>
<p>C++ makes things more complicated by offering both options, in a typical C++ fashion. You may return error codes, or use the newly added <a href="https://en.cppreference.com/w/cpp/utility/expected"><code>std::expected</code></a> which is basically a specialized union that either represents the function result or an error, but has no special handling or syntactical sugar support.</p>
<p>You may also throw and catch exceptions in the same way as many mainstream languages, but unlike them, all major C++ compilers let you disable exceptions altogether by passing a compiler flag like <code>-fno-exceptions</code>. The C++ standard library does use exceptions, but inconsistently: the C++17 <code>std::filesystem</code> implementation attempts to cater to both groups by offering two distinct set of functions that do the same thing, except one set uses exceptions and the other error codes (see e.g. <a href="https://en.cppreference.com/w/cpp/filesystem/create_directory"><code>std::filesystem::create_directory()</code></a>).</p>
<p>This should already tell you that the C++ community is split on whether exceptions are a good idea or not.
Like with so many problematic things in C++, the concerns about exceptions revolve around performance:</p>
<ul>
<li>Exceptions may be prohibitively expensive or not even supported in resource-constrained embedded environments, where C++ is often used.</li>
<li>Exceptions have a large run-time performance penalty when thrown.</li>
<li>Even when not thrown, the possible presence of exceptions may prohibit some compiler optimizations.</li>
<li>The possibility of exceptions significantly raise the size of the binary due to the unwind code that has to be generated.</li>
</ul>
<p>C++ has so far been trying to cater to both groups: <a href="https://en.cppreference.com/w/cpp/language/noexcept_spec"><code>noexcept</code></a> was added to mark functions that can never throw to try an alleviate some performance concerns about exceptions, and there is a proposal for <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0709r0.pdf">Zero-overhead deterministic exceptions</a>. On the other side, <code>std::variant</code> and <code>std::expected</code> were both added recently, both of which make it easier to return and store error values.</p>
<p>The C++ <code>noexcept</code> specifier is actually a bit of a lie: it doesn&rsquo;t actually promise that the given function cannot throw exceptions, but rather than exceptions will never escape from that function; this would call <code>std::terminate()</code> instead. It still works as a guarantee that can be leveraged by the caller, enabling optimizations by for example not having to emit code to call destructors when an exception is unwinding the stack. The canonical example is resizing an <code>std::vector</code>: when it is full while we are trying to insert a new element, a larger array needs to be allocated and the elements moved before the old array can be deallocated and insertion may proceed. However, a move operation throwing would result in the vector being in an inconsistent state, as some objects may already have been moved while others have not. Therefore, if the move constructor of the element type is declared as non-noexcept, then <code>std::vector</code> will copy objects instead of move them when reallocating, even though copying can be hugely more expensive; however if a copy operation throws then we can simply deallocate the new array and propagate the exception, leaving the vector in its original state.</p>
<h2 id="recoverable-errors-in-other-languages">Recoverable errors in other languages</h2>
<p>It is also interesting to see how different languages deal with error-handling, given its heavily debated nature.</p>
<ul>
<li>Most mainstream languages like Python, Java, C#, JavaScript, and PHP all primarily use exceptions.</li>
<li>C uses error codes given that it has no proper objects nor exceptions, though it does have the facilities to implement exceptions with <code>goto</code>, <a href="https://en.cppreference.com/w/cpp/utility/program/setjmp"><code>setjmp()</code> and <code>longjmp()</code></a>.</li>
<li>Rust mainly relies on a <a href="https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html"><code>Result</code> type</a> which is the equivalent of <code>std::expected</code>, but has dedicated language support. Rust does not have exceptions, but it does have unwinding panics and a way to catch them with <a href="https://doc.rust-lang.org/std/panic/fn.catch_unwind.html"><code>std::panic::catch_unwind</code></a>.</li>
<li>Python uses exceptions not only for errors but also for control flow, e.g. to indicate that there&rsquo;s no more data to consume in an iterator by raising <a href="https://docs.python.org/3/library/exceptions.html#StopIteration"><code>StopIteration</code></a>. Python also has <code>assert</code> as a language keyword, but it raises an <code>AssertionError</code> exception on failure, unlike in other languages.</li>
<li><a href="https://ziglang.org/documentation/master/#Errors">Zig uses an approach</a> similar in spirit to C&rsquo;s error codes but implemented more like Rust&rsquo;s <code>Result</code> or C++&rsquo;s <code>std::expected</code>: functions return either their return value or an error, where an error is essentially a dedicated enum.</li>
<li><a href="https://go.dev/blog/error-handling-and-go">Go has a simple approach</a> where functions typically return a <code>(result, error)</code> pair. Errors are not particularly special, and little-to-no language support exists for them.</li>
<li>The Microsoft .NET Framework that C# relies on used to throw an exception of type <a href="https://stackoverflow.com/a/4104845"><code>System.ExecutionEngineException</code></a> when the runtime detected a corrupted internal state: using a recoverable error mechanism to report a fundamentally unrecoverable error. At least they stopped doing this, and now the class is <a href="https://learn.microsoft.com/en-us/dotnet/api/system.executionengineexception?view=net-7.0">marked as obsolate</a>.</li>
</ul>
<p>I personally particularly dislike Go&rsquo;s approach, and struggle to see how that is a good idea: it doesn&rsquo;t even have the excuse of being an ancient, very low-level language like C.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/20/the-technology-of.html">The technology of Maglev trains</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/20/the-technology-of.html" class="u-url"><time class="dt-published" datetime="2023-08-20 14:18:32 &#43;0200">Aug 20, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;m a big fan of traveling by trains, and it always pains me how impractical it is in most parts of the world, even in places where they would make economic and environmental sense. Europe has been getting better at this, with excellent high-speed rail connections from Amsterdam to Paris for instance, but other than a few anomalies like that, you&rsquo;re mostly out of luck.</p>
<p>Maglev trains are super high-tech and an order of magnitude more expensive than traditional high-speed railways, so they are also much harder to economically justify, but what they lack in that area they more than make up for by the technology just being absolutely <em>awesome</em>. Is wanting to ride one a good enough reason to plan a trip to Japan? You know, it might be!</p>
<p>The Real Engineering channel on YouTube recently published a video on them, which I can heartily recommend:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/S4L_0CDsd1I' frameborder='0' allowfullscreen></iframe></div>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/13/the-carrot-problem.html">The Carrot Problem: sharing the secret of your success</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/13/the-carrot-problem.html" class="u-url"><time class="dt-published" datetime="2023-08-13 16:31:45 &#43;0200">Aug 13, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://www.atvbt.com/the-carrot-problem/">The Carrot Problem</a> seen via <a href="https://news.ycombinator.com/item?id=37100226">Hacker News</a>:</p>
<blockquote>
<p>In World War II, the story goes, the British invented a new kind of onboard radar that allowed its pilots to shoot down German planes at night.</p>
<p>They didn&rsquo;t want the Germans to know about this technology, but they had to give an explanation for their new, improbable powers.</p>
<p>So they invented a propaganda campaign that claimed their pilots had developed exceptional eyesight by eating &ldquo;an excess of carrots.&rdquo;</p>
<p>If you&rsquo;re going to trick people into doing something pointless, eating excessive carrots seems like one of the better ones. Still, there&rsquo;s an issue: people who believed the propaganda and tried to get super-sight would be spending time and effort on something that couldn&rsquo;t possibly solve their issue.</p>
<p>I&rsquo;ll call this a Carrot Problem.</p>
<p>Once you look for Carrot Problems, you see them everywhere. Essentially, any time someone achieves success in a way they don&rsquo;t want to admit publicly, they have to come up with an excuse for their abilities. And that means misleading a bunch of people into (potentially) wasting their time, or worse.</p>
</blockquote>
<p>I love the name, but this is genuinely good insight. I think I&rsquo;d distill the problem a bit more fundamentally down to this: <em>successful individuals or organizations rarely have an interest in sharing what made them successful</em>; most of them time in fact, they are heavily disincentivized from doing so.</p>
<p>The reasons aren&rsquo;t necessarily nefarious: even if you have done nothing immoral or illegal, sharing the secret of your success simply creates more competition for you, which is counter-productive for you.</p>
<p>Further, you may not actually even know what made you successful! Oh you might <em>think</em> that you know: it was definitely all your hard work, or connections, or unique idea, etc. but chances are that in reality you had a generous dose of luck<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> without which it all would have been for nothing.</p>
<p>Keep this in mind whenever you encounter the success story of a famous person or rich company: chances are that even if the story is accurate and true and they are not trying to make you eat carrots, it is unlikely to be reproducible, as it will almost inevitably fail to account for all the hidden &ldquo;lucky&rdquo; factors.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>By luck I mean circumstances that you do not control and are not even fully aware of. There is no such fundamental force in the universe, to the best of my knowledge and understanding.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/13/everything-you-need.html">Everything you need to know about light therapy</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/13/everything-you-need.html" class="u-url"><time class="dt-published" datetime="2023-08-13 15:43:38 &#43;0200">Aug 13, 2023</time></a>

		<div class="e-content">
			 <p>Here is a <a href="https://lorienpsych.com/2020/12/19/light-therapy/">comprehensive guide to light therapy</a> by a renowned and practicing psychiatrist. In short:</p>
<blockquote>
<p>Light therapy can treat seasonal affective disorder (eg winter depression) and sometimes regular depression. Buy this light box [see link below], and at some time between 6 AM and 9 AM, sit exactly 12 inches away from it and do some activity that doesn’t involve staring directly at the light box. Continue every morning for the period of time you’re at risk of depression. If bipolar, don’t try this without medical supervision.</p>
</blockquote>
<p>It is detailed, easy to read and understand (no medical jargon), and no-nonsense.</p>
<blockquote>
<p>If you’ve read this document, you already know more than 95% of psychiatrists about light therapy. Trust me, I’ve given lectures about this to psychiatrists and they don’t know any of this stuff.</p>
</blockquote>
<p>The linked light box: <a href="https://www.amazon.com/dp/B00PCN4UVU?ots=1&amp;slotNum=1&amp;imprToken=36bb3cd9-767a-b515-0b4&amp;ascsubtag=%5B%5Dst%5Bp%5Dcjnm77g18005q1by6kggjizam%5Bi%5Dh0nXBx%5Bd%5DD%5Bz%5Dm%5Bt%5Dw%5Br%5Dgoogle.com&amp;tag=thestrategistsite-20">Carex Day-Light Classic Plus Bright Light Therapy Lamp</a></p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/12/sometimes-setting-yourself.html">Sometimes setting yourself on fire sheds light on the situation</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/12/sometimes-setting-yourself.html" class="u-url"><time class="dt-published" datetime="2023-08-12 03:41:46 &#43;0200">Aug 12, 2023</time></a>

		<div class="e-content">
			 <p>In today&rsquo;s &ldquo;so that is a thing that happened&rdquo; column, found in a completely unrelated article (<a href="https://astralcodexten.substack.com/p/still-alive">Still Alive</a>):</p>
<blockquote>
<p>In Street Fighter, the hero confronts the Big Bad about the time he destroyed her village. The Big Bad has destroyed so much stuff he doesn&rsquo;t even remember: &ldquo;For you, the day [I burned] your village was the most important day of your life. For me, it was Tuesday.&rdquo;</p>
<p>&hellip;</p>
<p>In 2010, a corrupt policewoman demanded a bribe from impoverished pushcart vendor Mohammed Bouazizi. He couldn&rsquo;t afford it. She confiscated his goods, insulted him, and (according to some sources) slapped him. He was humiliated and destitute and had no hope of ever getting back at a police officer. So he made the very reasonable decision to douse himself in gasoline and set himself on fire in the public square. One thing led to another, and eventually a mostly-peaceful revolution ousted the government of Tunisia. I am very sorry for Mr. Bouazizi and his family. But he did find a way to make the offending policewoman remember the day she harassed him as something other than Tuesday. As the saying goes, &ldquo;sometimes setting yourself on fire sheds light on the situation&rdquo;.</p>
</blockquote>
<p>That is absolutely horrible, but I also feel like I have to ask about the practical applicability of that saying in the last sentence. (Also, that &ldquo;One thing led to another&rdquo; is carrying a whole lot in there.)</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/12/designs-vs-the.html">Designs vs the world</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/12/designs-vs-the.html" class="u-url"><time class="dt-published" datetime="2023-08-12 02:11:31 &#43;0200">Aug 12, 2023</time></a>

		<div class="e-content">
			 <p>A fun short article I found today: <a href="https://cabel.com/2023/07/30/fantasy-meets-reality/">Fantasy Meets Reality</a></p>
<blockquote>
<p>One of my favorite things to notice as a weirdo is when the good intentions of design slam into the hard reality of humans and the real world. It’s always interesting. [&hellip;]</p>
<p>When it comes to design in the real world, there are a few basic rules that seem to always apply: if it looks neat, people will want to take a photo with it. If it looks comfortable, people will want to sit on it. If it looks fun, people will play around on it. Etc.</p>
<p>And yet, designers are often still caught by surprise!</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		

		<a href="https://blog.gaborkozar.me/2023/08/11/i-didnt-know.html" class="u-url"><time class="dt-published" datetime="2023-08-11 01:05:29 &#43;0200">Aug 11, 2023</time></a>

		<div class="e-content">
			 <p>I didn&rsquo;t know, but apparently iPhone 14 and later offer <a href="https://support.apple.com/en-us/HT213426">Emergency SOS via satellite</a>:</p>
<blockquote>
<p>Emergency SOS via satellite can help you connect with emergency services under exceptional circumstances when no other means of reaching emergency services are available. If you call or text emergency services and can&rsquo;t connect because you&rsquo;re outside the range of cellular and Wi-Fi coverage, your iPhone tries to connect you via satellite to the help that you need.</p>
</blockquote>
<p>Apparently this has <a href="https://daringfireball.net/linked/2023/08/10/maui-emergency-sos">saved some people&rsquo;s lives</a> during the recent Maui wildfires.</p>
<p>Hell yeah, technology!</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		

		<a href="https://blog.gaborkozar.me/2023/08/11/i-recently-encountered.html" class="u-url"><time class="dt-published" datetime="2023-08-11 00:49:34 &#43;0200">Aug 11, 2023</time></a>

		<div class="e-content">
			 <p>I recently encountered this on <a href="https://news.ycombinator.com/item?id=37080506">Hacker News</a>: <a href="https://kentonshouse.com/">Kenton&rsquo;s House</a></p>
<blockquote>
<p>I used to own a house optimized for LAN parties, with fold-out gaming stations built into the walls. (I sold it in early 2020 and moved to Austin, TX, where I plan to build another one&hellip;)</p>
</blockquote>
<p>Now <em>that</em> is cool! Is that the dream? It very well might be.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/07/the-relative-age.html">The Relative Age Effect: How Your Birth Month Impacts Your Success</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/07/the-relative-age.html" class="u-url"><time class="dt-published" datetime="2023-08-07 00:45:57 &#43;0200">Aug 7, 2023</time></a>

		<div class="e-content">
			 <p>The key insight behind the <a href="https://en.wikipedia.org/wiki/Relative_age_effect">Relative Age Effect</a> is that schools and all types of education all over the world have fixed yearly start dates, and which year a kid starts depends on their age measured in years. One year is quite a long time, especially when it comes to the development of children: consider that they start attending primary school at the age of 6. The development difference between somebody 6 years and 1 day old vs 6 years and 364 days old is massive: the latter has been alive for 17% longer and had all that extra time to develop. This means that they are taller, stronger, and smarter: they will most likely do better in school or any areas of life.</p>
<p>Due to the Matthew Effect of compounding advantages, this advantage early on adds up and has a huge effect for success later in life, in a way that is statistically <em>glaringly</em> obvious as the video demonstrates. Children born in different months of the same year will have hugely different outcomes. Fascinating!</p>
<p>This is an excellent video explaining it with good visualizations and lots of sources and references:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/eYRgTj9MRLg' frameborder='0' allowfullscreen></iframe></div>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/08/07/modern-movies-on.html">Modern movies on human bodies: everyone is beautiful and no one is horny</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/08/07/modern-movies-on.html" class="u-url"><time class="dt-published" datetime="2023-08-07 00:08:29 &#43;0200">Aug 7, 2023</time></a>

		<div class="e-content">
			 <p>An interesting, thought-provoking, and well-written article:</p>
<h2 id="everyone-is-beautiful-and-no-one-is-hornyhttpsbloodknifecomeveryone-beautiful-no-one-horny"><a href="https://bloodknife.com/everyone-beautiful-no-one-horny/">Everyone is beautiful and no one is horny</a></h2>
<blockquote>
<p>Modern action and superhero films fetishize the body, even as they desexualize it.</p>
</blockquote>
<p>See also the discussion on <a href="https://news.ycombinator.com/item?id=36990215">Hacker News</a>.</p>
<blockquote>
<p>Of course there’s sex in a movie. Isn’t there always?</p>
<p>The answer, of course, is not anymore—at least not when it comes to modern blockbusters</p>
<p>We’re told that Tony Stark and Pepper Potts are an item, but no actual romantic or sexual chemistry between them is shown in the films. [&hellip;]</p>
</blockquote>
<p>I&rsquo;m not a particularly avid movie-watcher, and I feel mostly uninterested in the superhero movies that appear to have flooded the cinemas in the last few years. I say this to highlight the point that <em>even I have noticed this</em>: the utter sterility of many (most?) mainstream movies with their total lack of sexual chemistry. I have thought that this is just Hollywood appealing to disappointingly dated purist sensitivities that the United States seems to be eternally stuck in, but this article offers a much more interesting exploration:</p>
<blockquote>
<p>When a nation feels threatened, it gets swole. Germans and Norwegians became obsessed with individual self-improvement through physical fitness around the end of the Napoleonic Era. British citizens took up this Physical Culture as the 19th century—and their empire—began to wane. And yoga, in its current practice as a form of meditative strength training, came out of the Indian Independence movement of the 1920s and 30s.</p>
<p>The impetus of these movements isn’t fitness for the sake of pleasure, for the pure joys of strength and physical beauty. It’s competitive. It’s about getting strong enough to fight The Enemy, whoever that may be.</p>
<p>The United States is, of course, not immune to this. [&hellip;] The attack on the World Trade Center and the Pentagon sparked a new War on Terror, and America needed to get in shape so we could win that war. The USA’s hyper-militaristic troop-worshipping post-9/11 culture seeped into the panic over obesity and gave birth to a terrifying, swole baby.</p>
</blockquote>
<p>Go read the article! I&rsquo;m going to just pull this one paragraph out of the article without context, because I found it hilarious and accurate:</p>
<blockquote>
<p>Contemporary gym ads focus on rigidly isolated self-improvement: be your best self. Create a new you. We don’t exercise, we don’t work out: we train, and we train in fitness programs with names like Booty Bootcamp, as if we’re getting our booties battle-ready to fight in the Great Booty War. There is no promise of intimacy. Like our heroes in the Marvel Cinematic Universe, like Rico and Dizzy and all the other infantry in Starship Troopers, we are horny only for annihilation.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/27/why-is-online.html">Why is online dating terrible?</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/27/why-is-online.html" class="u-url"><time class="dt-published" datetime="2023-07-27 00:48:05 &#43;0200">Jul 27, 2023</time></a>

		<div class="e-content">
			 <p>It&rsquo;s not exactly controversial to say that online dating is awful, at least for those seeking heterosexual relationships. It sucks very differently for men and women though, and from what I hear it can actually be pretty good for homosexual relationships. (Especially for gay men, I think? I haven&rsquo;t really heard about it for women, but then again I do live under a rock, in a cave, and on another planet.)</p>
<p>This difference is interesting and worth exploring, though anything as socially and culturally involved as sex, dating, and relationships is obviously riddled with proverbial mines. I&rsquo;ve recently come across this video though that I think did an unusually good job at analyzing the situation:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/x3lypVnJ0HM' frameborder='0' allowfullscreen></iframe></div>
<p>He highlights two main problems: there are many more men on these dating apps then women, and women appear to judge the profiles of men much more harshly for attractiveness than vica-versa. It&rsquo;s obviously nigh-impossible to make online dating not be awful, with such a premise, though various attempts have been made.</p>
<p>Beyond the cultural and social controversy surrounding the whole topic, I would like to highlight one thing: running an effective online dating app would be <em>terrible</em> business under most models, whether it&rsquo;s making money from ads, optional purchasable extra features, or subscriptions. Imagine if the average user signed up for the app, created their profile, started using it, and the app would be so effective that they&rsquo;d immediately get genuine dates with real, interesting partners. They&rsquo;d probably find themselves in a new relationship in no time and would have no need for the app anymore!</p>
<p>A non-user doesn&rsquo;t make money, therefore it is the financial interest of the business to keep you a user. On the other hand, if the app is completely ineffective, then again it would lose all but the most desperate potential users. because then it&rsquo;s just a waste of time, why would you pay for it? In order to keep the active user count as high as possible, the app cannot be too effective nor too ineffective.</p>
<p>The incentives of the users (who want to find sexual or romantic partners) and the business (who wants you to keep using it for as long as possible) are greatly misaligned, and that is not good. Any success of any kind of social or economic system only comes from successfully aligning the interests of as many groups as possible.</p>
<p>Perhaps there is a business model that could work for online dating that would not be so horribly misaligned, but if so, I haven&rsquo;t seen it yet.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/26/prql-its-like.html">PRQL: it&#39;s like SQL but actually makes sense</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/26/prql-its-like.html" class="u-url"><time class="dt-published" datetime="2023-07-26 02:13:22 &#43;0200">Jul 26, 2023</time></a>

		<div class="e-content">
			 <p>I have never had to write particularly complex SQL queries, but I&rsquo;ve written and seen enough to know that that SQL gets nasty and complicated once you start using the non-trivial features, not to mention things like optimizing queries based on the execution plan, or optimizing indexes, or the other million things that make Database Engineer an actual job title.</p>
<p>One thing in particular that has always bothered me (even with my very limited experience!) about SQL is how it tries to read like English, which makes it harder to both read and write in many cases, the simplest and canonical one being that you have to write <code>SELECT ... FROM table</code>. What table we are querying is the first most important information for any readers, and it&rsquo;s also the first thing a query writer must decide, yet SQL forces you to write it second. This gets especially annoying when the <code>SELECT</code> clause is very complex and consists of multiple lines. To illustrate, here is an example I blatantly stole from an <a href="https://learnsql.com/blog/25-advanced-sql-query-examples/">SQL tutorial website</a>:</p>
<pre><code>SELECT
  SUM (CASE
    WHEN dept_id IN (‘SALES’,’HUMAN RESOURCES’)
    THEN salary
    ELSE 0 END) AS total_salary_sales_and_hr,
  SUM (CASE
    WHEN dept_id IN (‘IT’,’SUPPORT’)
    THEN salary
    ELSE 0 END) AS total_salary_it_and_support
FROM employee
</code></pre>
<p>The table you are reading from is <em>critical</em> information in giving you the context of what you are looking at and allow you to begin understanding what this query is supposed to be doing, and it&rsquo;s hidden at the very end of the query.</p>
<p>So yeah, SQL is not the best, and this is where Pipelined Relational Query Language (PRQL, pronounced “Prequel”) comes in and promises to do better: <a href="https://prql-lang.org/">PRQL: a modern language for transforming data</a> (<a href="https://news.ycombinator.com/item?id=36866861">Hacker News thread</a>), promising &ldquo;a simple, powerful, pipelined SQL replacement&rdquo;. At a glance, it looks like everything I&rsquo;d want it to be!</p>
<p>I&rsquo;m really glad this exists, even though I personally don&rsquo;t have much use for it at the moment.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/25/its-turtles-all.html">It&#39;s turtles all the way down: from high-level programming to CPU microcode</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/25/its-turtles-all.html" class="u-url"><time class="dt-published" datetime="2023-07-25 03:06:15 &#43;0200">Jul 25, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">It&rsquo;s turtles all the way down</a>:</p>
<blockquote>
<p>The following anecdote is told of William James. [&hellip;] After a lecture on cosmology and the structure of the solar system, James was accosted by a little old lady.</p>
<p>&ldquo;Your theory that the sun is the centre of the solar system, and the earth is a ball which rotates around it has a very convincing ring to it, Mr. James, but it&rsquo;s wrong. I&rsquo;ve got a better theory,&rdquo; said the little old lady.</p>
<p>&ldquo;And what is that, madam?&rdquo; inquired James politely.</p>
<p>&ldquo;That we live on a crust of earth which is on the back of a giant turtle.&rdquo;</p>
<p>Not wishing to demolish this absurd little theory by bringing to bear the masses of scientific evidence he had at his command, James decided to gently dissuade his opponent by making her see some of the inadequacies of her position.</p>
<p>&ldquo;If your theory is correct, madam,&rdquo; he asked, &ldquo;what does this turtle stand on?&rdquo;</p>
<p>&ldquo;You&rsquo;re a very clever man, Mr. James, and that&rsquo;s a very good question,&rdquo; replied the little old lady, &ldquo;but I have an answer to it. And it&rsquo;s this: The first turtle stands on the back of a second, far larger, turtle, who stands directly under him.&rdquo;</p>
<p>&ldquo;But what does this second turtle stand on?&rdquo; persisted James patiently.</p>
<p>To this, the little old lady crowed triumphantly,</p>
<p>&ldquo;It&rsquo;s no use, Mr. James—it&rsquo;s turtles all the way down.&rdquo;</p>
<p>—  J. R. Ross, Constraints on Variables in Syntax, 1967</p>
</blockquote>
<p>As a teenager, what caught my interest in programming was the question: wait, but how does it work?
It started &ndash; as is probably very common &ndash; with a game: a browser-based online game called Galactica. One day I discovered the View Source option of the browser, and the rest, as they say, is history.</p>
<p>That was how I started out with web development, using PHP, HTML, and just enough JavaScript and CSS to think that I Know Stuff. Then I switched to harder stuff: C# was my next main language. Shortly after (you know, when the rush just isn&rsquo;t the same anymore, so you need heavier stuff) I tried picking up C++. Learning C++ only by doing is very hard though: it is a supremely beginner-unfriendly language, so I didn&rsquo;t get very far with it. It was only during my Computer Science Bachelor&rsquo;s studies that I received more formal education in C++, and it became my favorite language: it allows you to go as low-level or high-level as you want. You want to take a random integer, convert it to a memory address, and write to it? Sure thing, buddy! <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> You&rsquo;d like to have a hash map mapping strings to callback functions defined for example as lambda expressions? You&rsquo;ve got it! <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>I should mention that I also have a fondness for C, but it&rsquo;s too cumbersome to work with it for any nontrivial project. Who wants to have to re-implement even trivial mechanisms like lists (or vectors, as C++ calls them) or strings? There&rsquo;s nothing really appealing about C, in my opinion, compared to C++, if you have the choice; of course, many micro-controllers and more exotic architectures only have working C compilers and toolchains.</p>
<p>For most practical purposes, there&rsquo;s just no reason to go lower level than C or C++, but of course, you can if you&rsquo;re determined: you can write Assembly. (C and C++ both support inline assembly, but that doesn&rsquo;t work for everything.) Some things you can only do in Assembly, usually things that are the responsibilities of the operating system kernel: configuring the CPU is one of them, context-switching between threads is another (since you need to switch stacks by overwriting <code>%rsp</code> on x86-64). But yeah, if C is often an inconvenient language to work in, Assembly is just torture, even with modern assemblers having added many convenience features. Generally speaking, you&rsquo;d write just a file or two in Assembly, containing the functionality you need, and call it from C (or C++ via <code>extern &quot;C&quot;</code>).</p>
<p>There&rsquo;s yet more turtles though. Assembly compiles down into machine code, which really is just highly unpleasant to even read, especially after the compiler applied optimizations to it (something that the C and C++ compilers are notoriously good at). It&rsquo;s a common idea that the CPU then executes the machine code, but this has not been true for a very long time: even machine code is just a &ldquo;high-level&rdquo; language for the CPU, except one that the CPU knows how to compile itself into <em>microcode</em>. The primary reason for this is the usual one: seeking extra performance.</p>
<p>Indeed, the most common motivations for anybody choosing to work in a language lower-level than something than C# or Java is the need to more closely control what the hardware does, often in order to ensure that it is <em>fast</em>. Game development and high-frequency trading are two good examples. But even working in C++ and doing clever things with memory only gets you so far: compiler optimizations will invoke Cthulhu and sacrifice virgin goats to turn your code into an unspeakable horror-show in the name of the performance. This involves doing things you&rsquo;d never want to do by hand, such as unrolling a loop, or creating multiple versions of the same code specialized under different run-time conditions.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>Applying optimizations on code written in unrestrictive languages such as C or C++ is no easy task though. These are the languages where the programmer is allowed to manually manipulate pointers or define (raw, untagged) unions. For optimizations to take place, the compiler must be able to make assumptions about what your code does: in general, the better the compiler understands what you are trying to do, the better and faster code it will be able to generate for you to achieve it. For these assumptions to be guaranteed to hold, you need rules, and therein lies the fundamental tension between the &ldquo;you are the boss&rdquo; philosophy of the language and the need to make rules so that the compiler can optimize your code. (This is also, sort of, the reason why code written in e.g. C# or Java can sometimes outperform common C++ code: those languages are a lot more restrictive, allowing for less fishy business, and thereby allowing the compiler and their runtime environment to have a better understanding and therefore generate faster code.)</p>
<p>This same idea applies a lot more broadly than just programming languages and compilers: if you&rsquo;ve ever written a highly performance-sensitive code as a library, you&rsquo;ve felt the struggle. On the one hand, you want your library to be easy to use, without the user having to understand your implementation details, but you also want to make sure that the performance stays optimal, which almost inevitably leads to implementation details leaking out of your elegant API: you need the user code to tell you what it wants to achieve in details that the user code fundamentally shouldn&rsquo;t really have to particularly care about but is forced to due to performance considerations. (A simple example is the size of an internal buffer to use.)</p>
<p>But, to get back to our stack of turtles, this is also why even machine code needs to be compiled: machine code often gets new and more complex or more specialized instructions to execute, such as <a href="https://stackoverflow.com/a/43845829">instructions prefixed with <code>rep</code></a> that allow an operation to be repeated more efficiently than writing a loop. These complex instructions need to be broken down into more fundamental, low-level instructions that is practical to build hardware circuitry for, but which may be specific even to particular CPU models. The opposite also happens: sometimes multiple machine code instructions are actually compiled into a single microcode instruction, called <a href="https://stackoverflow.com/a/56413946">macro-fusion</a>.</p>
<p>Microcode by the way can receive updates, when CPU vendors need to fix or work around bugs in the hardware (I know right), or to fix or mitigate security vulnerabilities. There have been many examples for the latter in the last 5 years, <a href="https://lock.cmpxchg8b.com/zenbleed.html">Zenbleed</a> (<a href="https://news.ycombinator.com/item?id=36848680">Hacker News thread</a>) being the most recent one. Updates live in volatile memory (usually the RAM), and so it must be <a href="https://wiki.gentoo.org/wiki/Microcode">loaded on every boot</a>, either by BIOS/UEFI, or in some cases, by the OS kernel directly.</p>
<p>Do we have an bigger turtle that underlies this microcode? We definitely do have at least
<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> one more: they are called <a href="https://en.wikipedia.org/wiki/Logic_gate">logic gates</a>, the fundamental primitives that ultimately underlie all instruction execution. Logic gates usually operate on just individual bits: a 0 (logical false) or 1 (logical true). They are very simple beasts, and there are not that many of them. The simplest ones are: <code>not</code> (flips a bit from 0 to 1 or from 1 to 0), <code>and</code> (yields 1 if both input bits are 1, else 0), and <code>or</code> (yields 1 if either input bits are 1, else 0).</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Of course, caveats apply, mostly revolving around <em>undefined behaviour</em>. The compiler needs to have a good understanding of what your code is trying to do in order to be able to properly optimize it. If you violate its assumptions, bad things happen: if you&rsquo;re lucky, your application will just crash. If you&rsquo;re not lucky, you get impossible-to-track down, strange, once-in-a-while odd behaviour in a completely unrelated part of the program.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>The C++11, 17, and 20 standards have added lots of convenience features to the language and standard library alike. C++03 and earlier are just sadness: if you are forced to work with them, my condolences!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>One example of this is <em>loop unswitching</em> where a loop containing a branch whose condition is invariant in a loop is pulled out of the loop, turning <code>while (true) { if (flag) do_a(); else do_b(); }</code> into <code>if (flag) { while (true) do_a(); } else { while (true) do_b(); }</code> if it can be proven that the value of <code>flag</code> cannot be affected by either <code>do_a()</code> or <code>do_b()</code>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>In between microcode and logic gates there are definitely some more layers of turtles, which I don&rsquo;t have a good idea about: it&rsquo;s the realm of hardware implementation.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/22/us-lawmakers-trading.html">US lawmakers trading and stock ownership</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/22/us-lawmakers-trading.html" class="u-url"><time class="dt-published" datetime="2023-07-22 01:43:15 &#43;0200">Jul 22, 2023</time></a>

		<div class="e-content">
			 <p><a href="https://www.wsj.com/articles/senators-to-propose-ban-on-u-s-lawmakers-executive-branch-members-owning-stock-6db6411">Senators to propose ban on US lawmakers and executive branch members owning single stocks</a>, once again. The <a href="https://news.ycombinator.com/item?id=36785467">Hacker News thread</a> is also quite informative, <a href="https://news.ycombinator.com/item?id=36786367">for example</a>:</p>
<blockquote>
<p>I started building out tools to track congressional stock trading in 2020.</p>
<p>Since then, I believe there have been 9 other proposals similar to this one. None of them have been even called to a vote.</p>
<p>It seem like Congress is pretty unwilling to regulate its own trading.
Until they are, feel free to track the trading here: <a href="https://www.quiverquant.com/congresstrading/">www.quiverquant.com/congresst&hellip;</a></p>
</blockquote>
<p>Like <a href="https://news.ycombinator.com/item?id=36785705">many others</a>, I feel quite disillusioned and uninspired by this:</p>
<blockquote>
<p>All show and no go.</p>
<p>Like most such &ldquo;reforms&rdquo;, it is intended for naive public consumption while also being easily circumvented.
Setup a trust or corporation run by a family member and continue with business a usual.</p>
</blockquote>
<p>Senators being allowed to do what is for all intents and purposes insider trading, is frankly, completely insane. Pelosi is perhaps the most egregious example: <a href="https://nypost.com/2022/10/05/house-speaker-nancy-pelosi-has-accrued-millions-from-husbands-trades-report/">House Speaker Nancy Pelosi has accrued millions from husband’s trades: report </a>:</p>
<blockquote>
<p>The speaker, one of the richest members of Congress, has vehemently denied sharing any information with her spouse — a venture capitalist.</p>
<p>However, many have questioned trades made by Paul Pelosi that happened to coincide with major congressional decisions.</p>
<p>In June, he exercised call options to purchase up to $5 million in the graphics card manufacturer Nvidia just weeks before the House considered a bill to provide more than $50 billion in subsidies to domestic semiconductor manufacturers, the Beacon said.</p>
</blockquote>
<p>Yeah. She&rsquo;s not going to jail. So I&rsquo;d strongly support efforts to curtail this, but I just do not believe that you actually can effectively do so in practice. Ultimately, the forms of power are interchangable: if you&rsquo;re influential, for example because you&rsquo;re in Congress, then you&rsquo;ll always have easy opportunities to get (more) rich off of it, and also vica versa.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/22/financial-news-and.html">Financial news and moving the markets</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/22/financial-news-and.html" class="u-url"><time class="dt-published" datetime="2023-07-22 01:25:19 &#43;0200">Jul 22, 2023</time></a>

		<div class="e-content">
			 <p>Daring Fireball wrote about the <a href="https://daringfireball.net/2023/07/apple_gpt_bloomberg">Apple GPT news</a>. I think the news itself is pretty uninteresting: Apple wants in on the ChatGPT gold-rush even as it seems to be winding down. The more interesting part is what he writes about Bloomberg:</p>
<blockquote>
<p>Bloomberg reporters are evaluated and receive bonuses tied to reporting market-moving news. They’re incentivized financially to make mountains out of molehills, and craters out of divots, to maximize the immediate effect of their reporting on stock prices. And Bloomberg appends these stock price movements right there in their reports, to drive home the notion that Bloomberg publishes market-moving news, so maybe you too should spend over $2,000 per month on a Bloomberg Terminal so that you can receive news reports from Bloomberg minutes before the general public, and buy, sell, and short stocks based on that news.</p>
<p>&hellip;</p>
<p>Apple’s brief 2.7 percent jump and Microsoft’s smaller but still-significant drop, both at 12:04pm, were clearly caused by Gurman’s report. Bloomberg Terminal subscribers get such reports before anyone else. [&hellip;] most of their original reporting is delivered with the goal of moving the stock prices of the companies they’re reporting on, for the purpose of proving the value of a Bloomberg Terminal’s hefty subscription fee to day-trading gamblers [&hellip;]</p>
</blockquote>
<p>Tinfoil hat time? I mean&hellip; that&rsquo;s quite the stretch. Even if Bloomberg doesn&rsquo;t report particular news, a million other websites will; how does Bloomberg move the market in particular?
Bloomberg is hardly the only company selling ahead-of-the-crowd services; it&rsquo;s obviously a lucrative thing to do successfully, given the herd mentality that all markets follow<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
They obviously want to communicate news that have <em>the potential</em> of moving stocks, otherwise they&rsquo;d not be a good source of finance news!</p>
<p>As for making mountains out of molehills, I think that&rsquo;s a fair criticism to an extent, but it&rsquo;s also not one that is in any way unique to Bloomberg. Clickbait and blowing news out of proportion is just business as usual for mass media these days, thanks to the advertising industry financing a large chunk of websites and competition <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">fierce enough to trigger</a> a race to the bottom with low-effort reporting. My personal impression is that Bloomberg is actually better than many in this regard.</p>
<p>By the way, the Apple stock <em>stayed up</em> after these news (see the <a href="https://www.investing.com/equities/apple-computer-inc">Investing.com chart</a>), only dropping back to previous levels some 24 hours later when manufacturing issues were reported with the upcoming iPhone 15. So it was <em>good reporting</em> by Bloomberg.</p>
<!-- raw HTML omitted -->
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Predicting how a market will move though is pretty much hopeless in the general case, as financial markets are a good example of a chaotic system in that it responds not just to events, but predictions and expectations as well, and does so in a recursive, self-predicting way: if enough people think that upon some news others will want to buy a particular stock, then the stock will move on the news. This is one of the factors that makes markets very &ldquo;noisy&rdquo;: prices move very frequently, even for large, fairly stable stocks.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/15/outofmemory-while-trying.html">Out-of-memory while trying to free it: into virtual memory</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/15/outofmemory-while-trying.html" class="u-url"><time class="dt-published" datetime="2023-07-15 03:10:14 &#43;0200">Jul 15, 2023</time></a>

		<div class="e-content">
			 <p>I recently came by this fun story on getting an out-of-memory error on Linux when trying to <em>free</em> memory: <a href="https://ayende.com/blog/199649-B/production-postmortem-enomem-when-trying-to-free-memory?Key=c9261116-c324-4280-a1e1-35da0d1fa882">Production postmortem: ENOMEM when trying to free memory</a></p>
<blockquote>
<p>That error made absolutely no sense, as you can imagine. We are trying to release memory, not allocate it. Common sense says that you can’t really fail when you are freeing memory. After all, how can you run out of memory? I’m trying to give you some, damn it!</p>
</blockquote>
<p>The linked blog post does a good job of explaining the problem briefly and clearly, so I&rsquo;m just going to provide some further context on how this can indeed be a problem.</p>
<p>Modern computer architectures like x86-64 and even most (all?) the ARMs have a lot funkier relationship with memory than most people &ndash; and even most software developers! &ndash; tend to be aware of. (This is a good thing! You don&rsquo;t need to know how to sausage is made. Abstraction is an invaluable tool for managing complexity.)</p>
<p>The memory that a running application (any running application) sees and lives in (where it loads all of its data and where all of its allocations live) is actually not a direct representation of the main memory of the computer (the RAM sticks). It used to be, long ago, but we&rsquo;ve moved away from that. Instead, what your process is interacting with is called <em>virtual memory</em>, or more specifically, a <em>virtual memory address-space</em>. Virtual memory is a layer of abstraction on top of <em>physical memory</em>, a decoupling useful because it grants additional performance, stability, and safety. How?</p>
<p>A process&rsquo;s virtual memory address-space is constructed specifically for that one particular process, and reflects <em>only</em> the parts of the physical memory that are used by it. Memory used by other processes is normally not visible here: it is simply not mapped (outside of special cases like shared memory created by <a href="https://man7.org/linux/man-pages/man3/shm_open.3.html"><code>shm_open()</code></a> and friends or debugging via <a href="https://man7.org/linux/man-pages/man2/ptrace.2.html"><code>ptrace()</code></a>). This means that, by default, no process can accidentally or maliciously access the memory of other processes and extract secrets from it or change the values in unexpected ways leading to erratic behaviour or crashes.</p>
<p>This layer of abstraction also enables other useful features that reduce overall memory usage at the expense of runtime performance:</p>
<ul>
<li>Demand-paging: memory allocated but not yet accessed doesn&rsquo;t actually need to be available: if the application decided to allocate a gigantic 2 GB buffer but hasn&rsquo;t yet used more than 100 KB of it, then the rest doesn&rsquo;t have to be available</li>
<li>Swapping: rarely used regions of memory are written to disk, and the memory it was used for is made available for new allocations</li>
<li>Compression: rarely used regions of memory are compressed, and the space freed up is reclaimed for other purposes</li>
<li>De-duplication: if there are multiple identical copies of the sequence of bytes in memory, we don&rsquo;t actually need to store it multiple times (though caveats apply!)</li>
</ul>
<p>For any physical memory to be accessible in a virtual memory address space, it has to be mapped in, typically by the operating system kernel. This is done by writing into a data structure called the <em>page table</em>, which is shared between the kernel and the CPU&rsquo;s memory management unit (MMU). A <em>page table entry</em> typically represents the mapping of some physical memory region into a virtual memory address space: in this case, the entry will contain the physical memory address that it corresponds to. You may have multiple page table entries refer to the same or overlapping regions of physical memory: that is totally cool and allowed! (This has some very interesting uses e.g. in cybersecurity that I wrote <a href="https://github.com/shdnx/dangless-malloc">my Master&rsquo;s thesis on</a>.)</p>
<p>A page table entry, beyond the physical memory address, also contains bits used for purposes. Some of these are used for protection, to indicate whether the region is readable, writable, or executable. This is enforced by the MMU: attempting to write to a virtual memory region whose page table entry does not have the writable bit set will fail (well, it will trigger a page fault, which can be handled &ndash; more on this later). For example, after a running application&rsquo;s executable is loaded into memory at startup, the region is marked as non-writable (but readable and executable) to prevent buggy software from accidentally modifying its own code and also to limit the options of malware. Similarly, memory regions where the application&rsquo;s runtime data is stored (so the stack, the heap, but also the .data, .bss, and .rodata sections containing global variables) are marked as non-executable, as that is a very easy attack vector for malicious users.  (They can be made executable, otherwise things like Jit-in-Time compilers would not work, but you have to explicitly request it. e.g. on Linux by using <code>mmap()</code> with <code>PROT_EXEC</code>.)</p>
<p>The page table entry also contains a &ldquo;dirty&rdquo; bit. This is set the MMU whenever that particular memory region is used. The kernel can periodically clear this bit, and then check later if the bit is set: if not, then the corresponding memory region has not been used recently, and so may be candidate to be swapped out or compressed.</p>
<p>We need a mechanism though for detecting when e.g. a swapped-out piece of memory is accessed. Clearly, such an access cannot simply proceed as normal: the page table entry is marked as invalid, there is no physical memory backing it. The MMU will trigger a <em>page fault</em> in this case, which is an interrupt that the kernel handles. The kernel will consult its own internal book-keeping of the process&rsquo;s memory and will find in this case that it has decided to swap the memory to disk: now, since the data is requested, the data must be loaded back from disk into main memory and the page table entry must be made valid again. Once this is done, the process resumes execution from the exact same place as it was left suspended when trying to access swapped memory, none the wiser to what happened in the background &ndash; except perhaps that some operation ending up having taken much more time than usual.</p>
<p>This pattern is actually very common, and is used for all of the memory-use optimizations I&rsquo;ve mentioned so far. For instance, in the case of memory de-duplication, identical memory regions may be referenced by multiple processes at the same time. So long as they all only read from it, life is good, but  when one of them wants to write to it, the kernel needs to intervene, as the write would make the memory region no longer contain what the other processes expect it to. This is achieved by marking the page table entries as non-writeable, such that attempting to write results in a page fault. On such page  fault, the kernel quietly duplicates the memory region (by allocating a new one and then copying into it) and updates the writing process&rsquo;s page table entry to point to the new address, restoring its writeability before returning control to the process.</p>
<p>You may have noticed that I was talking about individual bits of the page table entry. Indeed, these entries are extremely limited in size: typically just 64 bits (8 bytes), most of which is needed to store the referenced physical memory address. This is not always enough for more complicated features built on top of the virtual memory abstraction, so the kernel has additional book-keeping: in Linux, these are called <em>virtual memory areas</em> or VMAs for short: one for each distinct memory region of each running process. For reasons of simplicity and performance, these VMAs are stored in a large pre-allocated array with a fixed capacity.</p>
<p>This takes us back to the blog post that motivated this write-up: freeing or de-allocating memory may require an additional VMA to be allocated in the case where you&rsquo;re not freeing an entire allocated memory region, but only a part of it. This is not possible to do via the commonly used <code>malloc()</code> / <code>free()</code> APIs or even the C++ <code>new</code> and <code>delete</code> equivalent operators, but it is possible to do via direct system calls using <code>mmap()</code> and <code>munmap()</code>. To illustrate:</p>
<ol>
<li>We allocate 1000 bytes of memory</li>
<li>Linux creates a new VMA for us to record this (note: no actual memory has been allocated unless you used <code>MAP_POPULATE</code> as demand paging kicks in!)</li>
<li>We now wish to free the middle 500 bytes</li>
<li>Linux now has no choice but to split the original one VMA spanning 1000 bytes into two of 250 bytes each, with a 500 byte gap in the middle</li>
</ol>
<p>Granted, this is a bit contrived, and not even accurate as you cannot free 500 bytes: memory is split into pages of 4096 bytes (4 KB) each (by default, on x86-64). But you can imagine the same situation happening when e.g. different protection bits are applied to different parts of a huge memory region: some part of writeable, other parts are not. The VMA having to be split causes the memory deallocation operation to actually have to allocate memory: for the new VMA. This is how you can get an out-of-memory error.</p>
<p>As you can imagine, there are a lot more details to this topic. Some teasers:</p>
<ul>
<li>A single virtual memory address space can span up to 256 terrabytes of memory, making virtual memory much more plentiful than physical memory.</li>
<li>Resolving virtual memory addresses into physical memory addresses is slow even when performed by dedicated hardware: modern CPUs spend something like 25% of their total execution time just doing this.</li>
<li>Caching (the TLB) is used to mitigate this, and while its hit-rate is very high (96%+ in typical applications), it opens up timing-related side-channel attacks that have become well-known in recent years.</li>
<li>Huge pages of 2 MB and 1 GB are another effort to improve performance, but are a mess, and difficult to work with in practice.</li>
</ul>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/15/brought-down-by.html">Brought down by the font</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/15/brought-down-by.html" class="u-url"><time class="dt-published" datetime="2023-07-15 01:11:22 &#43;0200">Jul 15, 2023</time></a>

		<div class="e-content">
			 <p>Oh wow, now <em>this</em> is something: <a href="https://www.theverge.com/2017/7/12/15961354/pakistan-calibri-font-scandal-forged-documents">A Microsoft font may have exposed corruption in Pakistan</a>:</p>
<blockquote>
<p>The Microsoft font Calibri is now a key piece of evidence in a corruption investigation surrounding Pakistan’s prime minister. Investigators noticed that documents handed over by the prime minister’s daughter, Maryam Nawaz Sharif, were typed up in the font Calibri. But the documents were dated from 2006 — and Calibri wasn’t widely available at that point, making a good case that they were forged.</p>
</blockquote>
<p>I love everything about this, but mostly just the fact that there are people know and care enough about fonts of all things to be able to point things like this out. This is true digital forensics. I almost feel sorry for Pakistani Prime Minister Nawaz Sharif! Not enough though to keep me from highlight this amazing snippet of peak journalism from Arstechnica&rsquo;s slightly more detailed article <a href="https://arstechnica.com/tech-policy/2017/07/not-for-the-first-time-microsofts-fonts-have-caught-out-forgers/">Not for the first time, Microsoft’s fonts have caught out forgers</a>:</p>
<blockquote>
<p>Ultimately, the fallout of the corruption and cover-up is that Pakistan may soon, like Calibri itself, be sans-Sharif.</p>
</blockquote>
<p>The article goes on to note that this hasn&rsquo;t been the first time that fonts have been the demise of the mighty:</p>
<blockquote>
<p>Other Word features have caught out forgers, too. The &ldquo;Killian documents,&rdquo; which claimed President George W. Bush was declared unfit for service during his time with the Air National Guard, purported to have been produced on a typewriter in 1973. However, those documents used proportional fonts and curly quotes, making it spectacularly unlikely that they were authentic. Standard 1973 vintage typewriters didn&rsquo;t offer either proportional fonts or curly quotes, but 2004-vintage Microsoft Word did both.</p>
</blockquote>
<p>But yeah, apparently there are indeed people that both know and care a lot about fonts, such as <a href="https://www.youtube.com/watch?v=cJYm-de_UHE">this guy who researched the market share of fonts used for memes</a>. Peak Friday evening material.</p>
<p>This whole rabbit hole was enabled by the <a href="https://news.ycombinator.com/item?id=36719987">Hacker News discussion</a> on Microsoft&rsquo;s new Aptos font, aimed to replace Calibre as the default, which I only even clicked on because I thought they were referring to <a href="https://calibre-ebook.com/">Calibre the ebook management software</a> that I use.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/12/writing-a-popular.html">Writing a popular blog must be dreadful</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/12/writing-a-popular.html" class="u-url"><time class="dt-published" datetime="2023-07-12 01:38:49 &#43;0200">Jul 12, 2023</time></a>

		<div class="e-content">
			 <p>In <a href="https://astralcodexten.substack.com/p/why-do-i-suck">Why do I suck</a>, Scott Alexander is analyzing why his earlier articles (from 2013 - 2016) were much more popular than his recent ones.</p>
<p>I would like to highlight his insight into the perception difference between a small blog and a big one:</p>
<blockquote>
<p>If you have a small blog, and you have a cool thought or insight, you can post your cool thought or insight. People will say “interesting, I never thought of that before” and have vaguely positive feelings about you. If you have a big blog, people will get angry. They’ll feel it’s insulting for you to have opinions about a field when there are hundreds of experts who have written thousands of books about the field which you haven’t read. Unless you cite a dozen sources, it will be “armchair speculation” and you’ll be “speaking over real academics”. If anyone has ever had the same thought before, you’re plagiarizing them, or “reinventing the wheel”, or acting like a “guru”, or claiming that all knowledge springs Athena-like from your head with no prior influences.</p>
<p>I try really hard to block or ignore these people when I spot them, but they do a little bit of psychic damage each time.</p>
</blockquote>
<p>I wonder how much of that is simply bigger audience = worse audience, given <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">regression to the mean</a>. Basically: when your blog has 100 readers, it is not difficult for this 100 to be &ldquo;reasonable people&rdquo; (from you, the blog author&rsquo;s, perspective), but if you have 10 000, then it is much harder: as the size of a group grows, the more it tends to resemble the population at large. The population as a whole contains a whole lot of people who suck! (This seems universally true, regardless of what group you identify with.)</p>
<p>Another perspective on the same idea: out of 10 000 people, you&rsquo;re much more likely to annoy / step on the toes of somebody by something you write than out of 100 people. 10 000 people will also likely contain some trolls and griefers, who just tend to make things worse.</p>
<p>The other possible explanation is authority. A blog with few readers has little to no authority on any subject, and therefore is largely safe from the kind of scrutiny that having authority (even informal!) invites. So you can get away just expressing your opinions in a way that makes sense to you. Doing so when you have a large audience is risky, expectations are higher, and the whole thing becomes a lot more high-stakes.</p>
<p>Taking this further, what does this mean for official authorities like the CDC or the FED? Obviously that&rsquo;s kind of as high-stakes as it gets, but beyond that, they are very constrained in what they can actually say if they want people to actually take them seriously. They can&rsquo;t just say things that make sense to them! It needs to be as unambiguous as possible, supported by research and data.</p>
<hr>
<p>As an aside, I find it interesting that one of his plausible explanations is just that media overall is less bad today than it used to be. Not often do you see this proposed!</p>
<blockquote>
<p>Nowadays I think there are many good science bloggers, and the media has gotten embarrassed enough times that it will sometimes run a take by someone who knows what they’re talking about before publishing it.</p>
<p>In the same way, I see fewer people outright denying the existence of genetics, totally failing to understand AI risk, or utterly bungling basic concepts in risk and probability.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/08/regulation-unexpected-consequences.html">Regulation, unexpected consequences of (Sesame edition)</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/08/regulation-unexpected-consequences.html" class="u-url"><time class="dt-published" datetime="2023-07-08 23:43:54 &#43;0200">Jul 8, 2023</time></a>

		<div class="e-content">
			 <p>Matt Levine writes the amazing <a href="https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine">Money Stuff</a> newsletter for Bloomberg. I&rsquo;ve been a reader of his for a few years now, and I&rsquo;m a huge fan of his writing. He recently wrote about the <a href="https://www.bloomberg.com/opinion/articles/2023-06-21/big-firms-want-normal-crypto-markets">new US legislation on sesame seeds</a> (search for the title &ldquo;Sesame&rdquo;) that I found hilarious:</p>
<blockquote>
<p>Congress passed legislation intended to make life better for people allergic to sesame seeds. Instead, it made things worse.
The bill, passed with overwhelming bipartisan support and signed into law by President Biden in 2021, requires manufacturers to label sesame on their products starting this year.</p>
<p>In response, some companies began adding sesame to products that hadn’t included it in the past—saying it was safer to add sesame and label it, rather than certify they had eliminated all traces of it.</p>
</blockquote>
<p>Why would they do that? Clearly, if anything, that&rsquo;s the opposite of what the new regulation was intended for. Well:</p>
<blockquote>
<p>The issue is that it is hard to eliminate trace amounts of sesame, and the law now requires food manufacturers to label sesame as an allergen. Not putting sesame on the label effectively constitutes a promise that there is no sesame in the product, and if there is a little bit then you get in trouble: [&hellip;]</p>
<p>But if you say that the product definitely contains sesame, then you are immunized from trouble. So you just chuck some sesame into everything, change the labels, and you’re fine. It is easier to make sure that there is sesame than that there isn’t, so that’s what companies do.</p>
</blockquote>
<p>Unintended consequences! It <em>makes sense</em> that companies do this, given their incentives. A friend of mine put it as &ldquo;regulators just do not think like hackers&rdquo;, which is not how I&rsquo;d put it, but I do see his point: regulators should have known better than to do this. Don&rsquo;t they consult with experts and people in the industry? Someone would have definitely pointed this out to them.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/06/the-twitterkiller-metas.html">The Twitter-killer? Meta&#39;s new Threads app</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/06/the-twitterkiller-metas.html" class="u-url"><time class="dt-published" datetime="2023-07-06 00:29:43 &#43;0200">Jul 6, 2023</time></a>

		<div class="e-content">
			 <p>Facebook (err, sorry, Meta) is launching their Twitter competitor today in the US in the form of a new stand-alone app called <a href="https://apps.apple.com/us/app/threads-an-instagram-app/id6446901002">Threads</a>.</p>
<p>The Verge has a good summary on what is known: <a href="https://www.theverge.com/2023/7/5/23784480/threads-instagram-meta-news-twitter-competitor">Instagram’s Threads: all the updates on the new Twitter competitor</a>. I can also recommend the <a href="https://news.ycombinator.com/item?id=36580192">Hacker News thread</a>; it has received quite a bit of attention and drew a lot of discussion.</p>
<blockquote>
<p>As Twitter continues to flail about under Elon Musk, all eyes are on Instagram Threads as a potential replacement.</p>
</blockquote>
<p>Twitter&rsquo;s story with getting bought by Elon Musk is truly wild, and semi-seriously could be turned into a movie. Matt Levine wrote a lot about it in his excellent Money Stuff, and it was all great fun, truly peak 21st century amusement. Since the acquisition, Twitter has arguably received more attention than before, but also a lot more criticism, and is <a href="https://www.dexerto.com/tech/twitter-users-claim-elon-musks-recent-changes-may-be-self-ddosing-the-platform-2196413/">clearly struggling</a>, <a href="https://daringfireball.net/linked/2023/07/03/everything-continues-to-be-going-just-great-at-twitter">dear lord</a>, with even <a href="https://mashable.com/article/twitter-api-elon-musk-developer-issues-apps">high-paying customers being ignored</a>.</p>
<p>Elon Musk himself has stated multiple times that bankruptcy in the near future was possible. It remains to be seen whether these are the kind of temporary issues that can be expected from a fairly dramatic shift in leadership and strategy, or perhaps the beginning of the end of Twitter&rsquo;s prevalence.
Either way, this is excellent timing from Meta, and I expect they have a good chance of capitalizing on Twitter&rsquo;s plight.</p>
<blockquote>
<p>From what we know so far, Threads is “Instagram’s text-based conversation app” where “communities come together to discuss everything from the topics you care about today to what’ll be trending tomorrow.” The app is closely tied to Instagram, meaning you’ll get to use the same username across both apps, as well as quickly follow all of the accounts you’ve been following on Instagram.</p>
</blockquote>
<p>Sounds a lot like Twitter, i.e. a more social version of micro-blogging. Reportedly it will even have two-way <a href="https://joinmastodon.org/">Mastodon</a> integration, which is interesting and again a good move: Mastodon gained some popularity following Twitter&rsquo;s, uh, adventures, and by embracing it, Threads can attempt to woo Twitter refugees more effectively than by <em>not</em> integrating&hellip; not to mention, it&rsquo;s a greater blow to Twitter.</p>
<blockquote>
<p>The Twitter alternative from Meta appears set to launch on July 6th, but the Irish Independent reports that Ireland’s Data Protection Commission has been in contact with the company about the new product and confirmed the launch won’t extend to the EU “at this point.”</p>
</blockquote>
<p>GDPR strikes again! I will be curious to see what happens here; no doubt there&rsquo;s an element of power play here on the part of Meta. GDPR and other privacy legislation in the EU are clearly anti-Meta; Meta is probably evaluating whether it makes sense to even bother with it.</p>
<hr>
<p>An interesting perspective to take is that Threads appears to be very much like Facebook&rsquo;s own Wall, where people can write posts, upload photos, and engage with people in the comments. Facebook&rsquo;s own shift towards providing something more like a news feed has reduced the Wall&rsquo;s relevance a lot, but the Threads can potentially be its reincarnation in a purer form.</p>
<p>Threads is cleverly branded under Instagram rather than Facebook or Meta to leverage its better reputation. Meta knows that as far as associations go, people (at least in much of the US and Europe) tend to like and trust Instagram, but tend to at least distrust Facebook or Meta. Many, many people still aren&rsquo;t really aware that Instagram, and even WhatsApp are owned and operated by Meta now! They&rsquo;d very much like to keep it this way.</p>
<p>Branding- and image-wise, Meta is in an interesting situation. On the one hand, both in the United States and much of Europe, Facebook is generally a disliked and mistrusted brand, following lots and lots of bad publicity from, well, being Facebook and doing Facebook things. On the other hand, they still have a metric ton of daily active users, passing <a href="https://www.statista.com/statistics/346167/facebook-global-dau/">2 billion recently</a>, so clearly they are, uh, successful. Sadly.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/04/quantum-immortality-explored.html">Quantum immortality</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/04/quantum-immortality-explored.html" class="u-url"><time class="dt-published" datetime="2023-07-04 19:49:34 &#43;0200">Jul 4, 2023</time></a>

		<div class="e-content">
			 <p>Quantum immortality is a thought experiment that runs roughly like follows:</p>
<ol>
<li>If the many-worlds interpretation of quantum mechanics is correct, each &ldquo;choice&rdquo; (however this may be defined) creates a branching point: there will be a universe that explores each possibility. Simply put, if at an intersection you can choose to go left or right, in one universe you&rsquo;ll always go left, and in another you&rsquo;ll always go right, and the history of each universe will then evolve independently, and continue branching along each subsequent choice.</li>
<li>Which one of these many many universes will <em>you</em> (the awareness reading this text) experience? You are clearly in one of them: your awareness does not span universes. (At least, I would expect so. My apologies if it does! Please drop me an e-mail, I&rsquo;ll buy you a coffee.)</li>
<li>A person, pretty much by definition, is incapable of experiencing their own death. So you are clearly experiencing a universe in which you are not dead. (Again, my honest apologies if you are! Please drop me an e-mail, let&rsquo;s chat.)</li>
<li>What if this rule is universal? What if your awareness will always continue along a path of these possible universes where you remain alive (or perhaps just aware), however unlikely that may be?</li>
</ol>
<p><strong>Disclaimer</strong>: <em>This is purely a thought experiment. Do not take this seriously. As far as anyone can tell, you only have one life: do not throw it away.</em></p>
<p>This idea has some interesting consequences! The following is a short story by Robert Charles Wilson, originally published in 1998 (!), that explores this:</p>
<h3 id="divided-by-infinityhttpswwwtorcom20100805divided-by-infinity"><a href="https://www.tor.com/2010/08/05/divided-by-infinity/">Divided by Infinity</a></h3>
<hr>
<p>As I said, interesting idea! It would imply that you get away with any crazy shit! For starters:</p>
<ol>
<li>Buy a lottery ticket.</li>
<li>Set up some mechanism that will kill you immediately and without warning, should it turn out that you have <strong>not</strong> won the lottery.</li>
<li>Profit??</li>
</ol>
<p>Obviously this is not fool-proof, and, err, certainly not lifestyle advice! This would just be testing the likelihood of whatever mechanism you have set up failing against the likelihood of you winning the lottery. Examples of how this may come to be include but are not limited to:</p>
<ul>
<li>If you hire somebody to kill you, they will have an accident or something else that prevents them from completing their task.</li>
<li>If you set up some kind of machine or contraption, it will fail. The more fail-safe you make it, it will start failing in increasingly less plausible ways.</li>
<li>The lottery gets cancelled.</li>
<li>You win the lottery, but after the fact, your ticket turns out to be invalid or counterfeit.</li>
</ul>
<p>Another interesting consequence of this thought experiment is that it only confers &ldquo;immortality&rdquo; to <em>your own subjective awareness</em> or viewpoint. It does not protect others! So if you watch somebody else try e.g. the above, the result will most likely be the predictable one: they die. In <em>your</em> perspective that is, or rather, in the universe that you are experiencing. In <em>their</em> subjective perspective, they will survive; it will just not be a universe that you can ever experience or interact with.</p>
<p>Finally, we have been talking about <em>immortality</em>, but the premise does not actually promise that. It only promises a <strong>continuity of awareness</strong>. This does not protect against you suffering a horrible accident that will leave you alive and aware, but miserable! The possibilities start with the run-of-the-mill horribleness that comes from inhabiting a physical body (e.g. going blind or becoming totally paralyzed), but actually go much further: you could find yourself at the mercy of aliens experimenting on you, or your consciousness (and awareness) being uploaded into a computer and then being subjected to&hellip; whatever.</p>
<p>I will leave this here before it gets any darker. :)</p>
<p>To end this post on a lighter note, here is an only tangentially related but fun short story video about a guy using a One-minute Time Machine to try to pick up a woman. Enjoy:</p>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/CXhnPLMIET0' frameborder='0' allowfullscreen></iframe></div>
<p>(Thank you Melvin for the recommendation!)</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/07/01/light-and-fun.html">Light and fun fiction: Idol Words</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/07/01/light-and-fun.html" class="u-url"><time class="dt-published" datetime="2023-07-01 18:18:28 &#43;0200">Jul 1, 2023</time></a>

		<div class="e-content">
			 <p>At this point my blog has been alive for a few days now, and so it&rsquo;s high time I mentioned Scott Alexander, and his blog(s): the Star Slate Codex and Astral Codex Ten. He writes about complex topics in a refreshingly sober, straight-forward and illuminating manner. His articles will most definitely make a regular appearance here.</p>
<p>To start with, I would recommend a light and short piece of fiction he wrote: Idol Words. It&rsquo;s more a series of short scenes rather than a short story.
It semi-seriously explores the idea of three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly, with each &ldquo;scene&rdquo; featuring another person coming along to ask them a question. What kind of people would ask what kind of questions? The themes are perhaps predictable, but no less interesting or satisfying to read.</p>
<p>Enjoy:</p>
<h1 id="idol-wordshttpsastralcodextensubstackcompidol-words"><a href="https://astralcodexten.substack.com/p/idol-words">Idol Words</a></h1>
<blockquote>
<p>It was another boring day as the keeper of the three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly.</p>
</blockquote>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/30/im-very-excited.html">The Three Body Problem</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/30/im-very-excited.html" class="u-url"><time class="dt-published" datetime="2023-06-30 00:44:10 &#43;0200">Jun 30, 2023</time></a>

		<div class="e-content">
			 <p>I&rsquo;m very excited for the upcoming The Three Body Problem series!
I&rsquo;ve read and thoroughly enjoyed <a href="https://www.goodreads.com/book/show/20518872-the-three-body-problem">the book trilogy</a>, and it looks like they are sticking decently close to the source material. This must be a challenging story to put on screen!</p>
<p>The book&rsquo;s cover text:</p>
<blockquote>
<p>Set against the backdrop of China&rsquo;s Cultural Revolution, a secret military project sends signals into space to establish contact with aliens. An alien civilization on the brink of destruction captures the signal and plans to invade Earth. Meanwhile, on Earth, different camps start forming, planning to either welcome the superior beings and help them take over a world seen as corrupt, or to fight against the invasion.</p>
</blockquote>
<p>You can watch the teaser trailer for the series on YouTube:
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/5lj99Uz1d50' frameborder='0' allowfullscreen></iframe></div></p>
<p>The books form a trilogy, so the series will have plenty of material to work with!
My favorite book was the second, titled <a href="https://www.goodreads.com/book/show/23168817-the-dark-forest">The Dark Forest</a>, with a host of exciting new ideas, including a to me previously unknown theoretical explanation for the <a href="https://en.wikipedia.org/wiki/Fermi_paradox">Fermi Paradox</a>.</p>
<p>As an aside, this short video gives a brief explanation on the title-giving N-body Problem: that is, we can model the behaviour of a gravitational system with 2 objects, but we are incapable of doing so for 3 or more objects. That&rsquo;s kind of a problem, especially if you happen to live in a solar system with, let&rsquo;s say, 3 stars&hellip; :)
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/D89ngRr4uZg' frameborder='0' allowfullscreen></iframe></div></p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/30/inside-the-ai.html">Inside the AI factory: the human element of AIs</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/30/inside-the-ai.html" class="u-url"><time class="dt-published" datetime="2023-06-30 00:37:48 &#43;0200">Jun 30, 2023</time></a>

		<div class="e-content">
			 <p>A fascinating deep-dive from New York Magazine and The Verge into how AI models &ndash; including ChatGPT, Bard, and other LLMs (Large Language Models) &ndash; are trained, and the industry that provides the data, control, and feedback:</p>
<h2 id="inside-the-ai-factoryhttpsnymagcomintelligencerarticleai-artificial-intelligence-humans-technology-business-factoryhtml"><a href="https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html">Inside the AI factory</a></h2>
<blockquote>
<p>[&hellip;] ChatGPT seems so human because it was trained by an AI that was mimicking humans who were rating an AI that was mimicking humans who were pretending to be a better version of an AI that was trained on human writing.</p>
<p>This circuitous technique is called “reinforcement learning from human feedback,” or RLHF, and it’s so effective that it’s worth pausing to fully register what it doesn’t do. When annotators teach a model to be accurate, for example, the model isn’t learning to check answers against logic or external sources or about what accuracy as a concept even is. The model is still a text-prediction machine mimicking patterns in human writing, but now its training corpus has been supplemented with bespoke examples, and the model has been weighted to favor them. Maybe this results in the model extracting patterns from the part of its linguistic map labeled as accurate and producing text that happens to align with the truth, but it can also result in it mimicking the confident style and expert jargon of the accurate text while writing things that are totally wrong. There is no guarantee that the text the labelers marked as accurate is in fact accurate, and when it is, there is no guarantee that the model learns the right patterns from it.</p>
</blockquote>
<p>Makes sense, and it&rsquo;s so very important to recognize this! We, humans, like to anthropomorphize things, and doubly so for LLMs since they can come across so&hellip; genuinely human.</p>
<p>Generating training data for AIs though appears&hellip; distinctly not-so-fun, mainly because it is so alien. Indeed, you are training an <em>excessively dumb</em> machine:</p>
<blockquote>
<p>It is in part a product of the way machine-learning systems learn. Where a human would get the concept of “shirt” with a few examples, machine-learning programs need thousands, and they need to be categorized with perfect consistency yet varied enough (polo shirts, shirts being worn outdoors, shirts hanging on a rack) that the very literal system can handle the diversity of the real world.
[&hellip;]
Instruction writers must come up with rules that will get humans to categorize the world with perfect consistency. To do so, they often create categories no human would use. A human asked to tag all the shirts in a photo probably wouldn’t tag the reflection of a shirt in a mirror because they would know it is a reflection and not real. But to the AI, which has no understanding of the world, it’s all just pixels and the two are perfectly identical. Fed a dataset with some shirts labeled and other (reflected) shirts unlabeled, the model won’t work. So the engineer goes back to the vendor with an update: DO label reflections of shirts. Soon, you have a 43-page guide descending into red all-caps.</p>
</blockquote>
<p>I fear though that before this era is over, most of us will have a &ldquo;job&rdquo; like this one, if indeed any at all.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://blog.gaborkozar.me/2023/06/29/tech-erosion-the.html">Tech Erosion: the effect of technology (and AI) on our lives</a></h1>
		

		<a href="https://blog.gaborkozar.me/2023/06/29/tech-erosion-the.html" class="u-url"><time class="dt-published" datetime="2023-06-29 23:02:19 &#43;0200">Jun 29, 2023</time></a>

		<div class="e-content">
			 <p>StillDrinking has a lot of great essays. He has a way with words that makes reading his articles both entertaining and illuminating at the same time &ndash; a powerful combination.</p>
<p>Anyway, their essay on AI and ChatGPT, and how the world we have built will continue manages to be human-hostile in many ways in spite of (and indeed, in part <strong>because of</strong>) technology:</p>
<h2 id="tech-erosionhttpswwwstilldrinkingorgtech-erosion"><a href="https://www.stilldrinking.org/tech-erosion">Tech Erosion</a></h2>
<blockquote>
<p>People are already losing their jobs. It’s not only the artists, whom nobody cares about until they’re gone, it’s copyeditors and clerks and designers. And just like self-checkouts and airport entry surveys, the humans are replaced by something a little bit worse. But it’s cheaper, and novelty often obscures indignity long enough for it to entrench, and we all accept that everything is a little bit slower, a little bit less trustworthy, and everything has a little more friction to grind us down over each day. The replacement bots could be honed into better tools, but who will bother once they’re accepted? Market trends always converge on giving us as little as possible.</p>
</blockquote>
<p>I&rsquo;d also be amiss if I didn&rsquo;t highlight this crucial insight into our collective fears of AIs:</p>
<blockquote>
<p>The terror of building a super artificial intelligence is not due to having something super intelligent hanging around, it is the terror of having something super intelligent that acts like a human. Because if we manage to build something technologically superior to us that also acts like us, it will do what technologically superior humans always do to their neighbors.</p>
</blockquote>
<p>Perhaps there are things that we, humanity as a whole, should just decide that we should not build &ndash; not because we cannot or because it would be evil, but because we are not sure it&rsquo;s a net positive as a whole. Of course, if we <em>were</em> able to choose so, we probably would have less to be concerned about already, given that we&rsquo;d clearly have a mechanism for informed, preventive, and collective action.</p>

		</div>
	  </div>
	

</div>


    
    

  </body>

</html>
