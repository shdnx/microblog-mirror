{
	"version": "https://jsonfeed.org/version/1",
	"title": "The Stargazer",
	"icon": "https://micro.blog/shdnx/avatar.jpg",
	"home_page_url": "https://blog.gaborkozar.me/",
	"feed_url": "https://blog.gaborkozar.me/feed.json",
	"items": [
		
			{
				"id": "http://shdnx.micro.blog/2023/08/07/modern-movies-on.html",
				"title": "Modern movies on human bodies: everyone is beautiful and no one is horny",
				"content_html": "<p>An interesting, thought-provoking, and well-written article:</p>\n<h2 id=\"everyone-is-beautiful-and-no-one-is-hornyhttpsbloodknifecomeveryone-beautiful-no-one-horny\"><a href=\"https://bloodknife.com/everyone-beautiful-no-one-horny/\">Everyone is beautiful and no one is horny</a></h2>\n<blockquote>\n<p>Modern action and superhero films fetishize the body, even as they desexualize it.</p>\n</blockquote>\n<p>See also the discussion on <a href=\"https://news.ycombinator.com/item?id=36990215\">Hacker News</a>.</p>\n<blockquote>\n<p>Of course there’s sex in a movie. Isn’t there always?</p>\n<p>The answer, of course, is not anymore—at least not when it comes to modern blockbusters</p>\n<p>We’re told that Tony Stark and Pepper Potts are an item, but no actual romantic or sexual chemistry between them is shown in the films. [&hellip;]</p>\n</blockquote>\n<p>I&rsquo;m not a particularly avid movie-watcher, and I feel mostly uninterested in the superhero movies that appear to have flooded the cinemas in the last few years. I say this to highlight the point that <em>even I have noticed this</em>: the utter sterility of many (most?) mainstream movies with their total lack of sexual chemistry. I have thought that this is just Hollywood appealing to disappointingly dated purist sensitivities that the United States seems to be eternally stuck in, but this article offers a much more interesting exploration.</p>\n<p>Go read the article! I&rsquo;m going to just pull this one paragraph out of the article without context, because I found it hilarious and accurate:</p>\n<blockquote>\n<p>Contemporary gym ads focus on rigidly isolated self-improvement: be your best self. Create a new you. We don’t exercise, we don’t work out: we train, and we train in fitness programs with names like Booty Bootcamp, as if we’re getting our booties battle-ready to fight in the Great Booty War. There is no promise of intimacy. Like our heroes in the Marvel Cinematic Universe, like Rico and Dizzy and all the other infantry in Starship Troopers, we are horny only for annihilation.</p>\n</blockquote>\n",
				"content_text": "An interesting, thought-provoking, and well-written article:\r\n## [Everyone is beautiful and no one is horny](https://bloodknife.com/everyone-beautiful-no-one-horny/)\r\n> Modern action and superhero films fetishize the body, even as they desexualize it.\r\n\r\nSee also the discussion on [Hacker News](https://news.ycombinator.com/item?id=36990215).\r\n\r\n> Of course there’s sex in a movie. Isn’t there always?\r\n>\r\n> The answer, of course, is not anymore—at least not when it comes to modern blockbusters\r\n>\r\n> We’re told that Tony Stark and Pepper Potts are an item, but no actual romantic or sexual chemistry between them is shown in the films. [...]\r\n\r\nI'm not a particularly avid movie-watcher, and I feel mostly uninterested in the superhero movies that appear to have flooded the cinemas in the last few years. I say this to highlight the point that _even I have noticed this_: the utter sterility of many (most?) mainstream movies with their total lack of sexual chemistry. I have thought that this is just Hollywood appealing to disappointingly dated purist sensitivities that the United States seems to be eternally stuck in, but this article offers a much more interesting exploration.\r\n\r\nGo read the article! I'm going to just pull this one paragraph out of the article without context, because I found it hilarious and accurate:\r\n\r\n> Contemporary gym ads focus on rigidly isolated self-improvement: be your best self. Create a new you. We don’t exercise, we don’t work out: we train, and we train in fitness programs with names like Booty Bootcamp, as if we’re getting our booties battle-ready to fight in the Great Booty War. There is no promise of intimacy. Like our heroes in the Marvel Cinematic Universe, like Rico and Dizzy and all the other infantry in Starship Troopers, we are horny only for annihilation.\n",
				"date_published": "2023-08-07T00:08:29+02:00",
				"url": "https://blog.gaborkozar.me/2023/08/07/modern-movies-on.html",
				"tags": ["movies series and books"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/27/why-is-online.html",
				"title": "Why is online dating terrible?",
				"content_html": "<p>It&rsquo;s not exactly controversial to say that online dating is awful, at least for those seeking heterosexual relationships. It sucks very differently for men and women though, and from what I hear it can actually be pretty good for homosexual relationships. (Especially for gay men, I think? I haven&rsquo;t really heard about it for women, but then again I do live under a rock, in a cave, and on another planet.)</p>\n<p>This difference is interesting and worth exploring, though anything as socially and culturally involved as sex, dating, and relationships is obviously riddled with proverbial mines. I&rsquo;ve recently come across this video though that I think did an unusually good job at analyzing the situation:</p>\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/x3lypVnJ0HM' frameborder='0' allowfullscreen></iframe></div>\n<p>He highlights two main problems: there are many more men on these dating apps then women, and women appear to judge the profiles of men much more harshly for attractiveness than vica-versa. It&rsquo;s obviously nigh-impossible to make online dating not be awful, with such a premise, though various attempts have been made.</p>\n<p>Beyond the cultural and social controversy surrounding the whole topic, I would like to highlight one thing: running an effective online dating app would be <em>terrible</em> business under most models, whether it&rsquo;s making money from ads, optional purchasable extra features, or subscriptions. Imagine if the average user signed up for the app, created their profile, started using it, and the app would be so effective that they&rsquo;d immediately get genuine dates with real, interesting partners. They&rsquo;d probably find themselves in a new relationship in no time and would have no need for the app anymore!</p>\n<p>A non-user doesn&rsquo;t make money, therefore it is the financial interest of the business to keep you a user. On the other hand, if the app is completely ineffective, then again it would lose all but the most desperate potential users. because then it&rsquo;s just a waste of time, why would you pay for it? In order to keep the active user count as high as possible, the app cannot be too effective nor too ineffective.</p>\n<p>The incentives of the users (who want to find sexual or romantic partners) and the business (who wants you to keep using it for as long as possible) are greatly misaligned, and that is not good. Any success of any kind of social or economic system only comes from successfully aligning the interests of as many groups as possible.</p>\n<p>Perhaps there is a business model that could work for online dating that would not be so horribly misaligned, but if so, I haven&rsquo;t seen it yet.</p>\n",
				"content_text": "It's not exactly controversial to say that online dating is awful, at least for those seeking heterosexual relationships. It sucks very differently for men and women though, and from what I hear it can actually be pretty good for homosexual relationships. (Especially for gay men, I think? I haven't really heard about it for women, but then again I do live under a rock, in a cave, and on another planet.)\r\n\r\nThis difference is interesting and worth exploring, though anything as socially and culturally involved as sex, dating, and relationships is obviously riddled with proverbial mines. I've recently come across this video though that I think did an unusually good job at analyzing the situation:\r\n\r\n{{<youtube x3lypVnJ0HM>}}\r\n\r\nHe highlights two main problems: there are many more men on these dating apps then women, and women appear to judge the profiles of men much more harshly for attractiveness than vica-versa. It's obviously nigh-impossible to make online dating not be awful, with such a premise, though various attempts have been made.\r\n\r\nBeyond the cultural and social controversy surrounding the whole topic, I would like to highlight one thing: running an effective online dating app would be _terrible_ business under most models, whether it's making money from ads, optional purchasable extra features, or subscriptions. Imagine if the average user signed up for the app, created their profile, started using it, and the app would be so effective that they'd immediately get genuine dates with real, interesting partners. They'd probably find themselves in a new relationship in no time and would have no need for the app anymore!\r\n\r\nA non-user doesn't make money, therefore it is the financial interest of the business to keep you a user. On the other hand, if the app is completely ineffective, then again it would lose all but the most desperate potential users. because then it's just a waste of time, why would you pay for it? In order to keep the active user count as high as possible, the app cannot be too effective nor too ineffective.\r\n\r\nThe incentives of the users (who want to find sexual or romantic partners) and the business (who wants you to keep using it for as long as possible) are greatly misaligned, and that is not good. Any success of any kind of social or economic system only comes from successfully aligning the interests of as many groups as possible.\r\n\r\nPerhaps there is a business model that could work for online dating that would not be so horribly misaligned, but if so, I haven't seen it yet.\n",
				"date_published": "2023-07-27T00:48:05+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/27/why-is-online.html",
				"tags": ["technology"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/26/prql-its-like.html",
				"title": "PRQL: it's like SQL but actually makes sense",
				"content_html": "<p>I have never had to write particularly complex SQL queries, but I&rsquo;ve written and seen enough to know that that SQL gets nasty and complicated once you start using the non-trivial features, not to mention things like optimizing queries based on the execution plan, or optimizing indexes, or the other million things that make Database Engineer an actual job title.</p>\n<p>One thing in particular that has always bothered me (even with my very limited experience!) about SQL is how it tries to read like English, which makes it harder to both read and write in many cases, the simplest and canonical one being that you have to write <code>SELECT ... FROM table</code>. What table we are querying is the first most important information for any readers, and it&rsquo;s also the first thing a query writer must decide, yet SQL forces you to write it second. This gets especially annoying when the <code>SELECT</code> clause is very complex and consists of multiple lines. To illustrate, here is an example I blatantly stole from an <a href=\"https://learnsql.com/blog/25-advanced-sql-query-examples/\">SQL tutorial website</a>:</p>\n<pre><code>SELECT\r\n  SUM (CASE\r\n    WHEN dept_id IN (‘SALES’,’HUMAN RESOURCES’)\r\n    THEN salary\r\n    ELSE 0 END) AS total_salary_sales_and_hr,\r\n  SUM (CASE\r\n    WHEN dept_id IN (‘IT’,’SUPPORT’)\r\n    THEN salary\r\n    ELSE 0 END) AS total_salary_it_and_support\r\nFROM employee\r\n</code></pre>\n<p>The table you are reading from is <em>critical</em> information in giving you the context of what you are looking at and allow you to begin understanding what this query is supposed to be doing, and it&rsquo;s hidden at the very end of the query.</p>\n<p>So yeah, SQL is not the best, and this is where Pipelined Relational Query Language (PRQL, pronounced “Prequel”) comes in and promises to do better: <a href=\"https://prql-lang.org/\">PRQL: a modern language for transforming data</a> (<a href=\"https://news.ycombinator.com/item?id=36866861\">Hacker News thread</a>), promising &ldquo;a simple, powerful, pipelined SQL replacement&rdquo;. At a glance, it looks like everything I&rsquo;d want it to be!</p>\n<p>I&rsquo;m really glad this exists, even though I personally don&rsquo;t have much use for it at the moment.</p>\n",
				"content_text": "I have never had to write particularly complex SQL queries, but I've written and seen enough to know that that SQL gets nasty and complicated once you start using the non-trivial features, not to mention things like optimizing queries based on the execution plan, or optimizing indexes, or the other million things that make Database Engineer an actual job title.\r\n\r\nOne thing in particular that has always bothered me (even with my very limited experience!) about SQL is how it tries to read like English, which makes it harder to both read and write in many cases, the simplest and canonical one being that you have to write `SELECT ... FROM table`. What table we are querying is the first most important information for any readers, and it's also the first thing a query writer must decide, yet SQL forces you to write it second. This gets especially annoying when the `SELECT` clause is very complex and consists of multiple lines. To illustrate, here is an example I blatantly stole from an [SQL tutorial website](https://learnsql.com/blog/25-advanced-sql-query-examples/):\r\n\r\n    SELECT\r\n      SUM (CASE\r\n        WHEN dept_id IN (‘SALES’,’HUMAN RESOURCES’)\r\n        THEN salary\r\n        ELSE 0 END) AS total_salary_sales_and_hr,\r\n      SUM (CASE\r\n        WHEN dept_id IN (‘IT’,’SUPPORT’)\r\n        THEN salary\r\n        ELSE 0 END) AS total_salary_it_and_support\r\n    FROM employee\r\n\r\nThe table you are reading from is _critical_ information in giving you the context of what you are looking at and allow you to begin understanding what this query is supposed to be doing, and it's hidden at the very end of the query.\r\n\r\nSo yeah, SQL is not the best, and this is where Pipelined Relational Query Language (PRQL, pronounced “Prequel”) comes in and promises to do better: [PRQL: a modern language for transforming data](https://prql-lang.org/) ([Hacker News thread](https://news.ycombinator.com/item?id=36866861)), promising \"a simple, powerful, pipelined SQL replacement\". At a glance, it looks like everything I'd want it to be!\r\n\r\nI'm really glad this exists, even though I personally don't have much use for it at the moment.\n",
				"date_published": "2023-07-26T02:13:22+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/26/prql-its-like.html",
				"tags": ["software development"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/25/its-turtles-all.html",
				"title": "It's turtles all the way down: from high-level programming to CPU microcode",
				"content_html": "<p><a href=\"https://en.wikipedia.org/wiki/Turtles_all_the_way_down\">It&rsquo;s turtles all the way down</a>:</p>\n<blockquote>\n<p>The following anecdote is told of William James. [&hellip;] After a lecture on cosmology and the structure of the solar system, James was accosted by a little old lady.</p>\n<p>&ldquo;Your theory that the sun is the centre of the solar system, and the earth is a ball which rotates around it has a very convincing ring to it, Mr. James, but it&rsquo;s wrong. I&rsquo;ve got a better theory,&rdquo; said the little old lady.</p>\n<p>&ldquo;And what is that, madam?&rdquo; inquired James politely.</p>\n<p>&ldquo;That we live on a crust of earth which is on the back of a giant turtle.&rdquo;</p>\n<p>Not wishing to demolish this absurd little theory by bringing to bear the masses of scientific evidence he had at his command, James decided to gently dissuade his opponent by making her see some of the inadequacies of her position.</p>\n<p>&ldquo;If your theory is correct, madam,&rdquo; he asked, &ldquo;what does this turtle stand on?&rdquo;</p>\n<p>&ldquo;You&rsquo;re a very clever man, Mr. James, and that&rsquo;s a very good question,&rdquo; replied the little old lady, &ldquo;but I have an answer to it. And it&rsquo;s this: The first turtle stands on the back of a second, far larger, turtle, who stands directly under him.&rdquo;</p>\n<p>&ldquo;But what does this second turtle stand on?&rdquo; persisted James patiently.</p>\n<p>To this, the little old lady crowed triumphantly,</p>\n<p>&ldquo;It&rsquo;s no use, Mr. James—it&rsquo;s turtles all the way down.&rdquo;</p>\n<p>—  J. R. Ross, Constraints on Variables in Syntax, 1967</p>\n</blockquote>\n<p>As a teenager, what caught my interest in programming was the question: wait, but how does it work?\nIt started &ndash; as is probably very common &ndash; with a game: a browser-based online game called Galactica. One day I discovered the View Source option of the browser, and the rest, as they say, is history.</p>\n<p>That was how I started out with web development, using PHP, HTML, and just enough JavaScript and CSS to think that I Know Stuff. Then I switched to harder stuff: C# was my next main language. Shortly after (you know, when the rush just isn&rsquo;t the same anymore, so you need heavier stuff) I tried picking up C++. Learning C++ only by doing is very hard though: it is a supremely beginner-unfriendly language, so I didn&rsquo;t get very far with it. It was only during my Computer Science Bachelor&rsquo;s studies that I received more formal education in C++, and it became my favorite language: it allows you to go as low-level or high-level as you want. You want to take a random integer, convert it to a memory address, and write to it? Sure thing, buddy! <sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup> You&rsquo;d like to have a hash map mapping strings to callback functions defined for example as lambda expressions? You&rsquo;ve got it! <sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup></p>\n<p>I should mention that I also have a fondness for C, but it&rsquo;s too cumbersome to work with it for any nontrivial project. Who wants to have to re-implement even trivial mechanisms like lists (or vectors, as C++ calls them) or strings? There&rsquo;s nothing really appealing about C, in my opinion, compared to C++, if you have the choice; of course, many micro-controllers and more exotic architectures only have working C compilers and toolchains.</p>\n<p>For most practical purposes, there&rsquo;s just no reason to go lower level than C or C++, but of course, you can if you&rsquo;re determined: you can write Assembly. (C and C++ both support inline assembly, but that doesn&rsquo;t work for everything.) Some things you can only do in Assembly, usually things that are the responsibilities of the operating system kernel: configuring the CPU is one of them, context-switching between threads is another (since you need to switch stacks by overwriting <code>%rsp</code> on x86-64). But yeah, if C is often an inconvenient language to work in, Assembly is just torture, even with modern assemblers having added many convenience features. Generally speaking, you&rsquo;d write just a file or two in Assembly, containing the functionality you need, and call it from C (or C++ via <code>extern &quot;C&quot;</code>).</p>\n<p>There&rsquo;s yet more turtles though. Assembly compiles down into machine code, which really is just highly unpleasant to even read, especially after the compiler applied optimizations to it (something that the C and C++ compilers are notoriously good at). It&rsquo;s a common idea that the CPU then executes the machine code, but this has not been true for a very long time: even machine code is just a &ldquo;high-level&rdquo; language for the CPU, except one that the CPU knows how to compile itself into <em>microcode</em>. The primary reason for this is the usual one: seeking extra performance.</p>\n<p>Indeed, the most common motivations for anybody choosing to work in a language lower-level than something than C# or Java is the need to more closely control what the hardware does, often in order to ensure that it is <em>fast</em>. Game development and high-frequency trading are two good examples. But even working in C++ and doing clever things with memory only gets you so far: compiler optimizations will invoke Cthulhu and sacrifice virgin goats to turn your code into an unspeakable horror-show in the name of the performance. This involves doing things you&rsquo;d never want to do by hand, such as unrolling a loop, or creating multiple versions of the same code specialized under different run-time conditions.<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup></p>\n<p>Applying optimizations on code written in unrestrictive languages such as C or C++ is no easy task though. These are the languages where the programmer is allowed to manually manipulate pointers or define (raw, untagged) unions. For optimizations to take place, the compiler must be able to make assumptions about what your code does: in general, the better the compiler understands what you are trying to do, the better and faster code it will be able to generate for you to achieve it. For these assumptions to be guaranteed to hold, you need rules, and therein lies the fundamental tension between the &ldquo;you are the boss&rdquo; philosophy of the language and the need to make rules so that the compiler can optimize your code. (This is also, sort of, the reason why code written in e.g. C# or Java can sometimes outperform common C++ code: those languages are a lot more restrictive, allowing for less fishy business, and thereby allowing the compiler and their runtime environment to have a better understanding and therefore generate faster code.)</p>\n<p>This same idea applies a lot more broadly than just programming languages and compilers: if you&rsquo;ve ever written a highly performance-sensitive code as a library, you&rsquo;ve felt the struggle. On the one hand, you want your library to be easy to use, without the user having to understand your implementation details, but you also want to make sure that the performance stays optimal, which almost inevitably leads to implementation details leaking out of your elegant API: you need the user code to tell you what it wants to achieve in details that the user code fundamentally shouldn&rsquo;t really have to particularly care about but is forced to due to performance considerations. (A simple example is the size of an internal buffer to use.)</p>\n<p>But, to get back to our stack of turtles, this is also why even machine code needs to be compiled: machine code often gets new and more complex or more specialized instructions to execute, such as <a href=\"https://stackoverflow.com/a/43845829\">instructions prefixed with <code>rep</code></a> that allow an operation to be repeated more efficiently than writing a loop. These complex instructions need to be broken down into more fundamental, low-level instructions that is practical to build hardware circuitry for, but which may be specific even to particular CPU models. The opposite also happens: sometimes multiple machine code instructions are actually compiled into a single microcode instruction, called <a href=\"https://stackoverflow.com/a/56413946\">macro-fusion</a>.</p>\n<p>Microcode by the way can receive updates, when CPU vendors need to fix or work around bugs in the hardware (I know right), or to fix or mitigate security vulnerabilities. There have been many examples for the latter in the last 5 years, <a href=\"https://lock.cmpxchg8b.com/zenbleed.html\">Zenbleed</a> (<a href=\"https://news.ycombinator.com/item?id=36848680\">Hacker News thread</a>) being the most recent one. Updates live in volatile memory (usually the RAM), and so it must be <a href=\"https://wiki.gentoo.org/wiki/Microcode\">loaded on every boot</a>, either by BIOS/UEFI, or in some cases, by the OS kernel directly.</p>\n<p>Do we have an bigger turtle that underlies this microcode? We definitely do have at least\n<sup id=\"fnref:4\"><a href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\">4</a></sup> one more: they are called <a href=\"https://en.wikipedia.org/wiki/Logic_gate\">logic gates</a>, the fundamental primitives that ultimately underlie all instruction execution. Logic gates usually operate on just individual bits: a 0 (logical false) or 1 (logical true). They are very simple beasts, and there are not that many of them. The simplest ones are: <code>not</code> (flips a bit from 0 to 1 or from 1 to 0), <code>and</code> (yields 1 if both input bits are 1, else 0), and <code>or</code> (yields 1 if either input bits are 1, else 0).</p>\n<section class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\" role=\"doc-endnote\">\n<p>Of course, caveats apply, mostly revolving around <em>undefined behaviour</em>. The compiler needs to have a good understanding of what your code is trying to do in order to be able to properly optimize it. If you violate its assumptions, bad things happen: if you&rsquo;re lucky, your application will just crash. If you&rsquo;re not lucky, you get impossible-to-track down, strange, once-in-a-while odd behaviour in a completely unrelated part of the program.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\" role=\"doc-endnote\">\n<p>The C++11, 17, and 20 standards have added lots of convenience features to the language and standard library alike. C++03 and earlier are just sadness: if you are forced to work with them, my condolences!&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\" role=\"doc-endnote\">\n<p>One example of this is <em>loop unswitching</em> where a loop containing a branch whose condition is invariant in a loop is pulled out of the loop, turning <code>while (true) { if (flag) do_a(); else do_b(); }</code> into <code>if (flag) { while (true) do_a(); } else { while (true) do_b(); }</code> if it can be proven that the value of <code>flag</code> cannot be affected by either <code>do_a()</code> or <code>do_b()</code>.&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:4\" role=\"doc-endnote\">\n<p>In between microcode and logic gates there are definitely some more layers of turtles, which I don&rsquo;t have a good idea about: it&rsquo;s the realm of hardware implementation.&#160;<a href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</section>\n",
				"content_text": "[It's turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down):\n> The following anecdote is told of William James. [...] After a lecture on cosmology and the structure of the solar system, James was accosted by a little old lady.\n>\n> \"Your theory that the sun is the centre of the solar system, and the earth is a ball which rotates around it has a very convincing ring to it, Mr. James, but it's wrong. I've got a better theory,\" said the little old lady.\n>\n> \"And what is that, madam?\" inquired James politely.\n>\n> \"That we live on a crust of earth which is on the back of a giant turtle.\"\n>\n> Not wishing to demolish this absurd little theory by bringing to bear the masses of scientific evidence he had at his command, James decided to gently dissuade his opponent by making her see some of the inadequacies of her position.\n>\n> \"If your theory is correct, madam,\" he asked, \"what does this turtle stand on?\"\n>\n> \"You're a very clever man, Mr. James, and that's a very good question,\" replied the little old lady, \"but I have an answer to it. And it's this: The first turtle stands on the back of a second, far larger, turtle, who stands directly under him.\"\n>\n> \"But what does this second turtle stand on?\" persisted James patiently.\n>\n> To this, the little old lady crowed triumphantly,\n>\n> \"It's no use, Mr. James—it's turtles all the way down.\"\n>\n> —  J. R. Ross, Constraints on Variables in Syntax, 1967\n\nAs a teenager, what caught my interest in programming was the question: wait, but how does it work?\nIt started -- as is probably very common -- with a game: a browser-based online game called Galactica. One day I discovered the View Source option of the browser, and the rest, as they say, is history.\n\nThat was how I started out with web development, using PHP, HTML, and just enough JavaScript and CSS to think that I Know Stuff. Then I switched to harder stuff: C# was my next main language. Shortly after (you know, when the rush just isn't the same anymore, so you need heavier stuff) I tried picking up C++. Learning C++ only by doing is very hard though: it is a supremely beginner-unfriendly language, so I didn't get very far with it. It was only during my Computer Science Bachelor's studies that I received more formal education in C++, and it became my favorite language: it allows you to go as low-level or high-level as you want. You want to take a random integer, convert it to a memory address, and write to it? Sure thing, buddy! [^cpp-undefined-behaviour] You'd like to have a hash map mapping strings to callback functions defined for example as lambda expressions? You've got it! [^cpp-versions]\n\nI should mention that I also have a fondness for C, but it's too cumbersome to work with it for any nontrivial project. Who wants to have to re-implement even trivial mechanisms like lists (or vectors, as C++ calls them) or strings? There's nothing really appealing about C, in my opinion, compared to C++, if you have the choice; of course, many micro-controllers and more exotic architectures only have working C compilers and toolchains.\n\nFor most practical purposes, there's just no reason to go lower level than C or C++, but of course, you can if you're determined: you can write Assembly. (C and C++ both support inline assembly, but that doesn't work for everything.) Some things you can only do in Assembly, usually things that are the responsibilities of the operating system kernel: configuring the CPU is one of them, context-switching between threads is another (since you need to switch stacks by overwriting `%rsp` on x86-64). But yeah, if C is often an inconvenient language to work in, Assembly is just torture, even with modern assemblers having added many convenience features. Generally speaking, you'd write just a file or two in Assembly, containing the functionality you need, and call it from C (or C++ via `extern \"C\"`).\n\nThere's yet more turtles though. Assembly compiles down into machine code, which really is just highly unpleasant to even read, especially after the compiler applied optimizations to it (something that the C and C++ compilers are notoriously good at). It's a common idea that the CPU then executes the machine code, but this has not been true for a very long time: even machine code is just a \"high-level\" language for the CPU, except one that the CPU knows how to compile itself into _microcode_. The primary reason for this is the usual one: seeking extra performance.\n\nIndeed, the most common motivations for anybody choosing to work in a language lower-level than something than C# or Java is the need to more closely control what the hardware does, often in order to ensure that it is _fast_. Game development and high-frequency trading are two good examples. But even working in C++ and doing clever things with memory only gets you so far: compiler optimizations will invoke Cthulhu and sacrifice virgin goats to turn your code into an unspeakable horror-show in the name of the performance. This involves doing things you'd never want to do by hand, such as unrolling a loop, or creating multiple versions of the same code specialized under different run-time conditions.[^loop-unswitching]\n\nApplying optimizations on code written in unrestrictive languages such as C or C++ is no easy task though. These are the languages where the programmer is allowed to manually manipulate pointers or define (raw, untagged) unions. For optimizations to take place, the compiler must be able to make assumptions about what your code does: in general, the better the compiler understands what you are trying to do, the better and faster code it will be able to generate for you to achieve it. For these assumptions to be guaranteed to hold, you need rules, and therein lies the fundamental tension between the \"you are the boss\" philosophy of the language and the need to make rules so that the compiler can optimize your code. (This is also, sort of, the reason why code written in e.g. C# or Java can sometimes outperform common C++ code: those languages are a lot more restrictive, allowing for less fishy business, and thereby allowing the compiler and their runtime environment to have a better understanding and therefore generate faster code.)\n\nThis same idea applies a lot more broadly than just programming languages and compilers: if you've ever written a highly performance-sensitive code as a library, you've felt the struggle. On the one hand, you want your library to be easy to use, without the user having to understand your implementation details, but you also want to make sure that the performance stays optimal, which almost inevitably leads to implementation details leaking out of your elegant API: you need the user code to tell you what it wants to achieve in details that the user code fundamentally shouldn't really have to particularly care about but is forced to due to performance considerations. (A simple example is the size of an internal buffer to use.)\n\nBut, to get back to our stack of turtles, this is also why even machine code needs to be compiled: machine code often gets new and more complex or more specialized instructions to execute, such as [instructions prefixed with `rep`](https://stackoverflow.com/a/43845829) that allow an operation to be repeated more efficiently than writing a loop. These complex instructions need to be broken down into more fundamental, low-level instructions that is practical to build hardware circuitry for, but which may be specific even to particular CPU models. The opposite also happens: sometimes multiple machine code instructions are actually compiled into a single microcode instruction, called [macro-fusion](https://stackoverflow.com/a/56413946).\n\nMicrocode by the way can receive updates, when CPU vendors need to fix or work around bugs in the hardware (I know right), or to fix or mitigate security vulnerabilities. There have been many examples for the latter in the last 5 years, [Zenbleed](https://lock.cmpxchg8b.com/zenbleed.html) ([Hacker News thread](https://news.ycombinator.com/item?id=36848680)) being the most recent one. Updates live in volatile memory (usually the RAM), and so it must be [loaded on every boot](https://wiki.gentoo.org/wiki/Microcode), either by BIOS/UEFI, or in some cases, by the OS kernel directly.\n\nDo we have an bigger turtle that underlies this microcode? We definitely do have at least\n[^hardware-turtles] one more: they are called [logic gates](https://en.wikipedia.org/wiki/Logic_gate), the fundamental primitives that ultimately underlie all instruction execution. Logic gates usually operate on just individual bits: a 0 (logical false) or 1 (logical true). They are very simple beasts, and there are not that many of them. The simplest ones are: `not` (flips a bit from 0 to 1 or from 1 to 0), `and` (yields 1 if both input bits are 1, else 0), and `or` (yields 1 if either input bits are 1, else 0).\n\n[^cpp-undefined-behaviour]: Of course, caveats apply, mostly revolving around _undefined behaviour_. The compiler needs to have a good understanding of what your code is trying to do in order to be able to properly optimize it. If you violate its assumptions, bad things happen: if you're lucky, your application will just crash. If you're not lucky, you get impossible-to-track down, strange, once-in-a-while odd behaviour in a completely unrelated part of the program.\n[^cpp-versions]: The C++11, 17, and 20 standards have added lots of convenience features to the language and standard library alike. C++03 and earlier are just sadness: if you are forced to work with them, my condolences!\n[^loop-unswitching]: One example of this is _loop unswitching_ where a loop containing a branch whose condition is invariant in a loop is pulled out of the loop, turning `while (true) { if (flag) do_a(); else do_b(); }` into `if (flag) { while (true) do_a(); } else { while (true) do_b(); }` if it can be proven that the value of `flag` cannot be affected by either `do_a()` or `do_b()`.\n[^hardware-turtles]: In between microcode and logic gates there are definitely some more layers of turtles, which I don't have a good idea about: it's the realm of hardware implementation.\n",
				"date_published": "2023-07-25T03:06:15+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/25/its-turtles-all.html",
				"tags": ["technology","software development"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/22/us-lawmakers-trading.html",
				"title": "US lawmakers trading and stock ownership",
				"content_html": "<p><a href=\"https://www.wsj.com/articles/senators-to-propose-ban-on-u-s-lawmakers-executive-branch-members-owning-stock-6db6411\">Senators to propose ban on US lawmakers and executive branch members owning single stocks</a>, once again. The <a href=\"https://news.ycombinator.com/item?id=36785467\">Hacker News thread</a> is also quite informative, <a href=\"https://news.ycombinator.com/item?id=36786367\">for example</a>:</p>\n<blockquote>\n<p>I started building out tools to track congressional stock trading in 2020.</p>\n<p>Since then, I believe there have been 9 other proposals similar to this one. None of them have been even called to a vote.</p>\n<p>It seem like Congress is pretty unwilling to regulate its own trading.\nUntil they are, feel free to track the trading here: <a href=\"https://www.quiverquant.com/congresstrading/\">www.quiverquant.com/congresst&hellip;</a></p>\n</blockquote>\n<p>Like <a href=\"https://news.ycombinator.com/item?id=36785705\">many others</a>, I feel quite disillusioned and uninspired by this:</p>\n<blockquote>\n<p>All show and no go.</p>\n<p>Like most such &ldquo;reforms&rdquo;, it is intended for naive public consumption while also being easily circumvented.\nSetup a trust or corporation run by a family member and continue with business a usual.</p>\n</blockquote>\n<p>Senators being allowed to do what is for all intents and purposes insider trading, is frankly, completely insane. Pelosi is perhaps the most egregious example: <a href=\"https://nypost.com/2022/10/05/house-speaker-nancy-pelosi-has-accrued-millions-from-husbands-trades-report/\">House Speaker Nancy Pelosi has accrued millions from husband’s trades: report </a>:</p>\n<blockquote>\n<p>The speaker, one of the richest members of Congress, has vehemently denied sharing any information with her spouse — a venture capitalist.</p>\n<p>However, many have questioned trades made by Paul Pelosi that happened to coincide with major congressional decisions.</p>\n<p>In June, he exercised call options to purchase up to $5 million in the graphics card manufacturer Nvidia just weeks before the House considered a bill to provide more than $50 billion in subsidies to domestic semiconductor manufacturers, the Beacon said.</p>\n</blockquote>\n<p>Yeah. She&rsquo;s not going to jail. So I&rsquo;d strongly support efforts to curtail this, but I just do not believe that you actually can effectively do so in practice. Ultimately, the forms of power are interchangable: if you&rsquo;re influential, for example because you&rsquo;re in Congress, then you&rsquo;ll always have easy opportunities to get (more) rich off of it, and also vica versa.</p>\n",
				"content_text": "[Senators to propose ban on US lawmakers and executive branch members owning single stocks](https://www.wsj.com/articles/senators-to-propose-ban-on-u-s-lawmakers-executive-branch-members-owning-stock-6db6411), once again. The [Hacker News thread](https://news.ycombinator.com/item?id=36785467) is also quite informative, [for example](https://news.ycombinator.com/item?id=36786367):\r\n> I started building out tools to track congressional stock trading in 2020.\r\n>\r\n> Since then, I believe there have been 9 other proposals similar to this one. None of them have been even called to a vote.\r\n>\r\n> It seem like Congress is pretty unwilling to regulate its own trading.\r\n> Until they are, feel free to track the trading here: [www.quiverquant.com/congresst...](https://www.quiverquant.com/congresstrading/)\r\n\r\nLike [many others](https://news.ycombinator.com/item?id=36785705), I feel quite disillusioned and uninspired by this:\r\n> All show and no go.\r\n> \r\n> Like most such \"reforms\", it is intended for naive public consumption while also being easily circumvented.\r\n> Setup a trust or corporation run by a family member and continue with business a usual. \r\n\r\nSenators being allowed to do what is for all intents and purposes insider trading, is frankly, completely insane. Pelosi is perhaps the most egregious example: [House Speaker Nancy Pelosi has accrued millions from husband’s trades: report ](https://nypost.com/2022/10/05/house-speaker-nancy-pelosi-has-accrued-millions-from-husbands-trades-report/):\r\n\r\n> The speaker, one of the richest members of Congress, has vehemently denied sharing any information with her spouse — a venture capitalist.\r\n>\r\n> However, many have questioned trades made by Paul Pelosi that happened to coincide with major congressional decisions.\r\n>\r\n> In June, he exercised call options to purchase up to $5 million in the graphics card manufacturer Nvidia just weeks before the House considered a bill to provide more than $50 billion in subsidies to domestic semiconductor manufacturers, the Beacon said.\r\n\r\nYeah. She's not going to jail. So I'd strongly support efforts to curtail this, but I just do not believe that you actually can effectively do so in practice. Ultimately, the forms of power are interchangable: if you're influential, for example because you're in Congress, then you'll always have easy opportunities to get (more) rich off of it, and also vica versa.\n",
				"date_published": "2023-07-22T01:43:15+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/22/us-lawmakers-trading.html",
				"tags": ["finance"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/22/financial-news-and.html",
				"title": "Financial news and moving the markets",
				"content_html": "<p>Daring Fireball wrote about the <a href=\"https://daringfireball.net/2023/07/apple_gpt_bloomberg\">Apple GPT news</a>. I think the news itself is pretty uninteresting: Apple wants in on the ChatGPT gold-rush even as it seems to be winding down. The more interesting part is what he writes about Bloomberg:</p>\n<blockquote>\n<p>Bloomberg reporters are evaluated and receive bonuses tied to reporting market-moving news. They’re incentivized financially to make mountains out of molehills, and craters out of divots, to maximize the immediate effect of their reporting on stock prices. And Bloomberg appends these stock price movements right there in their reports, to drive home the notion that Bloomberg publishes market-moving news, so maybe you too should spend over $2,000 per month on a Bloomberg Terminal so that you can receive news reports from Bloomberg minutes before the general public, and buy, sell, and short stocks based on that news.</p>\n<p>&hellip;</p>\n<p>Apple’s brief 2.7 percent jump and Microsoft’s smaller but still-significant drop, both at 12:04pm, were clearly caused by Gurman’s report. Bloomberg Terminal subscribers get such reports before anyone else. [&hellip;] most of their original reporting is delivered with the goal of moving the stock prices of the companies they’re reporting on, for the purpose of proving the value of a Bloomberg Terminal’s hefty subscription fee to day-trading gamblers [&hellip;]</p>\n</blockquote>\n<p>Tinfoil hat time? I mean&hellip; that&rsquo;s quite the stretch. Even if Bloomberg doesn&rsquo;t report particular news, a million other websites will; how does Bloomberg move the market in particular?\nBloomberg is hardly the only company selling ahead-of-the-crowd services; it&rsquo;s obviously a lucrative thing to do successfully, given the herd mentality that all markets follow<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>.\nThey obviously want to communicate news that have <em>the potential</em> of moving stocks, otherwise they&rsquo;d not be a good source of finance news!</p>\n<p>As for making mountains out of molehills, I think that&rsquo;s a fair criticism to an extent, but it&rsquo;s also not one that is in any way unique to Bloomberg. Clickbait and blowing news out of proportion is just business as usual for mass media these days, thanks to the advertising industry financing a large chunk of websites and competition <a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">fierce enough to trigger</a> a race to the bottom with low-effort reporting. My personal impression is that Bloomberg is actually better than many in this regard.</p>\n<p>By the way, the Apple stock <em>stayed up</em> after these news (see the <a href=\"https://www.investing.com/equities/apple-computer-inc\">Investing.com chart</a>), only dropping back to previous levels some 24 hours later when manufacturing issues were reported with the upcoming iPhone 15. So it was <em>good reporting</em> by Bloomberg.</p>\n<!-- raw HTML omitted -->\n<section class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\" role=\"doc-endnote\">\n<p>Predicting how a market will move though is pretty much hopeless in the general case, as financial markets are a good example of a chaotic system in that it responds not just to events, but predictions and expectations as well, and does so in a recursive, self-predicting way: if enough people think that upon some news others will want to buy a particular stock, then the stock will move on the news. This is one of the factors that makes markets very &ldquo;noisy&rdquo;: prices move very frequently, even for large, fairly stable stocks.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</section>\n",
				"content_text": "Daring Fireball wrote about the [Apple GPT news](https://daringfireball.net/2023/07/apple_gpt_bloomberg). I think the news itself is pretty uninteresting: Apple wants in on the ChatGPT gold-rush even as it seems to be winding down. The more interesting part is what he writes about Bloomberg:\n\n> Bloomberg reporters are evaluated and receive bonuses tied to reporting market-moving news. They’re incentivized financially to make mountains out of molehills, and craters out of divots, to maximize the immediate effect of their reporting on stock prices. And Bloomberg appends these stock price movements right there in their reports, to drive home the notion that Bloomberg publishes market-moving news, so maybe you too should spend over $2,000 per month on a Bloomberg Terminal so that you can receive news reports from Bloomberg minutes before the general public, and buy, sell, and short stocks based on that news.\n> \n> ...\n> \n> Apple’s brief 2.7 percent jump and Microsoft’s smaller but still-significant drop, both at 12:04pm, were clearly caused by Gurman’s report. Bloomberg Terminal subscribers get such reports before anyone else. [...] most of their original reporting is delivered with the goal of moving the stock prices of the companies they’re reporting on, for the purpose of proving the value of a Bloomberg Terminal’s hefty subscription fee to day-trading gamblers [...]\n\nTinfoil hat time? I mean... that's quite the stretch. Even if Bloomberg doesn't report particular news, a million other websites will; how does Bloomberg move the market in particular?\nBloomberg is hardly the only company selling ahead-of-the-crowd services; it's obviously a lucrative thing to do successfully, given the herd mentality that all markets follow[^predicting-markets].\nThey obviously want to communicate news that have _the potential_ of moving stocks, otherwise they'd not be a good source of finance news!\n\nAs for making mountains out of molehills, I think that's a fair criticism to an extent, but it's also not one that is in any way unique to Bloomberg. Clickbait and blowing news out of proportion is just business as usual for mass media these days, thanks to the advertising industry financing a large chunk of websites and competition [fierce enough to trigger](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) a race to the bottom with low-effort reporting. My personal impression is that Bloomberg is actually better than many in this regard.\n\nBy the way, the Apple stock _stayed up_ after these news (see the [Investing.com chart](https://www.investing.com/equities/apple-computer-inc)), only dropping back to previous levels some 24 hours later when manufacturing issues were reported with the upcoming iPhone 15. So it was _good reporting_ by Bloomberg.\n\n[^predicting-markets]: Predicting how a market will move though is pretty much hopeless in the general case, as financial markets are a good example of a chaotic system in that it responds not just to events, but predictions and expectations as well, and does so in a recursive, self-predicting way: if enough people think that upon some news others will want to buy a particular stock, then the stock will move on the news. This is one of the factors that makes markets very \"noisy\": prices move very frequently, even for large, fairly stable stocks.\n\n<img src=\"uploads/2023/aapl-stock-news.png\" width=\"600\" height=\"330\" alt=\"\">\n",
				"date_published": "2023-07-22T01:25:19+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/22/financial-news-and.html",
				"tags": ["finance"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/15/outofmemory-while-trying.html",
				"title": "Out-of-memory while trying to free it: into virtual memory",
				"content_html": "<p>I recently came by this fun story on getting an out-of-memory error on Linux when trying to <em>free</em> memory: <a href=\"https://ayende.com/blog/199649-B/production-postmortem-enomem-when-trying-to-free-memory?Key=c9261116-c324-4280-a1e1-35da0d1fa882\">Production postmortem: ENOMEM when trying to free memory</a></p>\n<blockquote>\n<p>That error made absolutely no sense, as you can imagine. We are trying to release memory, not allocate it. Common sense says that you can’t really fail when you are freeing memory. After all, how can you run out of memory? I’m trying to give you some, damn it!</p>\n</blockquote>\n<p>The linked blog post does a good job of explaining the problem briefly and clearly, so I&rsquo;m just going to provide some further context on how this can indeed be a problem.</p>\n<p>Modern computer architectures like x86-64 and even most (all?) the ARMs have a lot funkier relationship with memory than most people &ndash; and even most software developers! &ndash; tend to be aware of. (This is a good thing! You don&rsquo;t need to know how to sausage is made. Abstraction is an invaluable tool for managing complexity.)</p>\n<p>The memory that a running application (any running application) sees and lives in (where it loads all of its data and where all of its allocations live) is actually not a direct representation of the main memory of the computer (the RAM sticks). It used to be, long ago, but we&rsquo;ve moved away from that. Instead, what your process is interacting with is called <em>virtual memory</em>, or more specifically, a <em>virtual memory address-space</em>. Virtual memory is a layer of abstraction on top of <em>physical memory</em>, a decoupling useful because it grants additional performance, stability, and safety. How?</p>\n<p>A process&rsquo;s virtual memory address-space is constructed specifically for that one particular process, and reflects <em>only</em> the parts of the physical memory that are used by it. Memory used by other processes is normally not visible here: it is simply not mapped (outside of special cases like shared memory created by <a href=\"https://man7.org/linux/man-pages/man3/shm_open.3.html\"><code>shm_open()</code></a> and friends or debugging via <a href=\"https://man7.org/linux/man-pages/man2/ptrace.2.html\"><code>ptrace()</code></a>). This means that, by default, no process can accidentally or maliciously access the memory of other processes and extract secrets from it or change the values in unexpected ways leading to erratic behaviour or crashes.</p>\n<p>This layer of abstraction also enables other useful features that reduce overall memory usage at the expense of runtime performance:</p>\n<ul>\n<li>Demand-paging: memory allocated but not yet accessed doesn&rsquo;t actually need to be available: if the application decided to allocate a gigantic 2 GB buffer but hasn&rsquo;t yet used more than 100 KB of it, then the rest doesn&rsquo;t have to be available</li>\n<li>Swapping: rarely used regions of memory are written to disk, and the memory it was used for is made available for new allocations</li>\n<li>Compression: rarely used regions of memory are compressed, and the space freed up is reclaimed for other purposes</li>\n<li>De-duplication: if there are multiple identical copies of the sequence of bytes in memory, we don&rsquo;t actually need to store it multiple times (though caveats apply!)</li>\n</ul>\n<p>For any physical memory to be accessible in a virtual memory address space, it has to be mapped in, typically by the operating system kernel. This is done by writing into a data structure called the <em>page table</em>, which is shared between the kernel and the CPU&rsquo;s memory management unit (MMU). A <em>page table entry</em> typically represents the mapping of some physical memory region into a virtual memory address space: in this case, the entry will contain the physical memory address that it corresponds to. You may have multiple page table entries refer to the same or overlapping regions of physical memory: that is totally cool and allowed! (This has some very interesting uses e.g. in cybersecurity that I wrote <a href=\"https://github.com/shdnx/dangless-malloc\">my Master&rsquo;s thesis on</a>.)</p>\n<p>A page table entry, beyond the physical memory address, also contains bits used for purposes. Some of these are used for protection, to indicate whether the region is readable, writable, or executable. This is enforced by the MMU: attempting to write to a virtual memory region whose page table entry does not have the writable bit set will fail (well, it will trigger a page fault, which can be handled &ndash; more on this later). For example, after a running application&rsquo;s executable is loaded into memory at startup, the region is marked as non-writable (but readable and executable) to prevent buggy software from accidentally modifying its own code and also to limit the options of malware. Similarly, memory regions where the application&rsquo;s runtime data is stored (so the stack, the heap, but also the .data, .bss, and .rodata sections containing global variables) are marked as non-executable, as that is a very easy attack vector for malicious users.  (They can be made executable, otherwise things like Jit-in-Time compilers would not work, but you have to explicitly request it. e.g. on Linux by using <code>mmap()</code> with <code>PROT_EXEC</code>.)</p>\n<p>The page table entry also contains a &ldquo;dirty&rdquo; bit. This is set the MMU whenever that particular memory region is used. The kernel can periodically clear this bit, and then check later if the bit is set: if not, then the corresponding memory region has not been used recently, and so may be candidate to be swapped out or compressed.</p>\n<p>We need a mechanism though for detecting when e.g. a swapped-out piece of memory is accessed. Clearly, such an access cannot simply proceed as normal: the page table entry is marked as invalid, there is no physical memory backing it. The MMU will trigger a <em>page fault</em> in this case, which is an interrupt that the kernel handles. The kernel will consult its own internal book-keeping of the process&rsquo;s memory and will find in this case that it has decided to swap the memory to disk: now, since the data is requested, the data must be loaded back from disk into main memory and the page table entry must be made valid again. Once this is done, the process resumes execution from the exact same place as it was left suspended when trying to access swapped memory, none the wiser to what happened in the background &ndash; except perhaps that some operation ending up having taken much more time than usual.</p>\n<p>This pattern is actually very common, and is used for all of the memory-use optimizations I&rsquo;ve mentioned so far. For instance, in the case of memory de-duplication, identical memory regions may be referenced by multiple processes at the same time. So long as they all only read from it, life is good, but  when one of them wants to write to it, the kernel needs to intervene, as the write would make the memory region no longer contain what the other processes expect it to. This is achieved by marking the page table entries as non-writeable, such that attempting to write results in a page fault. On such page  fault, the kernel quietly duplicates the memory region (by allocating a new one and then copying into it) and updates the writing process&rsquo;s page table entry to point to the new address, restoring its writeability before returning control to the process.</p>\n<p>You may have noticed that I was talking about individual bits of the page table entry. Indeed, these entries are extremely limited in size: typically just 64 bits (8 bytes), most of which is needed to store the referenced physical memory address. This is not always enough for more complicated features built on top of the virtual memory abstraction, so the kernel has additional book-keeping: in Linux, these are called <em>virtual memory areas</em> or VMAs for short: one for each distinct memory region of each running process. For reasons of simplicity and performance, these VMAs are stored in a large pre-allocated array with a fixed capacity.</p>\n<p>This takes us back to the blog post that motivated this write-up: freeing or de-allocating memory may require an additional VMA to be allocated in the case where you&rsquo;re not freeing an entire allocated memory region, but only a part of it. This is not possible to do via the commonly used <code>malloc()</code> / <code>free()</code> APIs or even the C++ <code>new</code> and <code>delete</code> equivalent operators, but it is possible to do via direct system calls using <code>mmap()</code> and <code>munmap()</code>. To illustrate:</p>\n<ol>\n<li>We allocate 1000 bytes of memory</li>\n<li>Linux creates a new VMA for us to record this (note: no actual memory has been allocated unless you used <code>MAP_POPULATE</code> as demand paging kicks in!)</li>\n<li>We now wish to free the middle 500 bytes</li>\n<li>Linux now has no choice but to split the original one VMA spanning 1000 bytes into two of 250 bytes each, with a 500 byte gap in the middle</li>\n</ol>\n<p>Granted, this is a bit contrived, and not even accurate as you cannot free 500 bytes: memory is split into pages of 4096 bytes (4 KB) each (by default, on x86-64). But you can imagine the same situation happening when e.g. different protection bits are applied to different parts of a huge memory region: some part of writeable, other parts are not. The VMA having to be split causes the memory deallocation operation to actually have to allocate memory: for the new VMA. This is how you can get an out-of-memory error.</p>\n<p>As you can imagine, there are a lot more details to this topic. Some teasers:</p>\n<ul>\n<li>A single virtual memory address space can span up to 256 terrabytes of memory, making virtual memory much more plentiful than physical memory.</li>\n<li>Resolving virtual memory addresses into physical memory addresses is slow even when performed by dedicated hardware: modern CPUs spend something like 25% of their total execution time just doing this.</li>\n<li>Caching (the TLB) is used to mitigate this, and while its hit-rate is very high (96%+ in typical applications), it opens up timing-related side-channel attacks that have become well-known in recent years.</li>\n<li>Huge pages of 2 MB and 1 GB are another effort to improve performance, but are a mess, and difficult to work with in practice.</li>\n</ul>\n",
				"content_text": "I recently came by this fun story on getting an out-of-memory error on Linux when trying to _free_ memory: [Production postmortem: ENOMEM when trying to free memory](https://ayende.com/blog/199649-B/production-postmortem-enomem-when-trying-to-free-memory?Key=c9261116-c324-4280-a1e1-35da0d1fa882)\r\n> That error made absolutely no sense, as you can imagine. We are trying to release memory, not allocate it. Common sense says that you can’t really fail when you are freeing memory. After all, how can you run out of memory? I’m trying to give you some, damn it!\r\n\r\nThe linked blog post does a good job of explaining the problem briefly and clearly, so I'm just going to provide some further context on how this can indeed be a problem.\r\n\r\nModern computer architectures like x86-64 and even most (all?) the ARMs have a lot funkier relationship with memory than most people -- and even most software developers! -- tend to be aware of. (This is a good thing! You don't need to know how to sausage is made. Abstraction is an invaluable tool for managing complexity.)\r\n\r\nThe memory that a running application (any running application) sees and lives in (where it loads all of its data and where all of its allocations live) is actually not a direct representation of the main memory of the computer (the RAM sticks). It used to be, long ago, but we've moved away from that. Instead, what your process is interacting with is called _virtual memory_, or more specifically, a _virtual memory address-space_. Virtual memory is a layer of abstraction on top of _physical memory_, a decoupling useful because it grants additional performance, stability, and safety. How?\r\n\r\nA process's virtual memory address-space is constructed specifically for that one particular process, and reflects _only_ the parts of the physical memory that are used by it. Memory used by other processes is normally not visible here: it is simply not mapped (outside of special cases like shared memory created by [`shm_open()`](https://man7.org/linux/man-pages/man3/shm_open.3.html) and friends or debugging via [`ptrace()`](https://man7.org/linux/man-pages/man2/ptrace.2.html)). This means that, by default, no process can accidentally or maliciously access the memory of other processes and extract secrets from it or change the values in unexpected ways leading to erratic behaviour or crashes.\r\n\r\nThis layer of abstraction also enables other useful features that reduce overall memory usage at the expense of runtime performance:\r\n\r\n- Demand-paging: memory allocated but not yet accessed doesn't actually need to be available: if the application decided to allocate a gigantic 2 GB buffer but hasn't yet used more than 100 KB of it, then the rest doesn't have to be available\r\n- Swapping: rarely used regions of memory are written to disk, and the memory it was used for is made available for new allocations\r\n- Compression: rarely used regions of memory are compressed, and the space freed up is reclaimed for other purposes\r\n- De-duplication: if there are multiple identical copies of the sequence of bytes in memory, we don't actually need to store it multiple times (though caveats apply!)\r\n\r\nFor any physical memory to be accessible in a virtual memory address space, it has to be mapped in, typically by the operating system kernel. This is done by writing into a data structure called the _page table_, which is shared between the kernel and the CPU's memory management unit (MMU). A _page table entry_ typically represents the mapping of some physical memory region into a virtual memory address space: in this case, the entry will contain the physical memory address that it corresponds to. You may have multiple page table entries refer to the same or overlapping regions of physical memory: that is totally cool and allowed! (This has some very interesting uses e.g. in cybersecurity that I wrote [my Master's thesis on](https://github.com/shdnx/dangless-malloc).)\r\n\r\nA page table entry, beyond the physical memory address, also contains bits used for purposes. Some of these are used for protection, to indicate whether the region is readable, writable, or executable. This is enforced by the MMU: attempting to write to a virtual memory region whose page table entry does not have the writable bit set will fail (well, it will trigger a page fault, which can be handled -- more on this later). For example, after a running application's executable is loaded into memory at startup, the region is marked as non-writable (but readable and executable) to prevent buggy software from accidentally modifying its own code and also to limit the options of malware. Similarly, memory regions where the application's runtime data is stored (so the stack, the heap, but also the .data, .bss, and .rodata sections containing global variables) are marked as non-executable, as that is a very easy attack vector for malicious users.  (They can be made executable, otherwise things like Jit-in-Time compilers would not work, but you have to explicitly request it. e.g. on Linux by using `mmap()` with `PROT_EXEC`.)\r\n\r\nThe page table entry also contains a \"dirty\" bit. This is set the MMU whenever that particular memory region is used. The kernel can periodically clear this bit, and then check later if the bit is set: if not, then the corresponding memory region has not been used recently, and so may be candidate to be swapped out or compressed.\r\n\r\nWe need a mechanism though for detecting when e.g. a swapped-out piece of memory is accessed. Clearly, such an access cannot simply proceed as normal: the page table entry is marked as invalid, there is no physical memory backing it. The MMU will trigger a _page fault_ in this case, which is an interrupt that the kernel handles. The kernel will consult its own internal book-keeping of the process's memory and will find in this case that it has decided to swap the memory to disk: now, since the data is requested, the data must be loaded back from disk into main memory and the page table entry must be made valid again. Once this is done, the process resumes execution from the exact same place as it was left suspended when trying to access swapped memory, none the wiser to what happened in the background -- except perhaps that some operation ending up having taken much more time than usual.\r\n\r\nThis pattern is actually very common, and is used for all of the memory-use optimizations I've mentioned so far. For instance, in the case of memory de-duplication, identical memory regions may be referenced by multiple processes at the same time. So long as they all only read from it, life is good, but  when one of them wants to write to it, the kernel needs to intervene, as the write would make the memory region no longer contain what the other processes expect it to. This is achieved by marking the page table entries as non-writeable, such that attempting to write results in a page fault. On such page  fault, the kernel quietly duplicates the memory region (by allocating a new one and then copying into it) and updates the writing process's page table entry to point to the new address, restoring its writeability before returning control to the process.\r\n\r\nYou may have noticed that I was talking about individual bits of the page table entry. Indeed, these entries are extremely limited in size: typically just 64 bits (8 bytes), most of which is needed to store the referenced physical memory address. This is not always enough for more complicated features built on top of the virtual memory abstraction, so the kernel has additional book-keeping: in Linux, these are called _virtual memory areas_ or VMAs for short: one for each distinct memory region of each running process. For reasons of simplicity and performance, these VMAs are stored in a large pre-allocated array with a fixed capacity.\r\n\r\nThis takes us back to the blog post that motivated this write-up: freeing or de-allocating memory may require an additional VMA to be allocated in the case where you're not freeing an entire allocated memory region, but only a part of it. This is not possible to do via the commonly used `malloc()` / `free()` APIs or even the C++ `new` and `delete` equivalent operators, but it is possible to do via direct system calls using `mmap()` and `munmap()`. To illustrate:\r\n\r\n1. We allocate 1000 bytes of memory\r\n2. Linux creates a new VMA for us to record this (note: no actual memory has been allocated unless you used `MAP_POPULATE` as demand paging kicks in!)\r\n3. We now wish to free the middle 500 bytes\r\n4. Linux now has no choice but to split the original one VMA spanning 1000 bytes into two of 250 bytes each, with a 500 byte gap in the middle\r\n\r\nGranted, this is a bit contrived, and not even accurate as you cannot free 500 bytes: memory is split into pages of 4096 bytes (4 KB) each (by default, on x86-64). But you can imagine the same situation happening when e.g. different protection bits are applied to different parts of a huge memory region: some part of writeable, other parts are not. The VMA having to be split causes the memory deallocation operation to actually have to allocate memory: for the new VMA. This is how you can get an out-of-memory error.\r\n\r\nAs you can imagine, there are a lot more details to this topic. Some teasers:\r\n\r\n- A single virtual memory address space can span up to 256 terrabytes of memory, making virtual memory much more plentiful than physical memory.\r\n- Resolving virtual memory addresses into physical memory addresses is slow even when performed by dedicated hardware: modern CPUs spend something like 25% of their total execution time just doing this.\r\n- Caching (the TLB) is used to mitigate this, and while its hit-rate is very high (96%+ in typical applications), it opens up timing-related side-channel attacks that have become well-known in recent years.\r\n- Huge pages of 2 MB and 1 GB are another effort to improve performance, but are a mess, and difficult to work with in practice.\n",
				"date_published": "2023-07-15T03:10:14+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/15/outofmemory-while-trying.html",
				"tags": ["software development"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/15/brought-down-by.html",
				"title": "Brought down by the font",
				"content_html": "<p>Oh wow, now <em>this</em> is something: <a href=\"https://www.theverge.com/2017/7/12/15961354/pakistan-calibri-font-scandal-forged-documents\">A Microsoft font may have exposed corruption in Pakistan</a>:</p>\n<blockquote>\n<p>The Microsoft font Calibri is now a key piece of evidence in a corruption investigation surrounding Pakistan’s prime minister. Investigators noticed that documents handed over by the prime minister’s daughter, Maryam Nawaz Sharif, were typed up in the font Calibri. But the documents were dated from 2006 — and Calibri wasn’t widely available at that point, making a good case that they were forged.</p>\n</blockquote>\n<p>I love everything about this, but mostly just the fact that there are people know and care enough about fonts of all things to be able to point things like this out. This is true digital forensics. I almost feel sorry for Pakistani Prime Minister Nawaz Sharif! Not enough though to keep me from highlight this amazing snippet of peak journalism from Arstechnica&rsquo;s slightly more detailed article <a href=\"https://arstechnica.com/tech-policy/2017/07/not-for-the-first-time-microsofts-fonts-have-caught-out-forgers/\">Not for the first time, Microsoft’s fonts have caught out forgers</a>:</p>\n<blockquote>\n<p>Ultimately, the fallout of the corruption and cover-up is that Pakistan may soon, like Calibri itself, be sans-Sharif.</p>\n</blockquote>\n<p>The article goes on to note that this hasn&rsquo;t been the first time that fonts have been the demise of the mighty:</p>\n<blockquote>\n<p>Other Word features have caught out forgers, too. The &ldquo;Killian documents,&rdquo; which claimed President George W. Bush was declared unfit for service during his time with the Air National Guard, purported to have been produced on a typewriter in 1973. However, those documents used proportional fonts and curly quotes, making it spectacularly unlikely that they were authentic. Standard 1973 vintage typewriters didn&rsquo;t offer either proportional fonts or curly quotes, but 2004-vintage Microsoft Word did both.</p>\n</blockquote>\n<p>But yeah, apparently there are indeed people that both know and care a lot about fonts, such as <a href=\"https://www.youtube.com/watch?v=cJYm-de_UHE\">this guy who researched the market share of fonts used for memes</a>. Peak Friday evening material.</p>\n<p>This whole rabbit hole was enabled by the <a href=\"https://news.ycombinator.com/item?id=36719987\">Hacker News discussion</a> on Microsoft&rsquo;s new Aptos font, aimed to replace Calibre as the default, which I only even clicked on because I thought they were referring to <a href=\"https://calibre-ebook.com/\">Calibre the ebook management software</a> that I use.</p>\n",
				"content_text": "Oh wow, now *this* is something: [A Microsoft font may have exposed corruption in Pakistan](https://www.theverge.com/2017/7/12/15961354/pakistan-calibri-font-scandal-forged-documents):\n\n> The Microsoft font Calibri is now a key piece of evidence in a corruption investigation surrounding Pakistan’s prime minister. Investigators noticed that documents handed over by the prime minister’s daughter, Maryam Nawaz Sharif, were typed up in the font Calibri. But the documents were dated from 2006 — and Calibri wasn’t widely available at that point, making a good case that they were forged.\n\nI love everything about this, but mostly just the fact that there are people know and care enough about fonts of all things to be able to point things like this out. This is true digital forensics. I almost feel sorry for Pakistani Prime Minister Nawaz Sharif! Not enough though to keep me from highlight this amazing snippet of peak journalism from Arstechnica's slightly more detailed article [Not for the first time, Microsoft’s fonts have caught out forgers](https://arstechnica.com/tech-policy/2017/07/not-for-the-first-time-microsofts-fonts-have-caught-out-forgers/):\n> Ultimately, the fallout of the corruption and cover-up is that Pakistan may soon, like Calibri itself, be sans-Sharif.\n\nThe article goes on to note that this hasn't been the first time that fonts have been the demise of the mighty:\n> Other Word features have caught out forgers, too. The \"Killian documents,\" which claimed President George W. Bush was declared unfit for service during his time with the Air National Guard, purported to have been produced on a typewriter in 1973. However, those documents used proportional fonts and curly quotes, making it spectacularly unlikely that they were authentic. Standard 1973 vintage typewriters didn't offer either proportional fonts or curly quotes, but 2004-vintage Microsoft Word did both.\n\nBut yeah, apparently there are indeed people that both know and care a lot about fonts, such as [this guy who researched the market share of fonts used for memes](https://www.youtube.com/watch?v=cJYm-de_UHE). Peak Friday evening material.\n\nThis whole rabbit hole was enabled by the [Hacker News discussion](https://news.ycombinator.com/item?id=36719987) on Microsoft's new Aptos font, aimed to replace Calibre as the default, which I only even clicked on because I thought they were referring to [Calibre the ebook management software](https://calibre-ebook.com/) that I use.\n",
				"date_published": "2023-07-15T01:11:22+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/15/brought-down-by.html",
				"tags": ["technology"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/12/writing-a-popular.html",
				"title": "Writing a popular blog must be dreadful",
				"content_html": "<p>In <a href=\"https://astralcodexten.substack.com/p/why-do-i-suck\">Why do I suck</a>, Scott Alexander is analyzing why his earlier articles (from 2013 - 2016) were much more popular than his recent ones.</p>\n<p>I would like to highlight his insight into the perception difference between a small blog and a big one:</p>\n<blockquote>\n<p>If you have a small blog, and you have a cool thought or insight, you can post your cool thought or insight. People will say “interesting, I never thought of that before” and have vaguely positive feelings about you. If you have a big blog, people will get angry. They’ll feel it’s insulting for you to have opinions about a field when there are hundreds of experts who have written thousands of books about the field which you haven’t read. Unless you cite a dozen sources, it will be “armchair speculation” and you’ll be “speaking over real academics”. If anyone has ever had the same thought before, you’re plagiarizing them, or “reinventing the wheel”, or acting like a “guru”, or claiming that all knowledge springs Athena-like from your head with no prior influences.</p>\n<p>I try really hard to block or ignore these people when I spot them, but they do a little bit of psychic damage each time.</p>\n</blockquote>\n<p>I wonder how much of that is simply bigger audience = worse audience, given <a href=\"https://en.wikipedia.org/wiki/Regression_toward_the_mean\">regression to the mean</a>. Basically: when your blog has 100 readers, it is not difficult for this 100 to be &ldquo;reasonable people&rdquo; (from you, the blog author&rsquo;s, perspective), but if you have 10 000, then it is much harder: as the size of a group grows, the more it tends to resemble the population at large. The population as a whole contains a whole lot of people who suck! (This seems universally true, regardless of what group you identify with.)</p>\n<p>Another perspective on the same idea: out of 10 000 people, you&rsquo;re much more likely to annoy / step on the toes of somebody by something you write than out of 100 people. 10 000 people will also likely contain some trolls and griefers, who just tend to make things worse.</p>\n<p>The other possible explanation is authority. A blog with few readers has little to no authority on any subject, and therefore is largely safe from the kind of scrutiny that having authority (even informal!) invites. So you can get away just expressing your opinions in a way that makes sense to you. Doing so when you have a large audience is risky, expectations are higher, and the whole thing becomes a lot more high-stakes.</p>\n<p>Taking this further, what does this mean for official authorities like the CDC or the FED? Obviously that&rsquo;s kind of as high-stakes as it gets, but beyond that, they are very constrained in what they can actually say if they want people to actually take them seriously. They can&rsquo;t just say things that make sense to them! It needs to be as unambiguous as possible, supported by research and data.</p>\n<hr>\n<p>As an aside, I find it interesting that one of his plausible explanations is just that media overall is less bad today than it used to be. Not often do you see this proposed!</p>\n<blockquote>\n<p>Nowadays I think there are many good science bloggers, and the media has gotten embarrassed enough times that it will sometimes run a take by someone who knows what they’re talking about before publishing it.</p>\n<p>In the same way, I see fewer people outright denying the existence of genetics, totally failing to understand AI risk, or utterly bungling basic concepts in risk and probability.</p>\n</blockquote>\n",
				"content_text": "In [Why do I suck](https://astralcodexten.substack.com/p/why-do-i-suck), Scott Alexander is analyzing why his earlier articles (from 2013 - 2016) were much more popular than his recent ones.\r\n\r\nI would like to highlight his insight into the perception difference between a small blog and a big one:\r\n> If you have a small blog, and you have a cool thought or insight, you can post your cool thought or insight. People will say “interesting, I never thought of that before” and have vaguely positive feelings about you. If you have a big blog, people will get angry. They’ll feel it’s insulting for you to have opinions about a field when there are hundreds of experts who have written thousands of books about the field which you haven’t read. Unless you cite a dozen sources, it will be “armchair speculation” and you’ll be “speaking over real academics”. If anyone has ever had the same thought before, you’re plagiarizing them, or “reinventing the wheel”, or acting like a “guru”, or claiming that all knowledge springs Athena-like from your head with no prior influences.\r\n> \r\n> I try really hard to block or ignore these people when I spot them, but they do a little bit of psychic damage each time.\r\n\r\nI wonder how much of that is simply bigger audience = worse audience, given [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean). Basically: when your blog has 100 readers, it is not difficult for this 100 to be \"reasonable people\" (from you, the blog author's, perspective), but if you have 10 000, then it is much harder: as the size of a group grows, the more it tends to resemble the population at large. The population as a whole contains a whole lot of people who suck! (This seems universally true, regardless of what group you identify with.)\r\n\r\nAnother perspective on the same idea: out of 10 000 people, you're much more likely to annoy / step on the toes of somebody by something you write than out of 100 people. 10 000 people will also likely contain some trolls and griefers, who just tend to make things worse.\r\n\r\nThe other possible explanation is authority. A blog with few readers has little to no authority on any subject, and therefore is largely safe from the kind of scrutiny that having authority (even informal!) invites. So you can get away just expressing your opinions in a way that makes sense to you. Doing so when you have a large audience is risky, expectations are higher, and the whole thing becomes a lot more high-stakes.\r\n\r\nTaking this further, what does this mean for official authorities like the CDC or the FED? Obviously that's kind of as high-stakes as it gets, but beyond that, they are very constrained in what they can actually say if they want people to actually take them seriously. They can't just say things that make sense to them! It needs to be as unambiguous as possible, supported by research and data.\r\n\r\n----\r\n\r\nAs an aside, I find it interesting that one of his plausible explanations is just that media overall is less bad today than it used to be. Not often do you see this proposed!\r\n> Nowadays I think there are many good science bloggers, and the media has gotten embarrassed enough times that it will sometimes run a take by someone who knows what they’re talking about before publishing it.\r\n> \r\n> In the same way, I see fewer people outright denying the existence of genetics, totally failing to understand AI risk, or utterly bungling basic concepts in risk and probability.\n",
				"date_published": "2023-07-12T01:38:49+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/12/writing-a-popular.html"
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/08/regulation-unexpected-consequences.html",
				"title": "Regulation, unexpected consequences of (Sesame edition)",
				"content_html": "<p>Matt Levine writes the amazing <a href=\"https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine\">Money Stuff</a> newsletter for Bloomberg. I&rsquo;ve been a reader of his for a few years now, and I&rsquo;m a huge fan of his writing. He recently wrote about the <a href=\"https://www.bloomberg.com/opinion/articles/2023-06-21/big-firms-want-normal-crypto-markets\">new US legislation on sesame seeds</a> (search for the title &ldquo;Sesame&rdquo;) that I found hilarious:</p>\n<blockquote>\n<p>Congress passed legislation intended to make life better for people allergic to sesame seeds. Instead, it made things worse.\nThe bill, passed with overwhelming bipartisan support and signed into law by President Biden in 2021, requires manufacturers to label sesame on their products starting this year.</p>\n<p>In response, some companies began adding sesame to products that hadn’t included it in the past—saying it was safer to add sesame and label it, rather than certify they had eliminated all traces of it.</p>\n</blockquote>\n<p>Why would they do that? Clearly, if anything, that&rsquo;s the opposite of what the new regulation was intended for. Well:</p>\n<blockquote>\n<p>The issue is that it is hard to eliminate trace amounts of sesame, and the law now requires food manufacturers to label sesame as an allergen. Not putting sesame on the label effectively constitutes a promise that there is no sesame in the product, and if there is a little bit then you get in trouble: [&hellip;]</p>\n<p>But if you say that the product definitely contains sesame, then you are immunized from trouble. So you just chuck some sesame into everything, change the labels, and you’re fine. It is easier to make sure that there is sesame than that there isn’t, so that’s what companies do.</p>\n</blockquote>\n<p>Unintended consequences! It <em>makes sense</em> that companies do this, given their incentives. A friend of mine put it as &ldquo;regulators just do not think like hackers&rdquo;, which is not how I&rsquo;d put it, but I do see his point: regulators should have known better than to do this. Don&rsquo;t they consult with experts and people in the industry? Someone would have definitely pointed this out to them.</p>\n",
				"content_text": "Matt Levine writes the amazing [Money Stuff](https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine) newsletter for Bloomberg. I've been a reader of his for a few years now, and I'm a huge fan of his writing. He recently wrote about the [new US legislation on sesame seeds](https://www.bloomberg.com/opinion/articles/2023-06-21/big-firms-want-normal-crypto-markets) (search for the title \"Sesame\") that I found hilarious:\r\n\r\n> Congress passed legislation intended to make life better for people allergic to sesame seeds. Instead, it made things worse.\r\n> The bill, passed with overwhelming bipartisan support and signed into law by President Biden in 2021, requires manufacturers to label sesame on their products starting this year.\r\n> \r\n> In response, some companies began adding sesame to products that hadn’t included it in the past—saying it was safer to add sesame and label it, rather than certify they had eliminated all traces of it.\r\n\r\nWhy would they do that? Clearly, if anything, that's the opposite of what the new regulation was intended for. Well:\r\n\r\n> The issue is that it is hard to eliminate trace amounts of sesame, and the law now requires food manufacturers to label sesame as an allergen. Not putting sesame on the label effectively constitutes a promise that there is no sesame in the product, and if there is a little bit then you get in trouble: [...]\r\n> \r\n> But if you say that the product definitely contains sesame, then you are immunized from trouble. So you just chuck some sesame into everything, change the labels, and you’re fine. It is easier to make sure that there is sesame than that there isn’t, so that’s what companies do.\r\n\r\nUnintended consequences! It _makes sense_ that companies do this, given their incentives. A friend of mine put it as \"regulators just do not think like hackers\", which is not how I'd put it, but I do see his point: regulators should have known better than to do this. Don't they consult with experts and people in the industry? Someone would have definitely pointed this out to them.\n",
				"date_published": "2023-07-08T23:43:54+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/08/regulation-unexpected-consequences.html"
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/06/the-twitterkiller-metas.html",
				"title": "The Twitter-killer? Meta's new Threads app",
				"content_html": "<p>Facebook (err, sorry, Meta) is launching their Twitter competitor today in the US in the form of a new stand-alone app called <a href=\"https://apps.apple.com/us/app/threads-an-instagram-app/id6446901002\">Threads</a>.</p>\n<p>The Verge has a good summary on what is known: <a href=\"https://www.theverge.com/2023/7/5/23784480/threads-instagram-meta-news-twitter-competitor\">Instagram’s Threads: all the updates on the new Twitter competitor</a>. I can also recommend the <a href=\"https://news.ycombinator.com/item?id=36580192\">Hacker News thread</a>; it has received quite a bit of attention and drew a lot of discussion.</p>\n<blockquote>\n<p>As Twitter continues to flail about under Elon Musk, all eyes are on Instagram Threads as a potential replacement.</p>\n</blockquote>\n<p>Twitter&rsquo;s story with getting bought by Elon Musk is truly wild, and semi-seriously could be turned into a movie. Matt Levine wrote a lot about it in his excellent Money Stuff, and it was all great fun, truly peak 21st century amusement. Since the acquisition, Twitter has arguably received more attention than before, but also a lot more criticism, and is <a href=\"https://www.dexerto.com/tech/twitter-users-claim-elon-musks-recent-changes-may-be-self-ddosing-the-platform-2196413/\">clearly struggling</a>, <a href=\"https://daringfireball.net/linked/2023/07/03/everything-continues-to-be-going-just-great-at-twitter\">dear lord</a>, with even <a href=\"https://mashable.com/article/twitter-api-elon-musk-developer-issues-apps\">high-paying customers being ignored</a>.</p>\n<p>Elon Musk himself has stated multiple times that bankruptcy in the near future was possible. It remains to be seen whether these are the kind of temporary issues that can be expected from a fairly dramatic shift in leadership and strategy, or perhaps the beginning of the end of Twitter&rsquo;s prevalence.\nEither way, this is excellent timing from Meta, and I expect they have a good chance of capitalizing on Twitter&rsquo;s plight.</p>\n<blockquote>\n<p>From what we know so far, Threads is “Instagram’s text-based conversation app” where “communities come together to discuss everything from the topics you care about today to what’ll be trending tomorrow.” The app is closely tied to Instagram, meaning you’ll get to use the same username across both apps, as well as quickly follow all of the accounts you’ve been following on Instagram.</p>\n</blockquote>\n<p>Sounds a lot like Twitter, i.e. a more social version of micro-blogging. Reportedly it will even have two-way <a href=\"https://joinmastodon.org/\">Mastodon</a> integration, which is interesting and again a good move: Mastodon gained some popularity following Twitter&rsquo;s, uh, adventures, and by embracing it, Threads can attempt to woo Twitter refugees more effectively than by <em>not</em> integrating&hellip; not to mention, it&rsquo;s a greater blow to Twitter.</p>\n<blockquote>\n<p>The Twitter alternative from Meta appears set to launch on July 6th, but the Irish Independent reports that Ireland’s Data Protection Commission has been in contact with the company about the new product and confirmed the launch won’t extend to the EU “at this point.”</p>\n</blockquote>\n<p>GDPR strikes again! I will be curious to see what happens here; no doubt there&rsquo;s an element of power play here on the part of Meta. GDPR and other privacy legislation in the EU are clearly anti-Meta; Meta is probably evaluating whether it makes sense to even bother with it.</p>\n<hr>\n<p>An interesting perspective to take is that Threads appears to be very much like Facebook&rsquo;s own Wall, where people can write posts, upload photos, and engage with people in the comments. Facebook&rsquo;s own shift towards providing something more like a news feed has reduced the Wall&rsquo;s relevance a lot, but the Threads can potentially be its reincarnation in a purer form.</p>\n<p>Threads is cleverly branded under Instagram rather than Facebook or Meta to leverage its better reputation. Meta knows that as far as associations go, people (at least in much of the US and Europe) tend to like and trust Instagram, but tend to at least distrust Facebook or Meta. Many, many people still aren&rsquo;t really aware that Instagram, and even WhatsApp are owned and operated by Meta now! They&rsquo;d very much like to keep it this way.</p>\n<p>Branding- and image-wise, Meta is in an interesting situation. On the one hand, both in the United States and much of Europe, Facebook is generally a disliked and mistrusted brand, following lots and lots of bad publicity from, well, being Facebook and doing Facebook things. On the other hand, they still have a metric ton of daily active users, passing <a href=\"https://www.statista.com/statistics/346167/facebook-global-dau/\">2 billion recently</a>, so clearly they are, uh, successful. Sadly.</p>\n",
				"content_text": "Facebook (err, sorry, Meta) is launching their Twitter competitor today in the US in the form of a new stand-alone app called [Threads](https://apps.apple.com/us/app/threads-an-instagram-app/id6446901002).\n\nThe Verge has a good summary on what is known: [Instagram’s Threads: all the updates on the new Twitter competitor](https://www.theverge.com/2023/7/5/23784480/threads-instagram-meta-news-twitter-competitor). I can also recommend the [Hacker News thread](https://news.ycombinator.com/item?id=36580192); it has received quite a bit of attention and drew a lot of discussion.\n\n> As Twitter continues to flail about under Elon Musk, all eyes are on Instagram Threads as a potential replacement.\n\nTwitter's story with getting bought by Elon Musk is truly wild, and semi-seriously could be turned into a movie. Matt Levine wrote a lot about it in his excellent Money Stuff, and it was all great fun, truly peak 21st century amusement. Since the acquisition, Twitter has arguably received more attention than before, but also a lot more criticism, and is [clearly struggling](https://www.dexerto.com/tech/twitter-users-claim-elon-musks-recent-changes-may-be-self-ddosing-the-platform-2196413/), [dear lord](https://daringfireball.net/linked/2023/07/03/everything-continues-to-be-going-just-great-at-twitter), with even [high-paying customers being ignored](https://mashable.com/article/twitter-api-elon-musk-developer-issues-apps).\n\nElon Musk himself has stated multiple times that bankruptcy in the near future was possible. It remains to be seen whether these are the kind of temporary issues that can be expected from a fairly dramatic shift in leadership and strategy, or perhaps the beginning of the end of Twitter's prevalence.\nEither way, this is excellent timing from Meta, and I expect they have a good chance of capitalizing on Twitter's plight.\n\n> From what we know so far, Threads is “Instagram’s text-based conversation app” where “communities come together to discuss everything from the topics you care about today to what’ll be trending tomorrow.” The app is closely tied to Instagram, meaning you’ll get to use the same username across both apps, as well as quickly follow all of the accounts you’ve been following on Instagram. \n\nSounds a lot like Twitter, i.e. a more social version of micro-blogging. Reportedly it will even have two-way [Mastodon](https://joinmastodon.org/) integration, which is interesting and again a good move: Mastodon gained some popularity following Twitter's, uh, adventures, and by embracing it, Threads can attempt to woo Twitter refugees more effectively than by *not* integrating... not to mention, it's a greater blow to Twitter.\n\n> The Twitter alternative from Meta appears set to launch on July 6th, but the Irish Independent reports that Ireland’s Data Protection Commission has been in contact with the company about the new product and confirmed the launch won’t extend to the EU “at this point.”\n\nGDPR strikes again! I will be curious to see what happens here; no doubt there's an element of power play here on the part of Meta. GDPR and other privacy legislation in the EU are clearly anti-Meta; Meta is probably evaluating whether it makes sense to even bother with it.\n\n-----\n\nAn interesting perspective to take is that Threads appears to be very much like Facebook's own Wall, where people can write posts, upload photos, and engage with people in the comments. Facebook's own shift towards providing something more like a news feed has reduced the Wall's relevance a lot, but the Threads can potentially be its reincarnation in a purer form.\n\nThreads is cleverly branded under Instagram rather than Facebook or Meta to leverage its better reputation. Meta knows that as far as associations go, people (at least in much of the US and Europe) tend to like and trust Instagram, but tend to at least distrust Facebook or Meta. Many, many people still aren't really aware that Instagram, and even WhatsApp are owned and operated by Meta now! They'd very much like to keep it this way.\n\nBranding- and image-wise, Meta is in an interesting situation. On the one hand, both in the United States and much of Europe, Facebook is generally a disliked and mistrusted brand, following lots and lots of bad publicity from, well, being Facebook and doing Facebook things. On the other hand, they still have a metric ton of daily active users, passing [2 billion recently](https://www.statista.com/statistics/346167/facebook-global-dau/), so clearly they are, uh, successful. Sadly.\n",
				"date_published": "2023-07-06T00:29:43+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/06/the-twitterkiller-metas.html",
				"tags": ["technology"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/04/quantum-immortality-explored.html",
				"title": "Quantum immortality",
				"content_html": "<p>Quantum immortality is a thought experiment that runs roughly like follows:</p>\n<ol>\n<li>If the many-worlds interpretation of quantum mechanics is correct, each &ldquo;choice&rdquo; (however this may be defined) creates a branching point: there will be a universe that explores each possibility. Simply put, if at an intersection you can choose to go left or right, in one universe you&rsquo;ll always go left, and in another you&rsquo;ll always go right, and the history of each universe will then evolve independently, and continue branching along each subsequent choice.</li>\n<li>Which one of these many many universes will <em>you</em> (the awareness reading this text) experience? You are clearly in one of them: your awareness does not span universes. (At least, I would expect so. My apologies if it does! Please drop me an e-mail, I&rsquo;ll buy you a coffee.)</li>\n<li>A person, pretty much by definition, is incapable of experiencing their own death. So you are clearly experiencing a universe in which you are not dead. (Again, my honest apologies if you are! Please drop me an e-mail, let&rsquo;s chat.)</li>\n<li>What if this rule is universal? What if your awareness will always continue along a path of these possible universes where you remain alive (or perhaps just aware), however unlikely that may be?</li>\n</ol>\n<p><strong>Disclaimer</strong>: <em>This is purely a thought experiment. Do not take this seriously. As far as anyone can tell, you only have one life: do not throw it away.</em></p>\n<p>This idea has some interesting consequences! The following is a short story by Robert Charles Wilson, originally published in 1998 (!), that explores this:</p>\n<h3 id=\"divided-by-infinityhttpswwwtorcom20100805divided-by-infinity\"><a href=\"https://www.tor.com/2010/08/05/divided-by-infinity/\">Divided by Infinity</a></h3>\n<hr>\n<p>As I said, interesting idea! It would imply that you get away with any crazy shit! For starters:</p>\n<ol>\n<li>Buy a lottery ticket.</li>\n<li>Set up some mechanism that will kill you immediately and without warning, should it turn out that you have <strong>not</strong> won the lottery.</li>\n<li>Profit??</li>\n</ol>\n<p>Obviously this is not fool-proof, and, err, certainly not lifestyle advice! This would just be testing the likelihood of whatever mechanism you have set up failing against the likelihood of you winning the lottery. Examples of how this may come to be include but are not limited to:</p>\n<ul>\n<li>If you hire somebody to kill you, they will have an accident or something else that prevents them from completing their task.</li>\n<li>If you set up some kind of machine or contraption, it will fail. The more fail-safe you make it, it will start failing in increasingly less plausible ways.</li>\n<li>The lottery gets cancelled.</li>\n<li>You win the lottery, but after the fact, your ticket turns out to be invalid or counterfeit.</li>\n</ul>\n<p>Another interesting consequence of this thought experiment is that it only confers &ldquo;immortality&rdquo; to <em>your own subjective awareness</em> or viewpoint. It does not protect others! So if you watch somebody else try e.g. the above, the result will most likely be the predictable one: they die. In <em>your</em> perspective that is, or rather, in the universe that you are experiencing. In <em>their</em> subjective perspective, they will survive; it will just not be a universe that you can ever experience or interact with.</p>\n<p>Finally, we have been talking about <em>immortality</em>, but the premise does not actually promise that. It only promises a <strong>continuity of awareness</strong>. This does not protect against you suffering a horrible accident that will leave you alive and aware, but miserable! The possibilities start with the run-of-the-mill horribleness that comes from inhabiting a physical body (e.g. going blind or becoming totally paralyzed), but actually go much further: you could find yourself at the mercy of aliens experimenting on you, or your consciousness (and awareness) being uploaded into a computer and then being subjected to&hellip; whatever.</p>\n<p>I will leave this here before it gets any darker. :)</p>\n<p>To end this post on a lighter note, here is an only tangentially related but fun short story video about a guy using a One-minute Time Machine to try to pick up a woman. Enjoy:</p>\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/CXhnPLMIET0' frameborder='0' allowfullscreen></iframe></div>\n<p>(Thank you Melvin for the recommendation!)</p>\n",
				"content_text": "Quantum immortality is a thought experiment that runs roughly like follows:\n\n1. If the many-worlds interpretation of quantum mechanics is correct, each \"choice\" (however this may be defined) creates a branching point: there will be a universe that explores each possibility. Simply put, if at an intersection you can choose to go left or right, in one universe you'll always go left, and in another you'll always go right, and the history of each universe will then evolve independently, and continue branching along each subsequent choice.\n2. Which one of these many many universes will _you_ (the awareness reading this text) experience? You are clearly in one of them: your awareness does not span universes. (At least, I would expect so. My apologies if it does! Please drop me an e-mail, I'll buy you a coffee.)\n3. A person, pretty much by definition, is incapable of experiencing their own death. So you are clearly experiencing a universe in which you are not dead. (Again, my honest apologies if you are! Please drop me an e-mail, let's chat.)\n4. What if this rule is universal? What if your awareness will always continue along a path of these possible universes where you remain alive (or perhaps just aware), however unlikely that may be?\n\n**Disclaimer**: *This is purely a thought experiment. Do not take this seriously. As far as anyone can tell, you only have one life: do not throw it away.*\n\nThis idea has some interesting consequences! The following is a short story by Robert Charles Wilson, originally published in 1998 (!), that explores this:\n### [Divided by Infinity](https://www.tor.com/2010/08/05/divided-by-infinity/)\n\n---------\n\nAs I said, interesting idea! It would imply that you get away with any crazy shit! For starters:\n\n1. Buy a lottery ticket.\n2. Set up some mechanism that will kill you immediately and without warning, should it turn out that you have **not** won the lottery.\n3. Profit??\n\nObviously this is not fool-proof, and, err, certainly not lifestyle advice! This would just be testing the likelihood of whatever mechanism you have set up failing against the likelihood of you winning the lottery. Examples of how this may come to be include but are not limited to:\n\n- If you hire somebody to kill you, they will have an accident or something else that prevents them from completing their task.\n- If you set up some kind of machine or contraption, it will fail. The more fail-safe you make it, it will start failing in increasingly less plausible ways.\n- The lottery gets cancelled.\n- You win the lottery, but after the fact, your ticket turns out to be invalid or counterfeit.\n\nAnother interesting consequence of this thought experiment is that it only confers \"immortality\" to _your own subjective awareness_ or viewpoint. It does not protect others! So if you watch somebody else try e.g. the above, the result will most likely be the predictable one: they die. In _your_ perspective that is, or rather, in the universe that you are experiencing. In _their_ subjective perspective, they will survive; it will just not be a universe that you can ever experience or interact with.\n\nFinally, we have been talking about _immortality_, but the premise does not actually promise that. It only promises a **continuity of awareness**. This does not protect against you suffering a horrible accident that will leave you alive and aware, but miserable! The possibilities start with the run-of-the-mill horribleness that comes from inhabiting a physical body (e.g. going blind or becoming totally paralyzed), but actually go much further: you could find yourself at the mercy of aliens experimenting on you, or your consciousness (and awareness) being uploaded into a computer and then being subjected to... whatever.\n\nI will leave this here before it gets any darker. :)\n\nTo end this post on a lighter note, here is an only tangentially related but fun short story video about a guy using a One-minute Time Machine to try to pick up a woman. Enjoy:\n\n{{<youtube CXhnPLMIET0>}}\n\n(Thank you Melvin for the recommendation!)\n",
				"date_published": "2023-07-04T19:49:34+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/04/quantum-immortality-explored.html",
				"tags": ["thoughtexperiments and ideas"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/07/01/light-and-fun.html",
				"title": "Light and fun fiction: Idol Words",
				"content_html": "<p>At this point my blog has been alive for a few days now, and so it&rsquo;s high time I mentioned Scott Alexander, and his blog(s): the Star Slate Codex and Astral Codex Ten. He writes about complex topics in a refreshingly sober, straight-forward and illuminating manner. His articles will most definitely make a regular appearance here.</p>\n<p>To start with, I would recommend a light and short piece of fiction he wrote: Idol Words. It&rsquo;s more a series of short scenes rather than a short story.\nIt semi-seriously explores the idea of three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly, with each &ldquo;scene&rdquo; featuring another person coming along to ask them a question. What kind of people would ask what kind of questions? The themes are perhaps predictable, but no less interesting or satisfying to read.</p>\n<p>Enjoy:</p>\n<h1 id=\"idol-wordshttpsastralcodextensubstackcompidol-words\"><a href=\"https://astralcodexten.substack.com/p/idol-words\">Idol Words</a></h1>\n<blockquote>\n<p>It was another boring day as the keeper of the three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly.</p>\n</blockquote>\n",
				"content_text": "At this point my blog has been alive for a few days now, and so it's high time I mentioned Scott Alexander, and his blog(s): the Star Slate Codex and Astral Codex Ten. He writes about complex topics in a refreshingly sober, straight-forward and illuminating manner. His articles will most definitely make a regular appearance here.\n\nTo start with, I would recommend a light and short piece of fiction he wrote: Idol Words. It's more a series of short scenes rather than a short story.\nIt semi-seriously explores the idea of three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly, with each \"scene\" featuring another person coming along to ask them a question. What kind of people would ask what kind of questions? The themes are perhaps predictable, but no less interesting or satisfying to read.\n\nEnjoy:\n\n# [Idol Words](https://astralcodexten.substack.com/p/idol-words)\n> It was another boring day as the keeper of the three omniscient idols, one of which always tells the truth, one of which always lies, and one of which answers randomly.\n",
				"date_published": "2023-07-01T18:18:28+02:00",
				"url": "https://blog.gaborkozar.me/2023/07/01/light-and-fun.html",
				"tags": ["thoughtexperiments and ideas"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/06/30/im-very-excited.html",
				"title": "The Three Body Problem",
				"content_html": "<p>I&rsquo;m very excited for the upcoming The Three Body Problem series!\nI&rsquo;ve read and thoroughly enjoyed <a href=\"https://www.goodreads.com/book/show/20518872-the-three-body-problem\">the book trilogy</a>, and it looks like they are sticking decently close to the source material. This must be a challenging story to put on screen!</p>\n<p>The book&rsquo;s cover text:</p>\n<blockquote>\n<p>Set against the backdrop of China&rsquo;s Cultural Revolution, a secret military project sends signals into space to establish contact with aliens. An alien civilization on the brink of destruction captures the signal and plans to invade Earth. Meanwhile, on Earth, different camps start forming, planning to either welcome the superior beings and help them take over a world seen as corrupt, or to fight against the invasion.</p>\n</blockquote>\n<p>You can watch the teaser trailer for the series on YouTube:\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/5lj99Uz1d50' frameborder='0' allowfullscreen></iframe></div></p>\n<p>The books form a trilogy, so the series will have plenty of material to work with!\nMy favorite book was the second, titled <a href=\"https://www.goodreads.com/book/show/23168817-the-dark-forest\">The Dark Forest</a>, with a host of exciting new ideas, including a to me previously unknown theoretical explanation for the <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\">Fermi Paradox</a>.</p>\n<p>As an aside, this short video gives a brief explanation on the title-giving N-body Problem: that is, we can model the behaviour of a gravitational system with 2 objects, but we are incapable of doing so for 3 or more objects. That&rsquo;s kind of a problem, especially if you happen to live in a solar system with, let&rsquo;s say, 3 stars&hellip; :)\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class='embed-container'><iframe src='https://www.youtube.com/embed/D89ngRr4uZg' frameborder='0' allowfullscreen></iframe></div></p>\n",
				"content_text": "I'm very excited for the upcoming The Three Body Problem series!\nI've read and thoroughly enjoyed [the book trilogy](https://www.goodreads.com/book/show/20518872-the-three-body-problem), and it looks like they are sticking decently close to the source material. This must be a challenging story to put on screen!\n\nThe book's cover text:\n> Set against the backdrop of China's Cultural Revolution, a secret military project sends signals into space to establish contact with aliens. An alien civilization on the brink of destruction captures the signal and plans to invade Earth. Meanwhile, on Earth, different camps start forming, planning to either welcome the superior beings and help them take over a world seen as corrupt, or to fight against the invasion.\n\nYou can watch the teaser trailer for the series on YouTube:\n{{<youtube 5lj99Uz1d50>}}\n\nThe books form a trilogy, so the series will have plenty of material to work with!\nMy favorite book was the second, titled [The Dark Forest](https://www.goodreads.com/book/show/23168817-the-dark-forest), with a host of exciting new ideas, including a to me previously unknown theoretical explanation for the [Fermi Paradox](https://en.wikipedia.org/wiki/Fermi_paradox).\n\nAs an aside, this short video gives a brief explanation on the title-giving N-body Problem: that is, we can model the behaviour of a gravitational system with 2 objects, but we are incapable of doing so for 3 or more objects. That's kind of a problem, especially if you happen to live in a solar system with, let's say, 3 stars... :)\n{{<youtube D89ngRr4uZg>}}\n",
				"date_published": "2023-06-30T00:44:10+02:00",
				"url": "https://blog.gaborkozar.me/2023/06/30/im-very-excited.html",
				"tags": ["movies series and books"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/06/30/inside-the-ai.html",
				"title": "Inside the AI factory: the human element of AIs",
				"content_html": "<p>A fascinating deep-dive from New York Magazine and The Verge into how AI models &ndash; including ChatGPT, Bard, and other LLMs (Large Language Models) &ndash; are trained, and the industry that provides the data, control, and feedback:</p>\n<h2 id=\"inside-the-ai-factoryhttpsnymagcomintelligencerarticleai-artificial-intelligence-humans-technology-business-factoryhtml\"><a href=\"https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html\">Inside the AI factory</a></h2>\n<blockquote>\n<p>[&hellip;] ChatGPT seems so human because it was trained by an AI that was mimicking humans who were rating an AI that was mimicking humans who were pretending to be a better version of an AI that was trained on human writing.</p>\n<p>This circuitous technique is called “reinforcement learning from human feedback,” or RLHF, and it’s so effective that it’s worth pausing to fully register what it doesn’t do. When annotators teach a model to be accurate, for example, the model isn’t learning to check answers against logic or external sources or about what accuracy as a concept even is. The model is still a text-prediction machine mimicking patterns in human writing, but now its training corpus has been supplemented with bespoke examples, and the model has been weighted to favor them. Maybe this results in the model extracting patterns from the part of its linguistic map labeled as accurate and producing text that happens to align with the truth, but it can also result in it mimicking the confident style and expert jargon of the accurate text while writing things that are totally wrong. There is no guarantee that the text the labelers marked as accurate is in fact accurate, and when it is, there is no guarantee that the model learns the right patterns from it.</p>\n</blockquote>\n<p>Makes sense, and it&rsquo;s so very important to recognize this! We, humans, like to anthropomorphize things, and doubly so for LLMs since they can come across so&hellip; genuinely human.</p>\n<p>Generating training data for AIs though appears&hellip; distinctly not-so-fun, mainly because it is so alien. Indeed, you are training an <em>excessively dumb</em> machine:</p>\n<blockquote>\n<p>It is in part a product of the way machine-learning systems learn. Where a human would get the concept of “shirt” with a few examples, machine-learning programs need thousands, and they need to be categorized with perfect consistency yet varied enough (polo shirts, shirts being worn outdoors, shirts hanging on a rack) that the very literal system can handle the diversity of the real world.\n[&hellip;]\nInstruction writers must come up with rules that will get humans to categorize the world with perfect consistency. To do so, they often create categories no human would use. A human asked to tag all the shirts in a photo probably wouldn’t tag the reflection of a shirt in a mirror because they would know it is a reflection and not real. But to the AI, which has no understanding of the world, it’s all just pixels and the two are perfectly identical. Fed a dataset with some shirts labeled and other (reflected) shirts unlabeled, the model won’t work. So the engineer goes back to the vendor with an update: DO label reflections of shirts. Soon, you have a 43-page guide descending into red all-caps.</p>\n</blockquote>\n<p>I fear though that before this era is over, most of us will have a &ldquo;job&rdquo; like this one, if indeed any at all.</p>\n",
				"content_text": "A fascinating deep-dive from New York Magazine and The Verge into how AI models -- including ChatGPT, Bard, and other LLMs (Large Language Models) -- are trained, and the industry that provides the data, control, and feedback:\n\n## [Inside the AI factory](https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html)\n> [...] ChatGPT seems so human because it was trained by an AI that was mimicking humans who were rating an AI that was mimicking humans who were pretending to be a better version of an AI that was trained on human writing.\n>\n> This circuitous technique is called “reinforcement learning from human feedback,” or RLHF, and it’s so effective that it’s worth pausing to fully register what it doesn’t do. When annotators teach a model to be accurate, for example, the model isn’t learning to check answers against logic or external sources or about what accuracy as a concept even is. The model is still a text-prediction machine mimicking patterns in human writing, but now its training corpus has been supplemented with bespoke examples, and the model has been weighted to favor them. Maybe this results in the model extracting patterns from the part of its linguistic map labeled as accurate and producing text that happens to align with the truth, but it can also result in it mimicking the confident style and expert jargon of the accurate text while writing things that are totally wrong. There is no guarantee that the text the labelers marked as accurate is in fact accurate, and when it is, there is no guarantee that the model learns the right patterns from it.\n\nMakes sense, and it's so very important to recognize this! We, humans, like to anthropomorphize things, and doubly so for LLMs since they can come across so... genuinely human.\n\nGenerating training data for AIs though appears... distinctly not-so-fun, mainly because it is so alien. Indeed, you are training an _excessively dumb_ machine:\n\n> It is in part a product of the way machine-learning systems learn. Where a human would get the concept of “shirt” with a few examples, machine-learning programs need thousands, and they need to be categorized with perfect consistency yet varied enough (polo shirts, shirts being worn outdoors, shirts hanging on a rack) that the very literal system can handle the diversity of the real world.\n> [...]\n> Instruction writers must come up with rules that will get humans to categorize the world with perfect consistency. To do so, they often create categories no human would use. A human asked to tag all the shirts in a photo probably wouldn’t tag the reflection of a shirt in a mirror because they would know it is a reflection and not real. But to the AI, which has no understanding of the world, it’s all just pixels and the two are perfectly identical. Fed a dataset with some shirts labeled and other (reflected) shirts unlabeled, the model won’t work. So the engineer goes back to the vendor with an update: DO label reflections of shirts. Soon, you have a 43-page guide descending into red all-caps.\n\nI fear though that before this era is over, most of us will have a \"job\" like this one, if indeed any at all.\n",
				"date_published": "2023-06-30T00:37:48+02:00",
				"url": "https://blog.gaborkozar.me/2023/06/30/inside-the-ai.html",
				"tags": ["technology","AI"]
			},
			{
				"id": "http://shdnx.micro.blog/2023/06/29/tech-erosion-the.html",
				"title": "Tech Erosion: the effect of technology (and AI) on our lives",
				"content_html": "<p>StillDrinking has a lot of great essays. He has a way with words that makes reading his articles both entertaining and illuminating at the same time &ndash; a powerful combination.</p>\n<p>Anyway, their essay on AI and ChatGPT, and how the world we have built will continue manages to be human-hostile in many ways in spite of (and indeed, in part <strong>because of</strong>) technology:</p>\n<h2 id=\"tech-erosionhttpswwwstilldrinkingorgtech-erosion\"><a href=\"https://www.stilldrinking.org/tech-erosion\">Tech Erosion</a></h2>\n<blockquote>\n<p>People are already losing their jobs. It’s not only the artists, whom nobody cares about until they’re gone, it’s copyeditors and clerks and designers. And just like self-checkouts and airport entry surveys, the humans are replaced by something a little bit worse. But it’s cheaper, and novelty often obscures indignity long enough for it to entrench, and we all accept that everything is a little bit slower, a little bit less trustworthy, and everything has a little more friction to grind us down over each day. The replacement bots could be honed into better tools, but who will bother once they’re accepted? Market trends always converge on giving us as little as possible.</p>\n</blockquote>\n<p>I&rsquo;d also be amiss if I didn&rsquo;t highlight this crucial insight into our collective fears of AIs:</p>\n<blockquote>\n<p>The terror of building a super artificial intelligence is not due to having something super intelligent hanging around, it is the terror of having something super intelligent that acts like a human. Because if we manage to build something technologically superior to us that also acts like us, it will do what technologically superior humans always do to their neighbors.</p>\n</blockquote>\n<p>Perhaps there are things that we, humanity as a whole, should just decide that we should not build &ndash; not because we cannot or because it would be evil, but because we are not sure it&rsquo;s a net positive as a whole. Of course, if we <em>were</em> able to choose so, we probably would have less to be concerned about already, given that we&rsquo;d clearly have a mechanism for informed, preventive, and collective action.</p>\n",
				"content_text": "StillDrinking has a lot of great essays. He has a way with words that makes reading his articles both entertaining and illuminating at the same time -- a powerful combination.\n\nAnyway, their essay on AI and ChatGPT, and how the world we have built will continue manages to be human-hostile in many ways in spite of (and indeed, in part **because of**) technology:\n\n## [Tech Erosion](https://www.stilldrinking.org/tech-erosion)\n\n> People are already losing their jobs. It’s not only the artists, whom nobody cares about until they’re gone, it’s copyeditors and clerks and designers. And just like self-checkouts and airport entry surveys, the humans are replaced by something a little bit worse. But it’s cheaper, and novelty often obscures indignity long enough for it to entrench, and we all accept that everything is a little bit slower, a little bit less trustworthy, and everything has a little more friction to grind us down over each day. The replacement bots could be honed into better tools, but who will bother once they’re accepted? Market trends always converge on giving us as little as possible.\n\nI'd also be amiss if I didn't highlight this crucial insight into our collective fears of AIs:\n> The terror of building a super artificial intelligence is not due to having something super intelligent hanging around, it is the terror of having something super intelligent that acts like a human. Because if we manage to build something technologically superior to us that also acts like us, it will do what technologically superior humans always do to their neighbors.\n\nPerhaps there are things that we, humanity as a whole, should just decide that we should not build -- not because we cannot or because it would be evil, but because we are not sure it's a net positive as a whole. Of course, if we _were_ able to choose so, we probably would have less to be concerned about already, given that we'd clearly have a mechanism for informed, preventive, and collective action.\n",
				"date_published": "2023-06-29T23:02:19+02:00",
				"url": "https://blog.gaborkozar.me/2023/06/29/tech-erosion-the.html",
				"tags": ["technology","AI"]
			}
	]
}
